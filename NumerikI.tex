%% LyX 2.0.5 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english,ngerman]{scrreprt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2.6cm,bmargin=3.5cm,lmargin=2.6cm,rmargin=2.6cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{color}
\usepackage{babel}
\usepackage{float}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,
 breaklinks=true,pdfborder={0 0 0},backref=page,colorlinks=false]
 {hyperref}
\hypersetup{pdftitle={Numerik I},
 pdfauthor={Andreas Völklein},
 pdfkeywords={Numerik, Mathematik}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\newcommand{\noun}[1]{\textsc{#1}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{enumitem}		% customizable list environments
\newlength{\lyxlabelwidth}      % auxiliary length 

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tikz}
%\usepackage{tikz-3dplot,cancel,polynom,algorithm2e}
\usetikzlibrary{matrix,arrows,calc,decorations,snakes,intersections,shapes,arrows}
\usetikzlibrary{external}
\tikzexternalize
\usepackage{latexsym,stmaryrd,stackrel,icomma,braket,bbm,subfig,framed}
\usepackage[explicit]{titlesec}

\pgfkeys{/pgf/number format/dec sep={\text{,}}}

% Inhaltsverzeichnis
\usepackage[subfigure]{tocloft}

\tocloftpagestyle{fancy}

\renewcommand{\cftchapindent}{1 em}
\renewcommand{\cftchapnumwidth}{1.5 em}

\renewcommand{\cftsecindent}{2.7 em}
\renewcommand{\cftsecnumwidth}{2.5em}

\renewcommand{\cftsubsecindent}{5.2 em}
\renewcommand{\cftsubsecnumwidth}{3.8 em}

\renewcommand{\cftsubsubsecindent}{9 em}
\renewcommand{\cftsubsubsecnumwidth}{4.5 em}

% Mathe-Operatoren
\DeclareMathOperator*{\exsop}{\exists}
\DeclareMathOperator*{\exsgop}{\exists!}
\DeclareMathOperator*{\fallop}{\forall}
\DeclareMathOperator*{\bcupdop}{\dot{\bigcup}}
\DeclareMathOperator*{\bcapdop}{\dot{\bigcap}}

%Operatornorm
\newcommand{\opnor}[1]{\abs{\hspace*{-1.1pt}\norm{#1}\hspace*{-1.1pt}}}

% nicht-totales Differential
\newcommand{\dBar}{\mathchar'26\mkern-12mu \textnormal{d}}

% Rotieren
\newcommand{\Rotate}[1]{
\begin{tikzpicture}
\node[rotate=90] {\ensuremath{#1}};
\end{tikzpicture}
}

%QED-Zeichen (Box)
\newcommand{\qed}{\ensuremath{\Box}}
\newcommand{\qqed}[1][\arabic{chapter}.\arabic{section}\ifnum\arabic{subsection}>0{.\arabic{subsection}}\fi]{\hspace*{1mm}\hfill\qed\ensuremath{_{\text{#1}}}}

% Mengen Modulo
\newcommand{\moduloT}[2]{
\mbox{\raisebox{0.6ex}{\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\Large /}
\raisebox{-0.6ex}{\hspace*{-1.5mm}\ensuremath{\displaystyle #2}}
}}

% Links Modulo
\newcommand{\lmoduloT}[2]{
\mbox{\raisebox{-0.6ex}{\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\Large \ensuremath{\backslash}}
\raisebox{0.6ex}{\hspace*{-1.5mm}\ensuremath{\displaystyle #2}}
}}

% Für Z/2Z, um nicht soviel schreiben zu müssen
\newcommand{\modloT}[2]{\moduloT{ \mathbb{#1}}{#2\mathbb{#1}}}

%Laplace-Beltrami-Operator
\newcommand{\LBO}{
\begin{minipage}{6mm}
 \begin{tikzpicture}
   \node at (0,0){$\Delta$};
   \draw[line width=0.75] (0.25,-0.13) -- (0.1,0.15);
 \end{tikzpicture}
\end{minipage}
}

%Die Modulo-Kommandos in klein, für die Darstellungen unter Quantoren.
\newcommand{\moduloScriptT}[2]{
\mbox{\raisebox{0.4ex}{\scriptsize\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\footnotesize /}
\raisebox{-0.4ex}{\hspace*{-1.5mm}\scriptsize\ensuremath{\displaystyle #2}}
}}

\newcommand{\lmoduloScriptT}[2]{
\mbox{\raisebox{-0.4ex}{\scriptsize\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\footnotesize \ensuremath{\backslash}}
\raisebox{0.4ex}{\hspace*{-1.5mm}\scriptsize\ensuremath{\displaystyle #2}}
}}

\newcommand{\modloScriptT}[2]{\moduloScriptT{ \mathbb{#1}}{#2\mathbb{#1}}}

% stehendes Winkelzeichen
\newcommand{\winkel}{
\begin{tikzpicture}[scale=0.25]
\draw ({-2+3^(1/2)},0) -- (0,1) -- ({2-3^(1/2)},0);
\draw ($(0,1) + ({cos(235)*0.7},{sin(315)*0.7})$) arc (235:315:0.7);
\end{tikzpicture}}

% Wurzel mit Häkchen
\newcommand{\hsqrt}[2][{}]{\setbox0=\hbox{$\sqrt[#1]{\phantom{|}\!\! #2\hspace*{1pt}}$}\dimen0=\ht0
  \advance\dimen0-0.2\ht0
  \setbox2=\hbox{\vrule height\ht0 depth -\dimen0}
  {\box0\lower0.4pt\box2}}

% Damit nicht immer "Kapitel 1" etc. über der Kapitelüberschrift steht
\titleformat{\chapter}
  {\huge\bfseries}
  {\textrm{\thechapter} }{0pt}
  {\textrm{#1} \thispagestyle{fancy}
  }

% Neudefinition der Abschnittsmarker für die Kopfzeile
\renewcommand\partmark[1]{\markboth{#1}{}}
\renewcommand\chaptermark[1]{\markright{\arabic{chapter} #1}}
\renewcommand\sectionmark[1]{}
\renewcommand\subsectionmark[1]{}

% Schriften auf Serif umstellen
\addtokomafont{descriptionlabel}{\rmfamily}
\addtokomafont{disposition}{\rmfamily}

\allowdisplaybreaks

% Kopf- und Fußzeile
% Höhe der Kopfzeile
\setlength{\headheight}{14pt}
% obere Trennlinie
%\renewcommand{\headrulewidth}{0.4pt}
\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
\fancyhead[L]{\textbf{Numerik I}} %Kopfzeile links
%\fancyhead[C]{\leftmark} %zentrierte Kopfzeile
\fancyhead[R]{\rightmark} %Kopfzeile rechts
\fancyfoot[C]{\thepage\quad\!\!\!\slash\quad\!\!\!\pageref{END-front}} %Seitenzahl der Front-Matter

\AtBeginDocument{
  \def\labelitemi{\normalfont\bfseries{--}}
  \def\labelitemii{\(\circ\)}
  \def\labelitemiii{\(\triangleright\)}
}

\makeatother

\begin{document}






\global\long\def\norm#1{\left\lVert #1\right\rVert }


\global\long\def\abs#1{\left\lvert #1\right\rvert }


\global\long\def\opnorm#1{\opnor{#1}}


\global\long\def\BRA#1{\Bra{#1}}


\global\long\def\KET#1{\Ket{#1}}


\global\long\def\BraKet#1{\Braket{#1}}


\global\long\def\mins{\text{-}}


\global\long\def\LB{\LBO}


\global\long\def\exs{\exsop}


\global\long\def\exsg{\exsgop}


\global\long\def\fall{\fallop}


\global\long\def\bcupd{\bcupdop}


\global\long\def\bcapd{\bcapdop}


\global\long\def\sr#1#2#3{\underset{#3}{\overset{#2}{#1}}}


\global\long\def\dd{\textnormal{d}}


\global\long\def\DD{\textnormal{D}}


\global\long\def\dbar{\dBar}


\global\long\def\TT{\textnormal{T}}


\global\long\def\ii{\textbf{i}}


\global\long\def\modulo#1#2{\moduloT{#1}{#2}}


\global\long\def\lmodulo#1#2{\lmoduloT{#1}{#2}}


\global\long\def\modlo#1#2{\modloT{#1}{#2}}


\global\long\def\moduloScript#1#2{\moduloScriptT{#1}{#2}}


\global\long\def\lmoduloScript#1#2{\lmoduloScriptT{#1}{#2}}


\global\long\def\modloScript#1#2{\modloScriptT{#1}{#2}}


\global\long\def\vek#1{\vectorsym{#1}}


\global\long\def\mat#1{\matrixsym{#1}}


\global\long\def\ten#1{\tensorsym{#1}}


\global\long\def\msd#1{\mathstrut_{#1}}


\global\long\def\msu#1{\mathstrut^{#1}}


\pagenumbering{roman}


\title{\hspace*{1mm}\vspace*{-15mm}\\
{\Huge Numerik I}}


\author{\vspace*{-5mm}\\
\textit{\small Vorlesung von}\\
\textit{\noun{\small Prof. Dr. Harald Garcke}}\\
\textit{\small im Wintersemester 2012/13}\\
\textit{\small Überarbeitung und Textsatz in \LyX{} von}\\
\textit{\noun{\small Andreas Völklein}}\\
\vspace*{5mm}\\
\includegraphics[clip,width=15cm]{unir}\\
\vspace*{3mm}\\
Stand: \today{\normalsize }\\
\vspace*{-30mm}}

\maketitle
\fancyhead[R]{Lizenz}
\setcounter{page}{2} % sonst gibt es aus irgendeinem Grund zweimal die Seite 1!?!?!?


\subsubsection*{ACHTUNG}

Diese Mitschrift ersetzt \emph{nicht} die Vorlesung.

Es wird daher \emph{dringend} empfohlen, die Vorlesung zu besuchen.

\vfill{}


\selectlanguage{english}%

\subsubsection*{Copyright Notice}

Copyright © 2012-2013 \noun{Andreas Völklein}

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;

with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
Texts.

A copy of the license is included in the section entitled “GNU Free
Documentation License”.


\subsubsection*{Disclaimer of Warranty}

\noun{Unless otherwise mutually agreed to by the parties in writing
and to the extent not prohibited by applicable law, }\textbf{\noun{the
Copyright Holders and any other party, who may distribute the Document
as permitted above,   provide the Document “as is}}\textbf{”,}\textbf{\noun{
without warranty of any kind}}\noun{, expressed, implied, statutory
or otherwise, including, but not limited to, the implied warranties
of merchantability, fitness for a particular purpose, non-infringement,
the absence of latent or other defects, accuracy, or the absence of
errors, whether or not discoverable.}


\subsubsection*{Limitation of Liability}

\textbf{\noun{In no event}}\noun{ unless required by applicable law
or agreed to in writing }\textbf{\noun{will the Copyright Holders,
or any other party, who may distribute the Document as permitted above,
be liable to you for any damages}}\noun{, including, but not limited
to, any general, special, incidental, consequential, punitive or exemplary
damages, however caused, regardless of the theory of liability, arising
out of or related to this license or any use of or inability to use
the Document, even if they have been advised of the possibility of
such damages.}

\textbf{\noun{In no event will the Copyright Holders'/Distributor's
liability to you}}\noun{, whether in contract, tort (including negligence),
or otherwise, }\textbf{\noun{exceed the amount you paid the Copyright
Holders/Distributor}}\noun{ for the document under this agreement.}

\selectlanguage{ngerman}%

\subsubsection*{Links}

Der Text der „\foreignlanguage{english}{GNU Free Documentation License}“
kann auch auf der Seite
\begin{quote}
\url{https://www.gnu.org/licenses/fdl-1.3.de.html}
\end{quote}
nachgelesen werden.

Eine transparente Kopie der aktuellen Version dieses Dokuments kann
von
\begin{quote}
\url{https://github.com/andiv/NumerikI}
\end{quote}
heruntergeladen werden.

\newpage{}

\fancyhead[R]{Literatur}


\subsection*{Literatur}

Differentialtopologie:
\begin{itemize}
\item \noun{Dahmen, Reusken}: \emph{Numerik für Ingenieure und Naturwissenschaftler};
Springer;\\
doi: 10.1007/978-3-540-76493-9
\item \noun{Deuflhard, Hohmann}: \emph{Numerische Mathematik I, Eine algorithmisch
orientierte Einführung};\emph{ }de Gruyter;\\
ISBN: 978-3-11-020354-7
\item Freund, Hoppe: \emph{Stoer/Bulirsch: Numerische Mathematik 1}; Springer;\\
doi: 10.1007/978-3-540-45390-1
\item \noun{Gene Golub, James Ortega}: \foreignlanguage{english}{\emph{Scientific
Computing}}; 1993, Academic Press\\
ISBN: 0-12-289253-4 
\item \noun{Hämmerlin, Hoffmann}: \emph{Numerische Mathematik}; 1994, Springer\\
ISBN: 3-540-58033-6
\item \noun{Press, Teukolsky, Vertterling, Flannery}: \foreignlanguage{english}{\emph{Numerical
Recipes in C++}}; 2002, Cambridge University Press\\
ISBN: 0-521-75033-4
\item \noun{Stoer, Bulirsch}:\emph{ Numerische Mathematik 2}; Springer\\
doi: 10.1007/b137272
\end{itemize}
{\small \newpage{}}\fancyhead[R]{Inhaltsverzeichnis}
\fancyhead[C]{}

\tableofcontents{}\label{END-front}\newpage{}\pagenumbering{arabic}
\fancyfoot[C]{\thepage\quad\!\!\!\slash\quad\!\!\!\pageref{END}} % Seitenzahl des Hauptteils
\fancyhead[R]{\rightmark}
%\fancyhead[C]{\leftmark}%DATE: Mo 15.10.2012


\chapter{Einführung}


\section{Aufgaben der numerischen Mathematik}
\begin{itemize}
\item Entwicklung und Analyse von Rechenmethoden zur angenäherten Lösung
von Problemen innerhalb der Mathematik und in den Anwendungen
\item Rechenmethoden $\leadsto$ Algorithmen $\leadsto$ Programm
\end{itemize}
\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=3em, column sep=2.5em, text height=2.5ex, text depth=0.75ex] 
  { \text{\tiny Numerische Mathematik} & \text{\tiny Informatik} & \text{\tiny $\sr{\text{Natur-, Wirtschafts-}}{\text{Modellbildung in}}{\text{und Geisteswissenschaften}}$}\\
		 & \text{\tiny Scientific Computing / Computational Science} & \\ };
  \path[->,font=\scriptsize] (m-1-1) edge node[left] {\tiny numerische Algorithmen} (m-2-2);
  \path[->,font=\scriptsize] (m-1-2) edge node[left] {\tiny $\sr{}{\text{effiziente}}{\text{Speichermethoden}}$} node[right]{\tiny $\sr{}{\text{Parallele}}{\text{Datenstrukturen}}$} (m-2-2);
  \path[->,font=\scriptsize] (m-1-3) edge node[auto] {\tiny mathematische Formulierung eines Problems} (m-2-2);
\end{tikzpicture}\caption{Numerik im Zusammenspiel verschiedener Disziplinen}

\par\end{centering}

\end{figure}


Betrachten wir ein klassisches Problem und studieren die obigen Aspekte.
\begin{enumerate}
\item Phänomen in der Natur\\
Beispiel: Bewegung von Himmelskörpern (z.B. Dreikörperproblem: Erde,
Sonne, Mond oder allgemeinere Mehrkörperprobleme)
\item Mathematisches Modell\\
Sei $x_{i}\left(t\right)\in\mathbb{R}^{3}$ die Position von Himmelskörper
$i$ ($i\in\left\{ 1,2,3\right\} $) zur Zeit $t$ (hier drei Körper).\\
Newtonsche Bewegungsgleichungen:
\begin{align*}
m_{i}\cdot\ddot{x}_{i} & =-\nabla_{x_{i}}U\left(x_{1}\left(t\right),x_{2}\left(t\right),x_{3}\left(t\right)\right)
\end{align*}
$m_{i}$: Masse des Körpers\\
$\ddot{x}_{i}$: zweite zeitliche Ableitung von $x_{i}$\\
$U$: potentielle Energie
\begin{align*}
U\left(x_{1},x_{2},x_{3}\right) & =-G\sum_{\sr{}{i\not=j}{i,j=1}}^{3}\frac{m_{i}m_{j}}{\abs{x_{i}-x_{j}}}
\end{align*}
$G$: Gravitationskonstante
\item Mathematische Theorie\\
Kennt man zum Anfangszeitpunkt Position und Geschwindigkeit der Himmelskörper,
dann hat die Differentialgleichung aus 2. genau eine (lokale) Lösung
nach dem Satz von Picard-Lindelöf.
\item Numerische Mathematik\\
Ersetze das (in $t$) kontinuierliche Problem aus 2. durch ein diskretes
Problem. Statt $x_{i}$ zu berechnen, berechnen wir eine Funktion
\begin{align*}
x_{i}^{h}:\underbrace{h\cdot\mathbb{N}_{0}}_{=\left\{ 0,h,2h,\ldots\right\} } & \to\mathbb{R}^{3}
\end{align*}
für kleines $h\in\mathbb{R}_{>0}$, die nur zu den Zeiten $h_{n}$
($n\in\mathbb{N}$) definiert ist und die Differentialgleichung näherungsweise
erfüllt.\\
z. B.: Berechne $x_{i}^{h}\left(nh\right)$, sodass für $i\in\left\{ 1,2,3\right\} $
gilt:
\begin{align*}
m_{i}\frac{x_{i}^{h}\left(\left(n+1\right)h\right)-2x_{i}^{h}\left(nh\right)+x_{i}^{h}\left(\left(n-1\right)h\right)}{h^{2}} & =-\nabla_{x_{i}}U\left(\left(x_{1}^{h},x_{2}^{h},x_{3}^{h}\right)\left(nh\right)\right)
\end{align*}
Verwende dabei die Näherung:
\begin{align*}
\ddot{x}_{i}\left(nh\right) & \approx\frac{1}{h}\left(\frac{x_{i}^{h}\left(\left(n+1\right)h\right)-x_{i}^{h}\left(nh\right)}{h}-\frac{x_{i}^{h}\left(nh\right)-x_{i}^{h}\left(\left(n-1\right)h\right)}{h}\right)=\\
 & =\frac{x_{i}^{h}\left(\left(n+1\right)h\right)-2x_{i}^{h}\left(nh\right)+x_{i}^{h}\left(\left(n-1\right)h\right)}{h^{2}}
\end{align*}
Frage: Konvergiert $x_{i}^{h}\to x_{i}$?
\item Numerische Berechnung\\
Entwickle Algorithmus zur Berechnung von $x_{i}^{h}$. $\leadsto$
Computerprogramm $\leadsto$ Ergebnisse\\
(Oben ist dies einfach.)
\item Analyse der Ergebnisse\\
Vergleichen der Computerergebnisse mit Messungen und mathematischen
Resultaten.\\
Falls Abweichungen auftreten, stellen sich die folgenden Fragen:

\begin{enumerate}[label=\alph*)]
\item Stimmt das mathematische Modell?
\item Ist meine numerische Methode gut? (Nähert 4. die Situation 2. gut
an?)
\item Wie groß ist der Einfluss von Mess- und Rundungsfehlern?
\end{enumerate}
\end{enumerate}
%DATE: Mi 17.10.2012


\section{Mit welchen Fragestellungen wollen wir uns beschäftigen?\label{sec:Fragestellungen}}
\begin{enumerate}
\item Lösen linearer Gleichungssysteme\\
Seien $A$ eine $\left(n\times n\right)$-Matrix (Notation: $A\in\mathbb{R}^{n\times n}$)
und $b\in\mathbb{R}^{n}$.\\
Gesucht ist eine Lösung $x\in\mathbb{R}^{n}$ mit $Ax=b$.\\
Beispiel: Wettervorhersage/Strömungen mit mehr als $10^{6}$ Unbekannten.

\begin{enumerate}[label=\alph*)]
\item \emph{Cramersche Regel:}
\begin{align*}
x_{j} & =\frac{\det\left(A\right)_{j}}{\det A}
\end{align*}
Die Matrix $\left(A\right)_{j}$ ergibt sich folgendermaßen: Ersetze
die $j$-te Spalte von $A$ durch $b$.\\
Die Determinanten ergeben lassen sich mit den Permutationen der symmetrischen
Gruppe $\mathfrak{S}\left(n\right)$ berechnen:
\begin{align*}
\det A & =\sum_{\pi\in\mathfrak{S}\left(n\right)}\text{sign}\left(\pi\right)a_{1,\pi\left(1\right)}\cdot\ldots\cdot a_{n,\pi\left(n\right)}
\end{align*}
Diese Berechnung einer Determinante benötigt etwa $n\cdot n!$ Multiplikationen
und Additionen. Nun sind aber die $n$ Determinanten $\det\left(A\right)_{j}$
und die Determinante $\det\left(A\right)$ zu berechnen. Insgesamt
benötigt man also $n\cdot\left(n+1\right)!$ Operationen.\\
Bei einer $\left(20\times20\right)$-Matrix (kleine Matrix!) sind
das in etwa $10^{21}$ Operationen. Falls jede arithmetische Operation
$10^{\mins6}$ Sekunden braucht, benötigen wir Rechenzeiten von $10^{15}\,\text{s}$,
also in etwa 30 Millionen Jahre! Ein Supercomputer mit $3\cdot10^{15}$
Flops bräuchte fast vier Tage.
\item \emph{Gaußsches Eliminationsverfahren:} Benötigt etwa $n^{3}$ Operationen,
bei $n=20$ sind das ungefähr 8000 Operationen und die Rechenzeit
beträgt weniger als $0{,}005$ Sekunden!
\end{enumerate}
\item Lösen nichtlinearer Gleichungen\\
Seien $a_{i}\in\mathbb{R}$ und $p\left(x\right)=\sum_{i=0}^{n}a_{i}x^{i}$.
Gesucht ist ein $x\in\mathbb{R}$ mit $p\left(x\right)=0$.\\
Beispiel: $p\left(x\right)=x^{2}-5$\\
Gesucht: Verfahren zur Berechnung von $\sqrt{5}$\\
Starte mit $x_{0}>0$ beliebig. Berechne iterativ für $i\in\mathbb{N}_{0}$:
\begin{align*}
x_{i+1} & =\frac{1}{2}\left(x_{i}+\frac{5}{x_{i}}\right)
\end{align*}
Behauptung: $x_{i}\xrightarrow{i\to\infty}\sqrt{5}$
\item Approximation\\
Beispiel: lineare Ausgleichsprobleme\\
Zusammenhang zwischen Spannung und Stromfluss\\
$U$: Spannung, $I$: Stromstärke, $R$: Widerstand (unbekannt), Ohmsches
Gesetz: $U=RI$\\
Messungen: $\left(U_{i},I_{i}\right)$ für $i\in\left\{ 1,\ldots,N\right\} $\\
Vermutung: Es gibt Messfehler.\\
\textcolor{green}{TODO: Abb1 einfügen}\\
Finde $R$ so, dass
\begin{align*}
f\left(z\right) & =\sum_{i=1}^{N}\left(U_{i}-zI_{i}\right)^{2}
\end{align*}
bei $R$ minimal wird, also $f\left(R\right)=\min_{z\in\mathbb{R}}f\left(z\right)$.
\item Interpolation\\
Ein Flugzeug soll durch die Punkte $\left(x_{i},y_{i}\right)$ fliegen.
Der Flug soll möglichst glatt verlaufen.\\
\textcolor{green}{TODO: Abb2 einfügen}\\
Gesucht: Eine Kurve (möglichst glatt), die durch die Punkte $\left(x_{i},y_{i}\right)_{i\in\left\{ 1,\ldots,N\right\} }$
geht. Die Kurve soll einfach zu berechnen sein.
\item Flächenberechnung/Integralberechnung\\
\textcolor{green}{TODO: Abb3 einfügen}\\
Berechne $\int_{a}^{b}f\left(x\right)\dd x$. Aus Erfahrung wissen
wir: Es ist nicht immer möglich Integrale exakt auszurechnen. Benötige
ein Näherungsverfahren.
\item Optimierungsverfahren (Numerik II)\\
Eine bayerische Brauerei produziert Pils und Weißbier.\\
Zur Produktion benötigen wir:


\noindent \begin{center}
\begin{tabular}{|c||c|c|}
\hline 
 & Pils & Weißbier\tabularnewline
\hline 
\hline 
Malz & 2 Einheiten & 4 Einheiten\tabularnewline
\hline 
Hopfen & 5 Einheiten & 2 Einheiten\tabularnewline
\hline 
Hefe & 5 Einheiten & 4 Einheiten\tabularnewline
\hline 
\end{tabular}
\par\end{center}

\item []{\addtocounter{enumii}{-1}}Zur Verfügung stehen: 120 Einheiten
Malz, 150 Einheiten Hopfen und 130 Einheiten Hefe\\
Ziel: Maximiere Gewinn. Gewinn ist für beide Biere gleich.\\
$x_{1}$: Liter Pils, $x_{2}$: Liter Weißbier; Maximiere $x_{1}+x_{2}$\\
Aufgabe: Maximiere $f\left(x_{1},x_{2}\right)=x_{1}+x_{2}$ unter
den Nebenbedingungen:
\begin{align*}
2x_{1}+4x_{2} & \le120 &  & x_{1}\ge0\\
5x_{1}+2x_{2} & \le150 &  & x_{2}\ge0\\
5x_{1}+4x_{2} & \le120
\end{align*}

\item Verhalten bei Störungen, Stabilität des Verfahrens\\
(Eingabefehler, Rundungsfehler, Diskretisierungsfehler)\\
Beispiele:

\begin{enumerate}[label=\roman*)]
\item $\frac{1}{10^{-8}}=10^{8}$; Störung des Nenners um $10^{-8}$ $\leadsto$
$\frac{1}{2\cdot10^{-8}}=5\cdot10^{7}$
\item $x^{2}+314x-2=0$ (allgemein: $ax^{2}+bx+c=0$);\\
Löse Gleichung mit $p$-$q$-Formel (Mitternachtsformel). Gerechnet
wird auf 5 signifikante Stellen genau, wobei sich ein relativer Fehler
$=\frac{\text{Fehler}}{\text{Größe der Lösung}}$ von $57\%$ ergibt.
Eine einfache aber geschickte Umformulierung ergibt einen relativen
Fehler von nur circa $1,5\cdot10^{-5}$. Diese ist wie folgt:
\begin{align*}
x_{1} & =\frac{1}{2a}\left(-b-\text{sign}\left(b\right)\sqrt{b^{2}-4ac}\right)\\
x_{2} & =\frac{c}{ax_{1}}=\frac{2c}{-b-\text{sign}\left(b\right)\sqrt{b^{2}-4ac}}
\end{align*}
Exakte Lösung: $0{,}0063693$\\
Mit Rundung: Mitternachtsformel: $x_{2}\approx0,01$; Umformulierung:
$x_{2}\approx0{,}0063692$
\end{enumerate}
\end{enumerate}

\chapter{Gaußsches Eliminationsverfahren\label{chap:Gau=0000DFsches-Eliminationsverfahren}}

Sei $A\in\mathbb{R}^{n\times n}$ eine reelle $n\times n$-Matrix.
Verwende die Schreibweise $A=\left(a_{ij}\right)_{i,j\in\left\{ 1,\ldots,n\right\} }$
mit den Einträgen $a_{ij}$.
\begin{align*}
A & =\left(\begin{array}{ccc}
a_{11} & \ldots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{n1} & \ldots & a_{nn}
\end{array}\right)
\end{align*}
Es sei $b\in\mathbb{R}^{n}$. Gesucht ist $x\in\mathbb{R}^{n}$ mit
$Ax=b$.


\section{Existenz und Eindeutigkeit von Lösungen}

\emph{Existenz:} Eine Lösung $x\in\mathbb{R}^{n}$ existiert genau
dann, wenn $\text{rg}\left(A\right)=\text{rg}\left(A,b\right)$ gilt,
das heißt $b$ eine Linearkombination der Spalten von $A$ ist.\\
Sei $\text{Rg}\left(A\right)=m<n$. Dann gilt:
\begin{enumerate}
\item $\dim\left(\ker\left(A\right)\right)=n-m\not=0$
\item $Ax=b$ hat entweder keine Lösung oder es existiert eine Lösung $x$
und für alle $y\in\ker\left(A\right)$ ist auch $x+y$ eine Lösung
von $Ax+b$.
\end{enumerate}
Gilt andererseits $\text{rg}\left(A\right)=n$, so ist $A$ surjektiv
und wegen $A\in\mathbb{R}^{n\times n}$ gilt äquivalent die Injektivität
von $A$, das heißt $\det\left(A\right)\not=0$, und es gibt genau
eine Lösung von $Ax=b$. Eine solche Matrix heißt \emph{regulär} oder
nicht \emph{singulär}.

Im Folgenden sei $A$ nicht singulär.


\section{Gleichungssysteme mit Dreiecksmatrizen}

Im Falle von Dreiecksmatrizen
\begin{align*}
A & =\left(\begin{array}{cccc}
a_{11} & \ldots & \ldots & a_{1n}\\
0 & \ddots &  & \vdots\\
\vdots & \ddots & \ddots & \vdots\\
0 & \ldots & 0 & a_{nn}
\end{array}\right)
\end{align*}
ergibt sich ein gestaffeltes Gleichungssystem.
\begin{align*}
Ax & =b\quad\Leftrightarrow\quad\begin{array}{rl}
a_{11}x_{1}+\ldots+a_{nn}x_{n} & =b_{1}\\
\vdots & \phantom{==}\vdots\\
a_{n-1,n-1}x_{n-1}+a_{n-1,n}x_{n} & =b_{n-1}\\
a_{nn}x_{n} & =b_{n}
\end{array}
\end{align*}


\noindent \begin{center}
\begin{tabular}{|c|c|c|}
\hline 
Auflösung & Multiplikation/Division & Addition/Subtraktion\tabularnewline
\hline 
\hline 
$x_{n}=\frac{b_{n}}{a_{n}}$ & 1 & 0\tabularnewline
\hline 
$x_{n-1}=\frac{\left(b_{n-1}-a_{n-1,n}x_{n}\right)}{a_{n-1,n-1}}$ & 2 & 1\tabularnewline
\hline 
$\vdots$ & $\vdots$ & $\vdots$\tabularnewline
\hline 
$x_{n}=\frac{\left(b_{1}-a_{12}x_{2}-\ldots-a_{1n}x_{n}\right)}{a_{11}}$ & $n$ & $n-1$\tabularnewline
\hline 
\end{tabular}
\par\end{center}

Aufwand in flops (\foreignlanguage{english}{floating point Operations},
Gleitkommaoperationen):
\begin{align*}
\sum_{i=1}^{n}\left(i+i-1\right) & =\frac{n\left(n+1\right)}{2}+\frac{n\left(n-1\right)}{2}=n^{2}
\end{align*}
Sei nun $A=LR$ mit einer unteren Dreiecksmatrix $L$ und einer oberen
Dreiecksmatrix $R$. Löse $Ax=b$ in zwei Schritten:
\begin{enumerate}
\item Bestimme $y$ mit $Ly=b$.
\item Bestimme $x$ mit $Rx=y$.
\end{enumerate}
Ziel: Bringe jede Matrix $A$ auf die Form $A=LR$.

Ist das möglich? Nahezu ja.


\section{Gaußsches Eliminationsverfahren (ohne Pivotisierung)}

Zunächst ein Beispiel:
\begin{align*}
A & =\left(\begin{array}{cc}
1 & 2\\
3 & 4
\end{array}\right) & \left(\begin{array}{cc}
1 & 2\\
3 & 4
\end{array}\right)x & =b
\end{align*}
\begin{align*}
x_{1}+2x_{2} & =b_{1}\\
3x_{1}+4x_{2} & =b_{2}
\end{align*}
Multipliziere erste Gleichung mit $\left(-3\right)$ und addiere das
Ergebnis zur zweiten Zeile:
\begin{align*}
x_{1}+2x_{2} & =b_{1}\\
-2x_{2} & =b_{2}-3b_{1}
\end{align*}
Das Gleichungssystem hat nun Dreiecksgestalt.

In Matrixform:
\begin{align*}
\left(\begin{array}{cc}
1 & 0\\
-3 & 1
\end{array}\right)\left(\begin{array}{cc}
1 & 2\\
3 & 4
\end{array}\right)x & =\left(\begin{array}{cc}
1 & 0\\
-3 & 1
\end{array}\right)b\\
\left(\begin{array}{cc}
1 & 2\\
0 & -2
\end{array}\right)x & =\left(\begin{array}{cc}
1 & 0\\
-3 & 1
\end{array}\right)b
\end{align*}
Nun gilt:
\begin{align*}
\left(\begin{array}{cc}
1 & 0\\
-3 & 1
\end{array}\right)\left(\begin{array}{cc}
1 & 0\\
3 & 1
\end{array}\right) & =\left(\begin{array}{cc}
1 & 0\\
0 & 1
\end{array}\right)\\
\Leftrightarrow\qquad\left(\begin{array}{cc}
1 & 0\\
-3 & 1
\end{array}\right)^{-1} & =\left(\begin{array}{cc}
1 & 0\\
3 & 1
\end{array}\right)
\end{align*}
\begin{align*}
\end{align*}
Also gilt:
\begin{align*}
\left(\begin{array}{cc}
1 & 0\\
3 & 1
\end{array}\right)\left(\begin{array}{cc}
1 & 2\\
0 & -2
\end{array}\right)x & =b
\end{align*}
Allgemein: Transformiere beliebiges invertierbares $A\in\mathbb{R}^{n\times n}$
auf Dreiecksgestalt.
\begin{itemize}
\item 1. Schritt: (Neue Zeile $i)=($ Alte Zeile $i$) $-$ $l_{i1}\cdot$
(Alte Zeile 1) für $i\in\left\{ 2,\ldots,n\right\} $\\
Dabei sei $l_{i1}=\frac{a_{i1}}{a_{11}}$ ($i\in\left\{ 2,\ldots,n\right\} $).
Voraussetzung: $a_{11}\not=0$
\begin{align*}
\leadsto\quad a_{11}x_{1}+a_{12}x_{2}+\ldots+a_{1n}x_{n} & =b_{1}\\
a_{22}^{\left(2\right)}x_{2}+\ldots+a_{2n}^{\left(2\right)}x_{n} & =b_{2}^{\left(2\right)}\\
\vdots\quad & \quad\vdots\\
a_{n2}^{\left(2\right)}x_{2}+\ldots+a_{nn}^{\left(2\right)}x_{n} & =b_{n}^{\left(2\right)}
\end{align*}

\item Iteriere diesen Prozess bis man die Dreiecksgestalt erhält. Im $k$-ten
Schritt ($a_{ij}^{\left(1\right)}:=a_{ij}$):
\begin{align*}
l_{ij} & =a_{ik}^{\left(k\right)} & i & \in\left\{ k+1,\ldots,n\right\} \\
a_{ij}^{\left(k+1\right)} & =a_{ij}^{\left(k\right)}-l_{ik}a_{kj}^{\left(k\right)} & i,j & \in\left\{ k+1,\ldots,n\right\} \\
b_{i}^{\left(k+1\right)} & =b_{i}^{\left(k\right)}-l_{ik}b_{k}^{\left(k\right)} & i & \in\left\{ k+1,\ldots,n\right\} 
\end{align*}
Stelle dies in Matrixform dar:
\begin{align*}
A^{\left(k\right)} & :=\left(\begin{array}{cccccc}
a_{11}^{\left(k\right)} & a_{12}^{\left(k\right)} & \ldots & \ldots & \ldots & a_{1n}^{\left(k\right)}\\
0 & \ddots &  &  &  & \vdots\\
\vdots & \ddots & \ddots &  &  & \vdots\\
\vdots &  & 0 & a_{kk}^{\left(k\right)} & \ldots & a_{kn}^{\left(k\right)}\\
\vdots &  & \vdots & \vdots & \ddots & \vdots\\
0 & \ldots & 0 & a_{nk}^{\left(k\right)} & \ldots & a_{nn}^{\left(k\right)}
\end{array}\right)
\end{align*}
\begin{align*}
A & =A^{\left(1\right)}\to A^{\left(2\right)}\to\ldots\to A^{\left(n\right)}=:R
\end{align*}
Die Operationen von $A^{\left(k-1\right)}\to A^{\left(k\right)}$
drücken wir durch Frobenius-Matrizen aus.\\
Eine Frobenius-Matrix hat (nach Definition) die Gestalt:
\begin{align*}
L_{k}:=\mathbbm{1}-l^{k}e_{k}^{\TT} & =\left(\begin{array}{ccccccc}
1 &  &  &  &  &  & 0\\
 & \ddots\\
 &  & \ddots\\
 &  &  & 1\\
 &  &  & -l_{k+1,k} & \ddots\\
 &  &  & \vdots &  & \ddots\\
0 &  &  & -l_{nk} & 0 &  & 1
\end{array}\right)
\end{align*}
$e_{k}$ ist der $k$-te Basisvektor und $e_{k}^{\TT}$ der transponierte
Vektor.
\begin{align*}
l^{k} & =\left(\begin{array}{c}
0\\
\vdots\\
0\\
l_{k+1,k}\\
\vdots\\
l_{nk}
\end{array}\right)
\end{align*}

\end{itemize}
%DATE: Mo 22.10.12


\section{Lemma \textmd{(\texorpdfstring{$LR$}{LR}-Zerlegung)}}
\begin{enumerate}
\item 
\begin{align*}
A^{\left(k+1\right)} & =L_{k}\cdot A^{(k)}
\end{align*}

\item 
\begin{align*}
b^{\left(k-1\right)} & =L_{k}\cdot b^{(k)}
\end{align*}

\item 
\begin{align*}
R & :=A^{(n)}=L_{n-1}\cdot\ldots\cdot L_{1}\cdot A
\end{align*}

\item 
\begin{align*}
L_{k}^{-1} & =\mathbbm{1}+l^{k}\cdot e_{k}^{\TT}
\end{align*}

\item 
\begin{align*}
L:=\left(L_{n-1}\cdot\dots\cdot L_{1}\right)^{-1} & =\mathbbm{1}+\sum_{k=1}^{n-1}l^{k}\cdot e_{k}^{\TT}=\left(\begin{array}{cccc}
1 &  &  & 0\\
l_{2,1} & \ddots\\
\vdots & \ddots & \ddots\\
l_{n1} & \ldots & l_{n,n-1} & 1
\end{array}\right)
\end{align*}

\item $R$ ist eine obere Dreiecksmatrix und es gilt:
\begin{align*}
A & =L\cdot R
\end{align*}

\end{enumerate}

\subsubsection*{Beweis}
\begin{enumerate}
\item Für $i,j\leq k$ ist $A_{ij}^{\left(k+1\right)}=A_{ij}^{\left(k\right)}$
und sonst $a_{ij}^{\left(k+1\right)}=a_{ij}^{\left(k\right)}-l_{ik}a_{kj}^{\left(k\right)}$.
Es gilt:
\begin{align*}
L_{k}\cdot A^{(k)} & =\left(\begin{array}{cccccccc}
1 &  &  &  &  &  &  & 0\\
 & \ddots\\
 &  & \ddots\\
\\
 &  &  & 1\\
 &  &  & -l_{k+1,k} & \ddots\\
 &  &  & \vdots &  &  & \ddots\\
0 &  &  & -l_{nk} &  & 0 &  & 1
\end{array}\right)A^{(k)}=\\
 & =\left(\begin{array}{cccccc}
a_{11}^{\left(k\right)} & a_{12}^{\left(k\right)} & \ldots & \ldots & \ldots & a_{1n}^{\left(k\right)}\\
0 & \ddots &  &  &  & \vdots\\
\vdots & \ddots & \ddots &  &  & \vdots\\
\vdots &  & 0 & a_{kk}^{\left(k\right)} & \ldots & a_{kn}^{\left(k\right)}\\
\vdots &  & \vdots & a_{k+1,k}^{\left(k\right)}-l_{k+1,k}a_{kk}^{\left(k\right)} &  & a_{k+1,n}^{\left(k\right)}-l_{k+1,k}a_{kn}^{\left(k\right)}\\
\vdots &  & \vdots & \vdots & \ddots & \vdots\\
0 & \ldots & 0 & a_{nk}^{\left(k\right)}-l_{nk}a_{kk}^{\left(k\right)} & \ldots & a_{nn}^{\left(k\right)}-l_{nk}a_{kn}^{\left(k\right)}
\end{array}\right)=A^{\left(k+1\right)}
\end{align*}

\item Analog zu 1. folgt dies.
\item Induktion ergibt: 
\begin{align*}
A^{(n)} & =L_{n-1}A^{(n-1)}=L_{n-1}\cdot L_{n-2}\cdot A^{\left(n-2\right)}=\dots=L_{n-1}\cdot\dots\cdot L_{1}\cdot A
\end{align*}

\item Es gilt:
\begin{align*}
L_{k} & =\mathbbm{1}-l^{k}\cdot e_{k}^{\TT}\\
\left(\mathbbm{1}+l^{k}\cdot e_{k}^{\TT}\right)\cdot\left(\mathbbm{1}-l^{k}\cdot e_{k}^{\TT}\right) & =\mathbbm{1}-l^{k}\cdot e_{k}^{\TT}+l^{k}e_{k}^{\TT}-\underbrace{l^{k}\cdot e_{k}^{\TT}\cdot l^{k}\cdot e_{k}^{\TT}}_{=0}=\mathbbm{1}\\
\Rightarrow\qquad\left(\mathbbm{1}-l^{k}\cdot e_{k}^{\TT}\right)^{-1} & =\mathbbm{1}+l^{k}\cdot e_{k}^{\TT}
\end{align*}

\item Damit folgt:
\begin{align*}
\left(L_{n-1}\cdot\dots\cdot L_{1}\right)^{-1} & =L_{1}^{-1}\cdot\dots\cdot L_{n-1}^{-1}=\left(\mathbbm{1}+l^{1}\cdot e_{1}^{\TT}\right)\cdot\dots\cdot\left(\mathbbm{1}+l^{n-1}\cdot e_{n-1}^{\TT}\right)
\end{align*}


\begin{description}
\item [{Behauptung:}] 
\begin{align*}
\prod_{i=1}^{n-1}\left(\mathbbm{1}+l^{i}\cdot e_{i}^{\TT}\right) & =\mathbbm{1}+\sum_{i=1}^{n-1}l^{i}e_{i}^{\TT}
\end{align*}

\item [{Beweis:}] Induktionsschritt $k\leadsto k+1$: 
\begin{align*}
\prod_{i=1}^{k+1}\left(\mathbbm{1}+l^{i}\cdot e_{i}^{\TT}\right) & =\left(\mathbbm{1}+l^{k+1}\cdot e_{k+1}^{\TT}\right)\left(\mathbbm{1}+\sum_{i=1}^{k}l^{k}e_{k}^{\TT}\right)=\\
 & =\mathbbm{1}+\sum_{i=1}^{k}l^{k}e_{k}^{\TT}+l^{k+1}\cdot e_{k+1}^{\TT}+l^{k+1}\cdot\underbrace{e_{k+1}^{\TT}\sum_{i=1}^{k}l^{k}e_{k}^{\TT}}_{=0}=\\
 & =\mathbbm{1}+\sum_{i=1}^{k+1}l^{k}e_{k}^{\TT}
\end{align*}
\qqed[Behauptung]
\end{description}
\item Dies gilt nach Konstruktion von $R$ und $L$.
\end{enumerate}
\qqed


\section{Algorithmus zum Eliminationsverfahren\label{sec:Algorithmus-zum-Eliminationsverfahren}}

Vorgehen beim Lösen des Gleichungssystems $A\cdot x=b$:
\begin{enumerate}
\item Bilde die $LR$-Zerlegung von $A$.
\item Löse das Gleichungssystem $L\cdot z=b$.
\item Löse das Gleichungssystem $R\cdot x=z$.
\end{enumerate}
\emph{Vorteil:} Hat man die $LR$-Zerlegung, so kann man das Gleichungssystem
für $A\cdot x=b$ für viele rechte Seiten lösen. Falls wir $A$ nicht
mehr brauchen, speichert man die Matrizen $L$ und $R$ auf der alten
Matrix $A$, um Speicherplatz zu sparen. 
\begin{align*}
A & \mapsto\begin{pmatrix}r_{1,1} & \dots & \dots & r_{1,n}\\
l_{2,1} & \ddots &  & \vdots\\
\vdots & \ddots & \ddots & \vdots\\
l_{n,1} & \dots & l_{n,n-1} & r_{n,n}
\end{pmatrix}
\end{align*}



\section{Landau-Symbole}

Es seien $f,g:D\to\mathbb{R}^{m}$, $D\subseteq\mathbb{R}^{n}$, $n,m\in\mathbb{N}$.
\begin{enumerate}[label=\roman*)]
\item Wir sagen $f=\mathcal{O}_{x_{0}}\left(g\right)$ für $x\to x_{0}$,
falls gilt:
\begin{align*}
\lim_{\sr{}{x\to x_{0}}{x\in D\setminus\left\{ x_{0}\right\} }}\sup & \frac{\norm{f\left(x\right)}}{\norm{g\left(x\right)}}<\infty
\end{align*}
(d. h. es gibt ein $K>0$ mit $\norm{f\left(x\right)}\le K\norm{g\left(x\right)}$
für $x$ nahe $x_{0}$.)
\item Wir sagen $f=o_{x_{0}}\left(g\right)$ für $x\to x_{0}$, falls gilt:
\begin{align*}
\lim_{\sr{}{x\to x_{0}}{x\in D\setminus\left\{ x_{0}\right\} }}\frac{\norm{f\left(x\right)}}{\norm{g\left(x\right)}} & =0
\end{align*}
Bemerkung: $x_{0}=\pm\infty$ ist zugelassen.
\end{enumerate}

\subsubsection*{Bemerkung}

Wenn klar ist, was $x_{0}$ ist, schreibt man häufig einfach $\mathcal{O}\left(g\right)$
beziehungsweise $o\left(g\right)$.


\section{Aufwand des Gaußschen Eliminationsverfahrens}

Zähle Gleitkommaoperationen ($+$, $\cdot$, $-$, $:$):

Es ergibt sich:
\begin{align*}
\sum_{k=1}^{n-1}\left(n-k\right)\left(1+\left(n-k\right)\cdot2\right) & =\sum_{l=1}^{n-1}l\left(1+2l\right)=\sum_{l=1}^{n-1}\left(l+2l^{2}\right)=\\
 & =\frac{n\left(n-1\right)}{2}+2\frac{n\left(n-1\right)\left(2n-1\right)}{6}=\frac{2}{3}n^{3}+\mathcal{O}_{\infty}\left(n^{2}\right)
\end{align*}
Für $n=20$ brauchen wir weniger als 6000 Operationen, im Gegensatz
zu $10^{21}$ für die Cramersche Regel!


\section{LR-Zerlegung nicht immer möglich/sinnvoll}
\begin{enumerate}
\item Beispiel:
\begin{align*}
A & =\left(\begin{array}{cc}
0 & 1\\
1 & 2
\end{array}\right)
\end{align*}
Hier ist $a_{11}=0$ und die $LR$-Zerlegung deshalb nicht anwendbar.
Als Ausweg kann man jedoch die Zeilen vertauschen.
\item Eine LR-Zerlegung kann große Fehler generieren.
\begin{align*}
A & =\left(\begin{array}{cc}
10^{-m} & 1\\
1 & 1
\end{array}\right) & b & =\left(\begin{array}{c}
1\\
2
\end{array}\right)
\end{align*}
Löse $Ax=b$.
\begin{align*}
A^{\left(2\right)} & =\left(\begin{array}{cc}
10^{-m} & 1\\
0 & 1-10^{m}
\end{array}\right) & b^{\left(2\right)} & =\left(\begin{array}{c}
1\\
2-10^{m}
\end{array}\right)
\end{align*}
$1-10^{m}$: zur Darstellung brauche $m-1$-Stellen: 9999999$\ldots$999\\
Hat der Rechner nur $n<m-1$ Stellen zur Verfügung, wird abgeschnitten
(oder gerundet) (vergleiche Kapitel \ref{chap:Zahlendarstellung_Fehleranalyse}):
\begin{align*}
\tilde{A}^{\left(2\right)} & =\left(\begin{array}{cc}
10^{-m} & 1\\
0 & -10^{m}
\end{array}\right) & b^{\left(2\right)} & =\left(\begin{array}{c}
1\\
-10^{m}
\end{array}\right)
\end{align*}
Die Lösung auf dem Rechner ist daher:
\begin{align*}
\tilde{x}_{2} & =1 & \tilde{x}_{1} & =0
\end{align*}
Die tatsächliche Lösung hingegen ist:
\begin{align*}
\left(1-10^{m}\right)x_{2} & =2-10^{m} & \Rightarrow\quad x_{2} & =\frac{2-10^{m}}{1-10^{m}}\approx1\\
x_{1} & =\frac{10^{m}}{10^{m}-1}\approx1
\end{align*}
Die schlechte Lösung kommt durch Rundungsfehler zustande. Die Vertauschung
der Zeilen rettet uns!
\begin{align*}
\left(\begin{array}{cc}
1 & 1\\
10^{-m} & 1
\end{array}\right)\left(\begin{array}{c}
x_{1}\\
x_{2}
\end{array}\right) & =\left(\begin{array}{c}
2\\
1
\end{array}\right)\\
\left(\begin{array}{cc}
1 & 1\\
0 & 1-10^{-m}
\end{array}\right)\left(\begin{array}{c}
x_{1}\\
x_{2}
\end{array}\right) & =\left(\begin{array}{c}
2\\
1-2\cdot10^{-m}
\end{array}\right)\\
\left(\begin{array}{cc}
1 & 1\\
0 & 1
\end{array}\right)\left(\begin{array}{c}
x_{1}\\
x_{2}
\end{array}\right) & \approx\left(\begin{array}{c}
2\\
1
\end{array}\right)\\
\Rightarrow\quad x_{1} & \approx1\\
x_{2} & \approx1
\end{align*}

\end{enumerate}

\section{Gauß-Elimination mit Pivotisierung}

Pivotstrategie zur Elimination in der $k$-ten Spalte:
\begin{enumerate}
\item Wähle $p\in\left\{ k,\ldots,n\right\} $, sodass gilt:
\begin{align*}
\abs{a_{pp}^{\left(k\right)}} & =\max_{j\in\left\{ k,\ldots,n\right\} }\abs{a_{jk}^{\left(k\right)}}
\end{align*}

\item Definiere
\begin{align*}
\tilde{A}^{\left(k\right)} & =\left(\tilde{a}^{\left(k\right)}\right)_{ij}
\end{align*}
mit für $j\in\left\{ 1,\ldots,n\right\} $:
\begin{align*}
\tilde{a}_{ij}^{\left(k\right)} & =\begin{cases}
a_{kj}^{\left(k\right)} & i=p\\
a_{pj}^{\left(k\right)} & i=k\\
a_{ij}^{\left(k\right)} & \text{sonst}
\end{cases}
\end{align*}
Dies vertauscht die Zeilen $k$ und $p$.
\item Eliminiere wie bisher $\tilde{A}^{\left(k\right)}\leadsto A^{\left(k+1\right)}$.
\end{enumerate}
Ziel: Schreibe dies in Matrixform:

Der Austausch von zwei Zeilen kann mit Hilfe von Permutationen beschrieben
werden. Sei
\begin{align*}
\sigma:\left\{ 1,\ldots,n\right\}  & \to\left\{ 1,\ldots,n\right\} 
\end{align*}
eine Permutation in $\mathfrak{S}_{n}$. Definiere die Matrix:
\begin{align*}
P_{\sigma} & =\left(e_{\sigma\left(1\right)}\ldots e_{\sigma\left(n\right)}\right)\in\mathbb{R}^{n\times n}
\end{align*}
Es gilt:
\begin{align*}
P_{\sigma}e_{i} & =e_{\sigma\left(i\right)}\\
\Rightarrow\qquad P_{\sigma}A & =\left(\begin{array}{c}
a_{\sigma^{-1}\left(1\right)}\\
\vdots\\
a_{\sigma^{-1}\left(n\right)}
\end{array}\right)
\end{align*}
Dabei ist $a_{i}=\left(a_{i1},\ldots,a_{in}\right)$ die $i$-te Zeile
von $A$. Damit lässt sich das Vorgehen 1. bis 3. wie folgt beschreiben:
\begin{align*}
R & =A^{\left(n\right)}=L_{n-1}P_{n-1}\cdot\ldots\cdot L_{2}P_{2}L_{1}P_{1}A
\end{align*}
Dabei vertauscht $P_{i}$ zwei Zeilen mit Index $\ge i$.

%DATE: Mi 24.10.12


\section{Lemma über Permutationen}

Sei $\sigma:\left\{ 1,\ldots,n\right\} \to\left\{ 1,\ldots,n\right\} $
eine Permutation. Dann ist $P_{\sigma}$ orthogonal, das heißt es
gilt $P_{\sigma}^{-1}=P_{\sigma}^{\TT}$. Außerdem gilt:
\begin{align*}
P_{\sigma}^{-1} & =P_{\sigma^{-1}}
\end{align*}



\subsubsection*{Beweis}

\begin{align*}
\left(P_{\sigma^{-1}}\circ P_{\sigma}\right)\left(e_{i}\right) & =e_{\sigma^{-1}\left(\sigma\left(i\right)\right)}=e_{i}
\end{align*}
Also gilt $P_{\sigma}^{-1}=P_{\sigma^{-1}}$. Sei nun $x,y\in\mathbb{R}^{n}$.
\begin{align*}
x\cdot y & =x^{\TT}y=\sum_{i=1}^{n}x_{i}y_{i}
\end{align*}
Um $P_{\sigma}^{\TT}=P_{\sigma}^{-1}$ zu zeigen, zeige:
\begin{align*}
x\cdot\left(P_{\sigma}^{-1}y\right) & =P_{\sigma}x\cdot y
\end{align*}
\begin{align*}
e_{i}\left(P_{\sigma}^{-1}e_{j}\right) & =e_{i}\cdot\left(P_{\sigma^{-1}}e_{j}\right)=e_{i}\cdot e_{\sigma^{-1}\left(j\right)}=\delta_{i,\sigma^{-1}\left(j\right)}=\\
 & \sr ={i=\sigma^{-1}\left(j\right)\Leftrightarrow\sigma\left(i\right)=j}{}\delta_{\sigma\left(i\right),j}=e_{\sigma\left(i\right)}\cdot e_{j}=P_{\sigma}e_{i}\cdot e_{j}
\end{align*}
\qqed


\section{Satz \textmd{(Existenz einer \texorpdfstring{$LR$}{LR}-Zerlegung
mit Spalten-Pivotisierung)}}

Für jede invertierbare Matrix $A$ existiert eine Permutationsmatrix
$P$, sodass die $LR$-Zerlegung $PA=LR$ möglich ist. $P$ kann so
gewählt werden, dass gilt:
\begin{align*}
\max_{1\le i,j\le n}\abs{l_{ij}} & \le1
\end{align*}



\subsubsection*{Beweis}

1. Schritt: Da $A$ invertierbar ist, folgt:
\begin{align*}
\abs{a_{p1}^{\left(1\right)}} & =\max_{1\le i\le n}\underbrace{\abs{a_{i1}^{\left(1\right)}}}_{=\abs{a_{i1}}}>0
\end{align*}
Vertauschung liefert ein $\tilde{a}_{11}^{\left(1\right)}\not=0$.
Die Elimination ist also möglich und
\begin{align*}
A^{\left(2\right)} & =L_{1}P_{1}A=\left(\begin{array}{cccc}
a_{11}^{\left(2\right)} & * & \ldots & *\\
0\\
\vdots &  & B^{\left(2\right)}\\
0
\end{array}\right)
\end{align*}
ist als Produkt invertierbarer Matrizen invertierbar und somit auch
$B^{\left(2\right)}$.\\
Dieses Verfahren kann iteriert werden. Damit ergibt sich:
\begin{align*}
R & =L_{n-1}P_{n-1}L_{n-2}P_{n-2}\ldots L_{1}P_{1}A
\end{align*}
Dabei vertauscht $P_{i}$ zwei Zeilen mit Index $\ge i$ und:
\begin{align*}
L_{i} & =\mathbbm{1}-l^{i}e_{i}^{\TT} & l_{ik} & =\begin{cases}
\frac{\tilde{a}_{ik}^{\left(k\right)}}{\tilde{a}_{kk}^{\left(k\right)}} & \text{für }i>k\\
0 & \text{für }i\le k
\end{cases}
\end{align*}
Ziel: Vertausche sukzessive $L_{i}$und Permutationsmatrizen $P_{j}$.

Behauptung: Für $i>k-1$ gilt
\begin{align*}
P_{i}L_{k-1} & =\hat{L}_{k-1}P_{i}
\end{align*}
mit:
\begin{align*}
\hat{L}_{k-1} & =\left(\mathbbm{1}-\left(P_{i}l^{k-1}\right)e_{k-1}^{\TT}\right)
\end{align*}
(Das heißt, im Vektor $l^{i}$ werden zwei Einträge gemäß Permutation
$P_{i}$ vertauscht.)
\begin{align*}
P_{i}L_{k-1} & =P_{i}\left(\mathbbm{1}-l^{k-1}e_{k-1}^{\TT}\right)P_{i}^{-1}P_{i}=\\
 & =\left(\mathbbm{1}-P_{i}l^{k-1}e_{k-1}^{\TT}P_{i}^{-1}\right)P_{i}
\end{align*}
Es gilt:
\begin{align*}
e_{k-1}^{\TT}P_{i}^{-1} & =e_{k-1}^{\TT}P_{i}^{\TT}=\left(P_{i}e_{k-1}\right)^{\TT}=e_{k-1}^{\TT}
\end{align*}
Denn $P_{i}$ vertauscht Zeilen mit Index $\ge i$ und es ist $i>k-1$.
Es folgt:
\begin{align*}
P_{i}L_{k-1} & =\left(\mathbbm{1}-P_{i}l^{k-1}e_{k-1}^{\TT}\right)P_{i}
\end{align*}
Per Induktion folgt:
\begin{align*}
P_{n-1}\cdot\ldots\cdot P_{k}L_{k-1} & =\left(\mathbbm{1}-\left(P_{n-1}\cdot\ldots\cdot P_{k}\right)l^{k-1}e_{k-1}^{\TT}\right)P_{n-1}\cdot\ldots\cdot P_{k}
\end{align*}

\begin{description}
\item [{Behauptung:}] Für $k\in\left\{ n-1,n-2,\ldots,1\right\} $ gilt:
\begin{align*}
L_{n-1}P_{n-1}\cdot\ldots\cdot L_{k}P_{k} & =\prod_{j=k}^{n-1}\left(\mathbbm{1}-\hat{l}^{j}e_{j}^{\TT}\right)P_{n-1}\cdot\ldots\cdot P_{k}
\end{align*}
\begin{align*}
\hat{l}^{j} & :=P_{n-1}\cdot\ldots\cdot P_{j+1}l^{j}
\end{align*}

\item [{Beweis:}] Induktion: Induktionsanfang bei $k=n-1$ ist klar.\\
Induktionsschritt $k\leadsto k-1$:
\begin{align*}
 & L_{n-1}P_{n-1}\cdot\ldots\cdot L_{k-1}P_{k-1}=\\
 & \qquad=\prod_{j=k}^{n-1}\left(\mathbbm{1}-\hat{l}^{j}e_{j}^{\TT}\right)P_{n-1}\cdot\ldots\cdot P_{k}L_{k-1}P_{k-1}=\\
 & \qquad=\prod_{j=k}^{n-1}\left(\mathbbm{1}-\hat{l}^{j}e_{j}^{\TT}\right)\left(\mathbbm{1}-\left(P_{n-1}\cdot\ldots\cdot P_{k}\right)l^{k-1}e_{k-1}^{\TT}\right)P_{n-1}\cdot\ldots\cdot P_{k}P_{k-1}=\\
 & \qquad=\prod_{j=k-1}^{n-1}\left(\mathbbm{1}-\hat{l}^{j}e_{j}^{\TT}\right)P_{n-1}\cdot\ldots\cdot P_{k-1}
\end{align*}
\qqed[Behauptung]
\end{description}
Damit ergibt sich:
\begin{align*}
R & =\hat{L}PA\\
\hat{L} & :=\prod_{j=1}^{n-1}\left(\mathbbm{1}-\hat{l}^{j}e_{j}^{\TT}\right)=\mathbbm{1}-\sum_{j=1}^{n-1}\hat{l}^{j}e_{j}^{\TT}\\
L & :=\hat{L}^{-1}=\prod_{j=1}^{n-1}\left(\mathbbm{1}+\hat{l}^{j}e_{j}^{\TT}\right)
\end{align*}
Es gilt $PA=LR$ und die Pivotisierungsstrategie sorgt dafür, dass
$\abs{l_{ij}}\le1$ ist.\qqed


\section{Bemerkungen}
\begin{enumerate}
\item $L$ ergibt sich aus Frobeniusmatrizen durch Vertauschen von Elementen
$l_{ik}$. Die Vertauschung ist analog wie in $A^{\left(k\right)}$.
Diese Tatsache wird im Algorithmus ausgenutzt.
\item Permutationsmatrizen $P$ können beschrieben (und gespeichert) werden,
indem man die Permutation der Indizes $\left\{ 1,\ldots,n\right\} $
angibt.\\
Auf dem Rechner nutze einen Vektor $s=\left(s_{1},\ldots,s_{n}\right)$
mit $s_{i}\in\left\{ 1,\ldots,n\right\} $. $s_{i}$ gibt an, welche
alte Zeile nach Vertauschung in Zeile $i$ steht. (vergleiche $PA$)
\item Die Zerlegung $PA=LR$ kann genutzt werden, um $\det\left(A\right)$
zu berechnen. Es gilt:
\begin{align*}
\det\left(A\right) & =\det\left(P^{-1}LR\right)=\underbrace{\det\left(P^{\TT}\right)}_{=\det\left(P\right)}\cdot\underbrace{\det\left(L\right)}_{=1}\cdot\underbrace{\det\left(R\right)}_{=r_{11}\cdot\ldots\cdot r_{nn}}
\end{align*}
Falls keine Permutation nötig war gilt:
\begin{align*}
\det\left(A\right) & =r_{11}\cdot\ldots\cdot r_{nn}
\end{align*}
Außerdem gilt:
\begin{align*}
\det\left(P_{\sigma}\right) & =\text{sign}\left(\sigma\right)
\end{align*}

\end{enumerate}

\section{Der Gauß-Algorithmus mit Pivotisierung\label{sec:Der-Gau=0000DF-Algorithmus-mit-Pivotisierung}}

Dies ist einen Algorithmus zur Bestimmung von $P$, $L$ und $R$,
sodass $PA=LR$ gilt.
\begin{quote}
\texttt{input($\left(a_{ij}\right)_{i,j\in\left\{ 1,\ldots,n\right\} }$)}

\texttt{for ($k=1,\ldots,n$) // $s_{k}=k$}

\texttt{\{\hspace*{8mm}$p=k$}

\texttt{\hspace*{1cm}for ($i=k+1,\ldots,n$)}

\texttt{\hspace*{2cm}\{if $\abs{a_{ik}}>\abs{a_{pk}}$ then $p=i$\}}

\texttt{\hspace*{1cm}if ($p\not=k$) then}

\texttt{\hspace*{1cm}\{ // Vertausche $s_{k}$ und $s_{p}$}

\texttt{\hspace*{2cm}for ($j=1,\ldots,n$)}

\texttt{\hspace*{3cm}\{Vertausche $a_{pj}$ und $a_{kj}$\} // Vertauscht
auch die $l_{ij}$!}

\texttt{\hspace*{1cm}\}}

\texttt{\hspace*{1cm}for ($i=k+1,\ldots,n$)}

\texttt{\hspace*{1cm}\{\hspace*{8mm}$a_{ik}=\frac{a_{ik}}{a_{kk}}$}

\texttt{\hspace*{2cm}for ($j=k+1,\ldots,n$)}

\texttt{\hspace*{3cm}\{$a_{ij}=a_{ij}-a_{ik}\cdot a_{kj}$\}}

\texttt{\hspace*{2cm}\}}

\texttt{\hspace*{1cm}\}}

\texttt{output ($\left(a_{ij}\right)_{i,j\in\left\{ 1,\ldots,n\right\} }$,
$\left(s_{k}\right)_{k\in\left\{ 1,\ldots,n\right\} }$)}
\end{quote}

\section{Eindeutigkeit der \texorpdfstring{$LR$}{LR}-Zerlegung}

Zu einer nicht-singulären Matrix $A$ gibt es höchstens eine Zerlegung
$A=LR$ mit einer unteren Dreiecksmatrix $L$ mit $l_{ii}=1$ und
einer unteren Dreiecksmatrix $R$.


\subsubsection*{Beweis}

Sei $A=\left(a_{ij}\right)_{ij}$, $L=\left(l_{ij}\right)_{ij}$ und
$R=\left(r_{ij}\right)_{ij}$ mit:
\begin{align*}
A & =LR\\
\Rightarrow\quad a_{ij} & =\sum_{k=1}^{\text{min}\left(i,j\right)}l_{ik}r_{kj}
\end{align*}
Für $i\le j$ gilt:
\begin{align*}
r_{ij} & =a_{ij}-\sum_{k<i}l_{ik}r_{kj}
\end{align*}


Für $i>j$ gilt:
\begin{align*}
a_{ij} & =\sum_{k=1}^{j-1}l_{ik}r_{kj}+l_{ij}\underbrace{r_{jj}}_{\not=0}\\
\Rightarrow\quad l_{ij} & =\frac{1}{r_{jj}}\left(a_{ij}-\sum_{k=1}^{j-1}l_{ik}r_{kj}\right)
\end{align*}
Wir können $l_{ij}$ und $r_{ij}$ rekursiv berechnen. Für $i\in\left\{ 2,\ldots,n\right\} $
berechne abwechselnd
\begin{align*}
r_{ij} & =a_{ij}-\sum_{k=1}^{i-1}l_{ik}r_{kj} &  & j\in\left\{ i,\ldots,n\right\} 
\end{align*}
und:
\begin{align*}
l_{ki} & =\frac{1}{r_{ii}}\left(a_{ki}-\sum_{j=1}^{i-1}l_{kj}r_{ji}\right) &  & k\in\left\{ i,\ldots,n\right\} 
\end{align*}
\begin{align*}
\left(\begin{array}{cccc}
r_{11} & \cdots & \cdots & r_{1n}\\
l_{21} & \ddots &  & \vdots\\
\vdots & \ddots & \ddots & \vdots\\
l_{n1} & \cdots & l_{n,n-1} & r_{nn}
\end{array}\right) & \xrightarrow[\text{Reihenfolge berechnen}]{\text{werden in dieser}}\left(\begin{array}{cccc}
\to & 1 & \to\\
\downarrow & \to & 3 & \to\\
2 & \downarrow &  & 5\\
\downarrow & 4 & 6\\
 & \downarrow &  & \cdots
\end{array}\right)
\end{align*}
Berechne in der Reihenfolge 1, 2, 3, $\ldots$. Dies zeigt, dass sich
die $\left(l_{ij}\right)$ und $\left(r_{ij}\right)$ eindeutig aus
den $\left(a_{ij}\right)$ berechnen lassen. Es folgt die Eindeutigkeit
der $LR$-Zerlegung und ein Berechnungsverfahren.\qqed

%DATE: Mo 29.10.12


\chapter{Zahlendarstellung und Fehleranalyse\label{chap:Zahlendarstellung_Fehleranalyse}}


\section{Ursachen von Fehlern}

Es gibt verschiedene Fehlerursachen: 
\begin{enumerate}
\item Fehler im mathematischen Modell 
\item Messfehler 
\item Rundungsfehler 
\item Approximationsfehler 
\end{enumerate}
Die Punkte 1. und 2. behandeln wir hier nicht.\\
Zum 4. Punkt ein Beispiel: Ein Integral 
\begin{align*}
\int_{a}^{b}f\left(x\right)\dd x
\end{align*}
ist eventuell nicht exakt berechenbar.
\begin{align*}
\int_{a}^{b}f\left(x\right)\dd x & \stackrel{\text{Fehler!}}{\approx}\sum_{i=1}^{N}f\left(x_{i}\right)\cdot h\\
x_{i} & =a+i\cdot h=a+i\cdot\frac{b-a}{N}
\end{align*}
In diesem Kapitel studieren wir den Einfluss von Fehlern auf die Qualität
von numerischen Problemen. Zunächst behandeln wir die Frage, wie Fehler
auf dem Rechner dargestellt werden?


\section{Zahlendarstellung}

Jede Zahl $x\neq0$ die auf dem Rechner dargestellt wird hat die Form:
\begin{align*}
x & =\left(-1\right)^{s}\cdot m\cdot d^{e}
\end{align*}
Dabei bestimmt $s\in\left\{ 0,1\right\} $ das Vorzeichen. $e\in\mathbb{Z}$
ist der Exponent, $m$ die Mantisse und $d\in\mathbb{N}_{\ge2}$ die
Basis (meist $d=2$). Es gilt
\begin{align*}
e_{\text{min}} & \le e\le e_{\text{max}}
\end{align*}
und:
\begin{align*}
m & =m_{0}+\sum_{i=1}^{l}m_{i}\; d^{-i}\\
m_{i} & \in\left\{ 0,1,\ldots,d-1\right\} \\
m_{0} & \not=0
\end{align*}
Hier sind $d,\ e_{\text{min}},\ e_{\text{max}}$ und $l$ fest.


\section{Beispiel \textmd{(Maschinenzahlen mit einfacher/doppelter Genauigkeit)}}

Wir betrachten Zahlen mit je 32 Bit also 4 Byte Speicherplatz. Es
sei $d=2$ (Dualdarstellung), also $m_{0}=1$, $e_{\text{min}}=-126$,
$e_{\text{max}}=127$ und $l=23$. Wie speichern wir $x=\left(-1\right)^{s}\cdot m\cdot d^{e}$?
Definiere $b:=e+127\in\left\{ 1,\ldots,254\right\} $ und schreibe
$b$ als Dualzahl:
\begin{align*}
b & =\sum_{i=0}^{7}b_{i}2^{i} & b_{i} & \in\left\{ 0,1\right\} 
\end{align*}
Die 32 Bit sind wie folgt belegt:
\begin{align*}
\fbox{\fbox{\ensuremath{s\vphantom{\big|}}}\fbox{\ensuremath{b_{0}\vphantom{\big|}}}\ \ensuremath{\ldots}\ \fbox{\ensuremath{b_{7}\vphantom{\big|}}}\fbox{\ensuremath{m_{1}\vphantom{\big|}}}\ \ensuremath{\ldots}\ \fbox{\ensuremath{m_{23}\vphantom{\big|}}}}
\end{align*}
$m_{0}=1$ muss nicht abgespeichert werden. Die Null wird durch
\begin{align*}
b_{1}=...=b_{7}=m_{1}=...=m_{23} & =0
\end{align*}
dargestellt. Unendlich entspricht:
\begin{align*}
b_{1}=...=b_{7} & =1\\
m_{1}=...=m_{23} & =0
\end{align*}
Plus unendlich hat $s=0$, minus unendlich $s=1$. Die Folge $b_{1}=...=b_{7}=1$,
$m_{i}\neq0$ für mindestens ein $i$ ergibt NaN (\foreignlanguage{english}{Not
a Number}). Letzteres ist zum Beispiel das Ergebnis von $\sqrt{-5}$.
Bei doppelter Genauigkeit (8 Byte) gilt $l=52$, $e_{\text{min}}=-1022$,
$e_{\text{max}}=1023$.


\section{Einige wichtige Zahlen}

Es gibt eine größte Zahl: 
\begin{align*}
x_{\text{max}} & \in\mathbb{F}\left(d,e_{\text{min}},e_{\text{max}},l\right)
\end{align*}
Sie ist: 
\begin{align*}
x & =\left(d-1\right)\cdot\left(1+\sum_{i=1}^{l}d^{-i}\right)\cdot d^{e_{\text{max}}}=\left(d-1\right)\frac{1-d^{-\left(l+1\right)}}{1-d^{-1}}d^{e_{\text{max}}}=\\
 & =\left(1-d^{-\left(l+1\right)}\right)\cdot d^{e_{\text{max}}+1}=d^{e_{\text{max}}+1}-d^{e_{\text{max}}-l}
\end{align*}
Bei Float wäre das ungefähr $3,4\cdot10^{38}$. Heute werden auch
nicht normalisierte Zahlen benutzt.\\
Beispiel: Float (einfache Genauigkeit in C). Falls $b_{0}=...=b_{7}=0$,
so entspricht dies der Zahl


\section{Relative und absolute Fehler}
\begin{enumerate}
\item Seien nun $x,\tilde{x}\in\mathbb{R}^{n}$ und $x\not=0$, so definieren
wir:
\begin{align*}
\varrho_{\tilde{x}}\left(x\right) & :=\frac{\norm{\tilde{x}-x}}{\norm x}\\
\alpha_{\tilde{x}}\left(x\right) & :=\norm{\tilde{x}-x}
\end{align*}

\item Sei $x\in\mathbb{R}$ und sei $\tilde{x}\in\mathbb{R}$ eine Näherung
von $x$. Dann ist
\begin{align*}
\alpha_{\tilde{x}}\left(x\right) & :=\abs{\tilde{x}-x}
\end{align*}
der \emph{absolute} und für $x\not=0$
\begin{align*}
\varrho_{\tilde{x}}\left(x\right) & :=\frac{\abs{\tilde{x}-x}}{\abs x}
\end{align*}
der \emph{relative}. Der relative Fehler ist skalierungsinvariant:
\begin{align*}
\varrho_{v\tilde{x}}\left(vx\right) & =\varrho_{\tilde{x}}\left(x\right)
\end{align*}
Die Grundoperationen $+,\,-,\,\cdot,\,/$ werden durch $\oplus,\,\ominus,\,\odot,\,\oslash$
auf dem Rechner realisiert. Für diese Grundoperationen gibt es Maschinenzahlen,
sodass $x,y\in\mathbb{F}$, sodass gilt:
\begin{align*}
\varrho_{x\oplus y}\left(x+y\right) & \le\varepsilon
\end{align*}
Entsprechende Ungleichungen gelten für $-,\,\cdot,\,/$.
\end{enumerate}

\section{Kondition von Problemen}

Wir wollen verstehen, wie sensitiv die Lösung eines Problems von den
Eingabedaten abhängig ist. Dabei betrachten wir nicht, wie wir das
Problem konkret lösen, zum Beispiel durch einen Algorithmus. Alle
Probleme, die wir behandeln, lassen sich mit Abbildungen formulieren.

Seien $E$ die Menge der Eingabedaten, $L$ die Menge der Lösungen
und:
\begin{align*}
f:E & \to L
\end{align*}
Gesucht ist $f\left(z\right)$!

Beispiel: Wir suchen eine Lösung des linearen Gleichungssystems $Ax=b$
mit $A\in\mathbb{R}^{n\times n}$ und $b\in\mathbb{R}^{n}$. Mit $E_{1}\in\text{GL}\left(n,\mathbb{R}\right)$
schreiben wir:
\begin{align*}
E & =E_{1}\times\mathbb{R}^{n}
\end{align*}
Dann ist:
\begin{align*}
f\left(A,b\right) & =A^{-1}b
\end{align*}
Frage: Wie stark ändert sich $f\left(z\right)$, falls $z$ fehlerbehaftet
ist?

Um dies zu diskutieren, brauchen wir neue Begriffe.


\section{Definition \textmd{(Operatornorm)}\label{sec:Def-Operatornorm}}

Auf $\mathbb{R}^{n}$ beziehungsweise $\mathbb{R}^{m}$ seien Normen
$\norm ._{a}$ beziehungsweise $\norm ._{b}$ gegeben. Für $A\in\mathbb{R}^{n\times m}$
ist die \emph{Operatornorm} gegeben durch:
\begin{align*}
\opnorm A_{a,b} & :=\sup_{x\in\mathbb{R}^{n}\setminus\left\{ 0\right\} }\frac{\norm{Ax}_{b}}{\norm x_{a}}
\end{align*}



\subsubsection*{Bemerkung}
\begin{enumerate}
\item Für alle $x\in\mathbb{R}^{n}$ gilt:
\begin{align*}
\norm{Ax}_{b} & \le\opnorm A\cdot\norm x_{a}
\end{align*}

\item $\opnorm A_{a,b}$ hängt von der Wahl von $\norm ._{a}$ und $\norm ._{b}$
ab.
\item Seien $A\in\mathbb{R}^{n\times m}$ und $B\in\mathbb{R}^{m\times l}$.
$\norm ._{a}$, $\norm ._{b}$ und $\norm ._{c}$ seien die Normen
auf $\mathbb{R}^{n}$, $\mathbb{R}^{m}$ beziehungsweise $\mathbb{R}^{l}$.
Dann gilt:
\begin{align*}
\opnorm{A\cdot B}_{ac} & \le\opnorm A_{ab}\cdot\opnorm B_{bc}
\end{align*}



\subsubsection*{Beweis}


\begin{align*}
\opnorm{AB}_{ac} & =\sup_{x\in\mathbb{R}\setminus\left\{ 0\right\} }\frac{\norm{ABx}}{\norm x}\le\sup\opnorm A\cdot\frac{\norm{Bx}}{\norm x}\le\opnorm A\cdot\opnorm B\cdot\norm x
\end{align*}
\qqed[3.]

\end{enumerate}
%DATE: Mi 31.10.12


\section{Satz und Definition \textmd{(Kondition)\label{sec:SuD-Kondition}}}

Sei $E\subseteq\mathbb{R}^{k_{1}}\times\ldots\times\mathbb{R}^{k_{l}}$
offen und $f:E\to\mathbb{R}^{n}\setminus\left\{ 0\right\} $ differenzierbar.\\
Dann gilt für alle $z\in\mathbb{R}^{k_{1}}\times\ldots\times\mathbb{\mathbb{R}}^{k_{l}}$
mit $z_{j}\not=0$ für $j\in\left\{ 1,\ldots,l\right\} $.
\begin{align*}
\varrho_{f\left(\overline{z}\right)}\left(f\left(z\right)\right) & \le\sum_{j=1}^{l}\kappa_{j}\left(f,z\right)\varrho_{\overline{z}}\left(z_{j}\right)+R\left(z-\overline{z}\right) &  & \text{für }\overline{z}\to z
\end{align*}
Dabei ist
\begin{align*}
\varrho_{\overline{z}_{j}}\left(z_{j}\right) & :=\frac{\norm{z_{j}-\overline{z}_{j}}_{j}}{\norm{z_{j}}_{j}} &  & \text{der relativer Fehler der Daten,}\\
\varrho_{f\left(\overline{z}\right)_{j}}\left(f\left(z\right)\right) & :=\frac{\norm{f\left(z\right)-f\left(\overline{z}\right)}}{\norm{f\left(z\right)}} &  & \text{der relativer Fehler der Lösung,}\\
\kappa_{j}\left(f,z\right) & :=\frac{\left(\norm{\abs{\DD_{j}f\left(z\right)}}_{j}\norm{z_{j}}_{j}\right)}{\norm{f\left(z\right)}} &  & \text{der Verstärkungsfaktor}
\end{align*}
und $\norm ._{j}$ und $\norm .$ sind Normen auf $\mathbb{R}^{k_{j}}$
beziehungsweise $\mathbb{R}^{n}$ für $j\in\left\{ 1,\ldots,l\right\} $.
Für $R$ gilt:
\begin{align*}
R\left(z-\overline{z}\right) & =o_{0}\left(\sum_{j=1}^{l}\norm{z_{j}-\overline{z}_{j}}_{j}\right)
\end{align*}
Der Ausdruck $\kappa_{j}\left(f,z\right)$ heißt \emph{Kondition von
$f$ im Punkte $z$ in Richtung $j$}.


\subsubsection*{Beweis}

Die Differenzierbarkeit von $f$ liefert:
\begin{align*}
f\left(\overline{z}\right) & =f\left(z\right)+\sum_{j=1}^{l}\DD_{j}f\left(z\right)\left(\overline{z}_{j}-z_{j}\right)+\overline{R}
\end{align*}
Damit folgt:
\begin{align*}
\overline{R} & =o_{0}\left(\sum_{j=1}^{l}\norm{\overline{z}_{j}-z_{j}}_{j}\right)
\end{align*}
\begin{align*}
\frac{f\left(\overline{z}\right)-f\left(z\right)}{\norm{f\left(z\right)}} & =\frac{1}{\norm{f\left(z\right)}}\sum_{j=1}^{l}\DD_{j}f\left(z\right)\left(\overline{z}_{j}-z_{j}\right)+\frac{1}{\norm{f\left(z\right)}}\overline{R}
\end{align*}
Es folgt:
\begin{align*}
\varrho_{f\left(\overline{z}\right)}\left(f\left(z\right)\right) & =\frac{\norm{f\left(\overline{z}\right)-f\left(z\right)}}{\norm{f\left(z\right)}}\le\frac{1}{\norm{f\left(z\right)}}\sum_{j=1}^{l}\norm{\DD_{j}f\left(z\right)\left(\overline{z}_{j}-z_{j}\right)}+\underbrace{\frac{1}{\norm{f\left(z\right)}}\norm{\overline{R}}}_{=:R}\le\\
 & \le\frac{1}{\norm{f\left(z\right)}}\sum_{j=1}^{l}\frac{\norm{z_{j}}_{j}}{\norm{z_{j}}_{j}}\norm{\abs{\DD_{j}f\left(z\right)}}_{j}\norm{\overline{z}_{j}-z_{j}}_{j}+R=\\
 & =\sum_{j=1}^{l}\kappa_{j}\left(f,z\right)\varrho_{\overline{z}}\left(z_{j}\right)+R
\end{align*}
Dabei erfüllt $R$ die geforderten Eigenschaften.\qqed


\section{Bemerkung}

Die Kondition wird auch als Verstärkungsfaktor bezeichnet. Eine hohe
Kondition bewirkt, dass ein Fehler in den Daten zu großen Fehlern
in der Lösung $f\left(z\right)$ führen kann.

Achtung: Die Ungleichung liefert nur eine Abschätzung, die manchmal
zu pessimistisch ist.


\section{Kondition der elementaren Operationen}
\begin{enumerate}[label=\alph*)]
\item \emph{Multiplikation}: Betrachte:
\begin{align*}
f:\mathbb{R}\times\mathbb{R} & \to\mathbb{R}\\
\left(a,b\right) & \mapsto ab
\end{align*}
Wende den Satz \ref{sec:SuD-Kondition} an ($k_{1}=k_{2}=1$). Alle
Normen sind der Betrag. Für alle $a,b\in\mathbb{R}\setminus\left\{ 0\right\} $
gilt:
\begin{align*}
\kappa_{1}\left(f,\left(a,b\right)\right) & =\frac{\abs{D_{1}f\left(a,b\right)}\cdot\abs a}{\abs{ab}}=\frac{\abs b}{\abs b}=1
\end{align*}
Es folgt:
\begin{align*}
\varrho_{\overline{a}\cdot\overline{b}}\left(a\cdot b\right) & \le\varrho_{\overline{a}}\left(a\right)+\varrho_{\overline{b}}\left(b\right)+R
\end{align*}
Also ist die Multiplikation (und analog die Division) gut konditioniert.
Fehler werden nicht wesentlich verstärkt.
\item \emph{Addition}: Betrachte:
\begin{align*}
f:\mathbb{R}\times\mathbb{R} & \to\mathbb{R}\\
\left(a,b\right) & \mapsto a+b
\end{align*}
Wende den Satz \ref{sec:SuD-Kondition} an ($k_{1}=k_{2}=1$). Alle
Normen sind der Betrag. Für alle $a,b\in\mathbb{R}$ gilt:
\begin{align*}
\kappa_{1}\left(f,\left(a,b\right)\right) & =\frac{\abs{D_{1}f\left(a,b\right)}\cdot\abs a}{\abs{a+b}}=\frac{\abs 1\cdot\abs a}{\abs{a+b}}=\frac{\abs a}{\abs{a+b}}\\
\kappa_{2}\left(f,\left(a,b\right)\right) & =\frac{\abs b}{\abs{a+b}}
\end{align*}
Es folgt:
\begin{align*}
\varrho_{\overline{a}+\overline{b}}\left(a+b\right) & \le\frac{\abs a}{\abs{a+b}}\varrho_{\overline{a}}\left(a\right)+\frac{\abs b}{\abs{a+b}}\varrho_{\overline{b}}\left(b\right)+R
\end{align*}
$\kappa_{1}$ und $\kappa_{2}$ werden groß, falls $a+b$ klein ist.
Also ist die Addition schlecht konditioniert, wenn $a\approx-b$ ist.
Fehler werden dann wesentlich verstärkt.
\end{enumerate}
Auf dem Rechner macht sich dies durch Auslöschung führender Zahlen
bemerkbar.\\
Beispiel: Dezimalzahlen im Rechner mit höchstens 6 Ziffern.\\
Daten: $a=1{,}2346789$; $b=1{,}2345678$
\begin{align*}
a-b & =0{,}000111
\end{align*}
Auf dem Rechner:
\begin{align*}
\overline{a} & =1{,}23467 & \overline{b} & =1{,}23456
\end{align*}
\begin{align*}
\overline{a}-\overline{b} & =0{,}00011
\end{align*}
Also sind nur zwei Ziffern genau. Dies bedeutet einen großen relativen
Fehler.


\section{Kondition des Skalarprodukt}

\begin{align*}
f:\mathbb{R}^{n}\times\mathbb{R}^{n} & \to\mathbb{R}\\
\left(x,y\right) & \mapsto x\cdot y=\sum_{i=1}^{n}x_{i}y_{i}
\end{align*}
Der $\mathbb{R}^{n}$ sei versehen mit der euklidischen Norm. Es gilt:
\begin{align*}
\DD_{1}f\left(x,y\right) & =\left(y_{1},\ldots,y_{n}\right)
\end{align*}
\begin{align*}
\abs{\norm{\DD_{1}f\left(x,y\right)}} & =\sup_{x\in\mathbb{R}\setminus\left\{ 0\right\} }\frac{\abs{x\cdot y}}{\norm x}\stackrel{\max.\text{ bei }x=y}{=}\norm y
\end{align*}
\begin{align*}
\Rightarrow\quad\kappa_{1}\left(f,\left(x,y\right)\right) & =\frac{\norm y\cdot\norm x}{\abs{x\cdot y}}=\frac{1}{\abs{\cos\left(x,y\right)}}
\end{align*}
Dabei ist $\cos\left(x,y\right)$ der Kosinus des Winkels zwischen
$x$ und $y$.

Mit Satz \ref{sec:SuD-Kondition} folgt, dass das Skalarprodukt schlecht
konditioniert ist, wenn $x$ und $y$ nahezu senkrecht aufeinander
stehen.

\textcolor{green}{TODO: Abb1 einfügen}


\section{Kondition linearer Gleichungssysteme\label{sec:Kondition-linearer-Gleichungssysteme}}

Gesucht ist $x$, sodass $Ax=b$ ist, das heißt $x=A^{-1}b$.\\
Frage: Wie stark ändert sich $x$, wenn $A$ und $b$ leicht gestört
sind?

Betrachte: 
\begin{align*}
f:\text{GL}\left(n\right)\times\mathbb{R}^{n} & \mapsto\mathbb{R}^{n}\\
\left(A,b\right) & \mapsto A^{-1}b
\end{align*}
$\norm .$ sei Norm auf $\mathbb{R}^{n}$ und $\norm{\abs .}$ die
zugehörige Operatornorm. Es gilt:
\begin{align*}
\kappa_{2}\left(A,b\right) & =\frac{\norm{\abs{\DD_{2}f}}\cdot\norm b}{\norm{A^{-1}b}}=\frac{\norm{\abs{A^{-1}}}\cdot\norm{Ax}}{\norm x}\le\frac{\norm{\abs{A^{-1}}}\cdot\norm{\abs A}\cdot\norm x}{\norm x}=\norm{\abs{A^{-1}}}\cdot\norm{\abs A}
\end{align*}
Weiter gilt:
\begin{align*}
\kappa_{1}\left(A,b\right) & =\frac{\norm{\abs{\DD_{1}f}}\cdot\norm{\abs A}}{\norm{A^{-1}b}}
\end{align*}
Es gilt:
\begin{align*}
\DD_{1}f & =\DD_{A}\left(A^{-1}b\right)
\end{align*}
Frage: Wie leite ich $A\mapsto A^{-1}$ ab?


\subsection{Lemma \textmd{(Neumannsche Reihe)}}

Sei $\norm{\abs .}$ eine Operatornorm und sei $B\in\text{GL}\left(n\right)$
mit $\norm{\abs B}<1$. Dann gilt:
\begin{align*}
\left(\mathbbm{1}-B\right)^{-1} & =\sum_{k=0}^{\infty}B^{k}
\end{align*}



\subsubsection*{Beweis}

Zunächst zeigen wir:
\begin{align*}
\sum_{k=0}^{\infty}B^{k} & <\infty
\end{align*}
Sei $q:=\norm{\abs B}<1$.
\begin{align*}
\norm{\abs{\sum_{k=m}^{l}B^{k}}} & \le\sum_{k=m}^{l}\norm{\abs{B^{k}}}\stackrel{\ref{sec:Def-Operatornorm}}{\le}\sum_{k=m}^{l}\norm{\abs B}^{k}\le\sum_{k=m}^{l}q^{k}\xrightarrow[\text{geo. R.}]{m,l\to\infty}0
\end{align*}
Das Cauchysche Konvergenz-Kriterium liefert:
\begin{align*}
\lim_{l\to\infty}\sum_{k=0}^{l}B^{k} & <\infty
\end{align*}
Es gilt:
\begin{align*}
\norm{\abs{B^{k}}} & \le\norm{\abs B}^{k}=q^{k}\xrightarrow{k\to\infty}0
\end{align*}
\begin{align*}
\left(\sum_{k=0}^{\infty}B^{k}\right)\left(\mathbbm{1}-B\right) & =\lim_{l\to\infty}\left(\sum_{k=0}^{l}B^{k}\right)\left(\mathbbm{1}-B\right)=\\
 & =\lim_{l\to\infty}\left(\sum_{k=0}^{l}B^{k}-B^{k+1}\right)=\lim_{l\to\infty}\left(\mathbbm{1}-B^{l+1}\right)=\mathbbm{1}
\end{align*}
\qqed


\subsection{Lemma}

Die Abbildung
\begin{align*}
h:\text{GL}\left(n\right) & \to\text{GL}\left(n\right)\\
A & \mapsto A^{-1}
\end{align*}
ist differenzierbar und es gilt:
\begin{align*}
\DD h\left(A\right):\mathbb{R}^{n\times n} & \to\mathbb{R}^{n\times n}\\
B & \mapsto-A^{-1}BA^{-1}
\end{align*}



\subsubsection*{Beweis}

Sei $\norm{\abs B}$ klein.
\begin{align*}
\left(A+B\right)^{-1} & =\left(A\left(\mathbbm{1}+A^{-1}B\right)\right)^{-1}=\left(\mathbbm{1}+A^{-1}B\right)^{-1}A^{-1}=\\
 & \sr ={\text{Neumannsche}}{\text{Reihe}}\left(\sum_{k=0}^{\infty}\left(-A^{-1}B\right)^{k}\right)A^{-1}=A^{-1}-A^{-1}BA^{-1}+\mathcal{O}_{0}\left(\norm{\abs B}^{2}\right)
\end{align*}
Es folgt:
\begin{align*}
\left(A+B\right)^{-1}-A^{-1} & =-A^{-1}BA^{-1}+\mathcal{O}_{0}\left(\norm{\abs B}^{2}\right)
\end{align*}
Damit erfüllt die Abbildung $B\mapsto-A^{-1}BA^{-1}$ die Definition
der Ableitung.\qqed

Es war zu berechnen:
\begin{align*}
\DD_{1}f & =\DD_{A}\left(A^{-1}b\right)
\end{align*}
Nach dem Lemma gilt:
\begin{align*}
\DD_{1}f\left(A,b\right):\mathbb{R}^{n\times n} & \to\mathbb{R}^{n}\\
B & \mapsto-A^{-1}BA^{-1}b
\end{align*}
\begin{align*}
\norm{\abs{\DD_{1}f\left(A,b\right)}} & =\sup_{B}\frac{\norm{A^{-1}BA^{1-}b}}{\norm{\abs B}}\le\sup_{B}\frac{\norm{\abs{A^{-1}}}\cdot\norm{\abs B}\cdot\norm x}{\norm{\abs B}}=\norm{\abs{A^{-1}}}\cdot\norm x
\end{align*}
Insgesamt folgt:
\begin{align*}
\kappa_{1}\left(A,b\right) & =\frac{\norm{\abs{\DD_{1}f}}\cdot\norm{\abs A}}{\norm{A^{-1}b}}\le\frac{\norm{\abs{A^{-1}}}\cdot\norm x\cdot\norm{\abs A}}{\norm x}=\norm{\abs{A^{-1}}}\cdot\norm{\abs A}
\end{align*}
\qqed[\ref{sub:Kondition-linearer-Gleichungssysteme}]

%DATE: Mo 5.11.12


\section{Kondition einer Matrix}
\begin{description}
\item [{Definition:}] Sei $A\in\text{GL}\left(\mathbb{R},n\right)$ und
$\norm .$ eine Norm auf $\mathbb{R}^{n}$. Dann definieren wir die
\emph{Kondition} von $A$ wie folgt:
\begin{align*}
K\left(A\right) & :=\opnorm A\cdot\opnorm{A^{-1}}
\end{align*}
(Dabei wird die gleiche Norm in Bild und Urbild verwendet.)
\end{description}
Die Kondition einer Matrix $A$ gibt an, wie stark Eingangsfehler
die Lösung des Gleichungssystems $Ax=b$ beeinflussen können. Die
Aussage gilt für Fehler in $A$ und $b$!
\begin{description}
\item [{Lemma:}] $K\left(A\right)\ge1$
\item [{Beweis:}] Es gilt:
\begin{align*}
1 & =\opnorm{\mathbbm{1}}=\opnorm{A\cdot A^{-1}}\le\opnorm A\cdot\opnorm{A^{-1}}=K\left(A\right)
\end{align*}
\qqed[Lemma]
\end{description}
Ist $K\left(A\right)$ sehr groß, so sagen wir, $A$ ist schlecht
konditioniert.


\section{Kondition von nichtlinearen Gleichungen}

Sei $g:\mathbb{R}^{n}\to\mathbb{R}^{n}$ stetig differenzierbar und
$x\in\mathbb{R}^{n}$.
\begin{description}
\item [{Problem~(P):}] Gesucht ist $y\in\mathbb{R}^{n}$, sodass $y=g\left(x\right)$
gilt.
\end{description}
Es gelte:
\begin{align*}
\det\left(\DD g\left(y\right)\right) & \not=0
\end{align*}
Dann ist $\DD g\left(y\right)$ invertierbar und es existiert nach
dem Satz über die lokale Umkehrbarkeit (siehe Analysis II) eine Inverse
$f=g^{-1}$ in einer lokalen Umgebung von $x$.

Die Abbildung, die die Lösung von (P) beschreibt, lautet:
\begin{align*}
x & \mapsto g^{-1}\left(x\right)=:f\left(x\right)
\end{align*}
Die Kondition dieses Problems ist gegeben durch:
\begin{align*}
K\left(f,x\right) & =\frac{\opnorm{\DD f\left(x\right)}}{\norm{f\left(x\right)}}\cdot\norm x=\frac{\opnorm{\left(\DD g\left(f\left(x\right)\right)\right)^{-1}}}{\norm{f\left(x\right)}}\norm x=\frac{\opnorm{\left(\DD g\left(y\right)\right)^{-1}}\cdot\norm x}{\norm y}
\end{align*}
Beispiel $n=1$:
\begin{align*}
K\left(f,x\right) & =\frac{\abs{g'\left(y\right)}^{-1}}{\abs y}\cdot\abs{g\left(y\right)}=\frac{\abs x}{\abs{g'\left(y\right)}\cdot\abs y}
\end{align*}


\textcolor{green}{TODO: Abbildung einfügen}


\section{Stabilität von Algorithmen}

Sei $E\subseteq\mathbb{R}^{n}$ und $f:E\to\mathbb{R}^{n}$.

Gesucht ist $y=f\left(x\right)$ für $x\in E$.

Auf dem Rechner stehen elementare Operationen wie $+,\,-,\,\cdot,\,/$
zur Verfügung, die mit einer relativen Genauigkeit $\varepsilon$
realisiert sind. Für alle $x\in\mathbb{F}$ gilt für jede elementare
Operation $\varphi$ also:
\begin{align*}
\varrho_{\overline{\varphi\left(x\right)}}\left(\varphi\left(x\right)\right) & \le\varepsilon
\end{align*}



\subsubsection*{Definition \textmd{(Algorithmus)}}

Eine Zerlegung der Abbildung $f:E\to\mathbb{R}^{n}$ der Form $f=f^{\left(l\right)}\circ\ldots\circ f^{\left(1\right)}$
mit $l\in\mathbb{N}$, $f^{\left(i\right)}U_{i}\to U_{i+1}$ ($U_{i}\subseteq\mathbb{R}^{k_{i}}$,
$\mathbb{R}^{k_{i+1}}$) und $k_{i}\in\mathbb{N}$, $k_{1}=k$ sowie
$k_{l+1}=n$ heißt \emph{Algorithmus}, falls alle $f^{\left(i\right)}$
durch elementare Operationen ausführbar sind.

Beispiel:
\begin{align*}
f:\mathbb{R} & \to\mathbb{R}\\
x & \mapsto\frac{x^{2}-1}{x^{2}+1}
\end{align*}
\begin{align*}
f^{\left(1\right)}:\mathbb{R} & \to\mathbb{R}\\
x & \mapsto x^{2}
\end{align*}
\begin{align*}
f^{\left(2\right)}:\mathbb{R} & \to\mathbb{R}^{2}\\
x & \mapsto\left(y-1,y+1\right)\\
f^{\left(3\right)}:\mathbb{R}^{2} & \to\mathbb{R}\\
\left(x,y\right) & \mapsto\frac{x}{y}
\end{align*}
\begin{align*}
f\left(x\right) & =\left(f^{\left(3\right)}\circ f^{\left(2\right)}\circ f^{\left(1\right)}\right)\left(x\right)
\end{align*}
Die Umsetzung auf dem Rechner
\begin{align*}
\overline{f} & =\overline{f}^{\left(b\right)}\circ\ldots\circ\overline{f}^{\left(1\right)}:\mathbb{F}^{k}\to\mathbb{F}^{n}
\end{align*}
heißt \emph{Implementation} von $f$.

Im Allgemeinen gibt es viele Möglichkeiten, $f$ in Elementare Operationen
aufzuspalten, das heißt viele Algorithmen.

\emph{Ziel:} Verstehe den Einfluss eines Algorithmus auf die Fehlerverstärkung.


\section{Lemma}

Sei $x\in\mathbb{R}^{k}$ und $\overline{x}\in\mathbb{F}^{k}$, sodass
$\varrho_{\overline{x}_{i}}\left(x_{i}\right)\le\varepsilon$ für
$i\in\left\{ 1,\ldots,k\right\} $ gilt. Sei weiter $f=f^{\left(l\right)}\circ\ldots\circ f^{\left(1\right)}$
ein Algorithmus zur Berechnung von $f$ und sei $\overline{f}=\overline{f}^{\left(l\right)}\circ\ldots\circ\overline{f}^{\left(1\right)}$
eine Implementierung. Mit den Abkürzungen
\begin{align*}
x^{\left(j+1\right)}: & =f^{\left(j\right)}\circ\ldots\circ f^{\left(1\right)}\left(x\right) & x^{\left(1\right)} & :=x\\
\overline{x}^{\left(j+1\right)}: & =\overline{f}^{\left(j\right)}\circ\ldots\circ\overline{f}^{\left(1\right)}\left(\overline{x}\right) & \overline{x}^{\left(1\right)} & :=\overline{x}
\end{align*}
gilt:
\begin{align*}
\varrho_{\overline{x}_{i}^{\left(j+1\right)}}\left(x_{i}^{j+1}\right) & \le\varepsilon+\sum_{m}\kappa_{m}\left(f_{i}^{\left(j\right)},x^{\left(j\right)}\right)\varrho_{\overline{x}_{m}^{\left(j\right)}}\left(x_{m}^{\left(j\right)}\right)+R
\end{align*}
Dabei ist $R$ ein Restterm, der quadratisch in den Fehlern und $\varepsilon$
ist. Es sei stets $f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)\not=0$
für alle $i,j$.


\subsubsection*{Beweis}

\begin{align*}
\varrho_{\overline{x}_{i}^{\left(j+1\right)}}\left(x_{i}^{\left(j+1\right)}\right) & =\frac{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)-\overline{f}_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)}}\le\\
 & \le\frac{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)-f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)}}+\frac{\abs{f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)-\overline{f}_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)}}\le\\
 & \le\frac{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)-f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)}}+\underbrace{\frac{\abs{f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)-\overline{f}_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}}_{\le\text{eps}}\cdot\underbrace{\frac{\abs{f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)}}}_{\le1+\frac{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)-f_{i}^{\left(j\right)}\left(\overline{x}^{\left(j\right)}\right)}}{\abs{f_{i}^{\left(j\right)}\left(x^{\left(j\right)}\right)}}}\le\\
 & \le\varrho_{f\left(\overline{x}\right)}\left(f\left(x\right)\right)+\text{eps}+R\le\\
 & \le\sum_{m}\kappa_{m}\left(f_{i}^{\left(j\right)},x^{\left(j\right)}\right)\varrho_{\overline{x}_{m}^{\left(j\right)}}\left(x_{m}^{\left(j\right)}\right)+\text{eps}+R
\end{align*}
\qqed


\section{Bemerkungen}

Nutze obiges Lemma rekursiv, um entstehende Fehler zu analysieren.
Insgesamt erhalte eine Abschätzung der Form:
\begin{align*}
\varrho_{\overline{f}\left(x\right)}\left(f\left(x\right)\right) & \le K\left(x\right)\varepsilon+R
\end{align*}
Dabei ist $K$ eine Größe, die sich aus Produkten der Konditionen
$\kappa_{m}\left(f_{i}^{\left(j\right)},x^{\left(j\right)}\right)$
zusammensetzt und $R$ ist quadratisch in $\varepsilon$.

Der Quotient $\frac{K\left(x\right)}{K\left(f,x\right)}$ ist ein
Indikator für die Güte (Stabilität) eines Algorithmus. Der Quotient
sagt aus, wie stark die Fehler durch den Algorithmus zusätzlich verstärkt
werden. Je kleiner der Quotient ist, desto besser ist der Algorithmus.


\section{Beispiel \textmd{(quadratische Gleichung)}}

Gesucht ist ein $x$ mit $x^{2}+2px-q=0$. Nehme $p,q\ge0$ und $p\gg q$
an. Die Nullstellen sind:
\begin{align*}
x_{\pm} & =-p\pm\sqrt{p^{2}+q}
\end{align*}
Ziel: Berechne die größere Nullstelle. Sei $f\left(p,q\right)=-p+\sqrt{p^{2}+q}$
und $w:=\sqrt{p^{2}+q}$. Betrachte zwei Algorithmen zur Berechnung
von $f$. Nutze die Identität $x_{+}\cdot x_{-}=-1$, mit der folgt:
\begin{align*}
f\left(p,q\right) & =\frac{-q}{-p-\sqrt{p^{2}+q}}
\end{align*}
Setze voraus, dass die Wurzelfunktion eine elementare Operation ist.
\begin{align*}
 & \text{Algorithmus }A &  & \text{Algorithmus }B\\
f_{1} & :=p\cdot p & q_{1} & :=p\cdot p\\
f_{2} & :=f_{1}+q & q_{2} & :=q_{1}+q\\
f_{3} & :=\sqrt{f_{m}} & q_{3} & :=\sqrt{q_{2}}\\
f_{4} & :=-p+f_{3} & q_{4} & :=-p-g_{3}\\
 &  & q_{5} & :=\frac{-q}{g_{4}}
\end{align*}
Zur Vereinfachung seien $p$ und $q$ Maschinenzahlen, also $\varrho_{\overline{p}}\left(p\right)=0$
und $\varrho_{\overline{q}}\left(q\right)=0$. Definiere:
\begin{align*}
\varrho_{i} & :=\varrho_{\overline{f}_{i}}\left(f_{i}\right) & \delta_{i} & :=\varrho_{\overline{g}_{i}}\left(g_{i}\right)
\end{align*}
Es gilt bei Vernachlässigung quadratischer Fehlerterme:
\begin{align*}
\varrho_{1} & \le\varepsilon+K\left(f_{1},p\right)\varrho_{\overline{p}}\left(p\right)=\varepsilon & \delta_{1} & \le\varepsilon\\
\varrho_{2} & \le\varepsilon+\underbrace{K\left(f_{2},f_{1}\right)}_{=\frac{\abs{f_{1}}}{\abs{f_{1}+q}}\le1}\varrho_{1}\le2\varepsilon & \delta_{2} & \le2\varepsilon\\
\varrho_{3} & \le\varepsilon+\underbrace{K\left(f_{3},f_{2}\right)}_{=\frac{\frac{1}{2}\frac{1}{\sqrt{f_{2}}}f_{2}}{\sqrt{f_{2}}}=\frac{1}{2}}\varrho_{2}\le2\varepsilon & \delta_{3} & \le2\varepsilon\\
\varrho_{4} & \le\varepsilon+\underbrace{K\left(f_{4},f_{3}\right)}_{=\frac{f_{3}}{-p+f_{3}}}\varrho_{3}\le\left(1+2\frac{\sqrt{pq}}{\sqrt{p^{2}+q}-p}\right)\varepsilon & \delta_{4} & \le\varepsilon+K\left(g_{4},g_{3}\right)\delta_{3}\le3\varepsilon\\
 &  & \delta_{5} & \le4\varepsilon
\end{align*}


%DATE: Mi 7.11.12

\begin{align*}
f\left(p,q\right) & =-p+\sqrt{p^{2}+q}
\end{align*}
Die Kondition von $f$ ist:
\begin{align*}
\kappa_{1}\left(f,\left(p,q\right)\right) & =\frac{\abs{\partial_{p}f}\cdot\abs p}{\abs f}=\abs{\frac{\left(-1+\frac{p}{\sqrt{p^{2}+q}}\right)p}{-p+\sqrt{p^{2}+q}}}=\abs{\frac{-p}{\sqrt{p^{2}+q}}\frac{\sqrt{p^{2}+q}-p}{-p+\sqrt{p^{2}+q}}}\le1\\
\kappa_{1}\left(f,\left(p,q\right)\right) & =\frac{\abs{\partial_{q}f}\cdot\abs q}{\abs f}=\frac{1}{2}\left(\frac{\abs p}{\sqrt{p^{2}+q}}+1\right)\le1
\end{align*}
Es folgt, dass das Berechnen von Nullstellen gut konditioniert ist.

Algorithmus A ist schlecht, da er die Fehler eines gut konditionierten
Problems stark verstärkt. ($p\gg q$)

Algorithmus B ist gut konditioniert. (Gesamtfehler ist gegen $4\text{eps}$
beschränkt.)


\section{Rückwärtsanalyse}

Bisher haben wir stets die Vorwärtsanalyse betrachtet:

Gesucht ist $f\left(x\right)$. Statt mit $x$ rechnen wir mit $\overline{x}\left(\in\mathbb{F}\right)$.

Berechne $f\left(\overline{x}\right)$ durch Implementation eines
Algorithmus:
\begin{align*}
\overline{f} & =\overline{f}^{\left(l\right)}\circ\ldots\circ\overline{f}^{\left(1\right)}
\end{align*}
Dann haben wir den relativen Fehler
\begin{align*}
\frac{\norm{\overline{f}\left(\overline{x}\right)-f\left(x\right)}}{\norm{f\left(x\right)}}
\end{align*}
untersucht.

Rückwärtsanalyse: Finde $\tilde{x}$, sodass $f\left(\tilde{x}\right)=\overline{f}\left(\overline{x}\right)$
und schätze $\norm{x-\tilde{x}}$ ab, das heißt:

Interpretiere das Ergebnis als exakte Lösung einer gestörten Eingabe.

Dazu ein Beispiel:


\section{Satz\label{sec:Satz-Rueckwaertsanalyse-LGS}}

Zur Lösung eines linearen Gleichungssystems $Ax=b$ mit $A\in\mathbb{R}^{n\times n}$
und $b,\ x\in\mathbb{R}^{n}$, sei die Gauß-Elimination (siehe \ref{sec:Algorithmus-zum-Eliminationsverfahren})
und die anschließende Auflösung des gestaffelten Gleichungssystems
(wie in \ref{sec:Der-Gau=0000DF-Algorithmus-mit-Pivotisierung}) auf
einem Rechner mit Maschinengenauigkeit eps implementiert.

Das Programm berechnet ein $\overline{x}\in\mathbb{R}^{n}$. Es gilt,
dass $\overline{x}$ Lösung von
\begin{align*}
\overline{A}\overline{x} & =b
\end{align*}
ist ($\overline{A}=\left(\overline{a}_{ij}\right);\ A=\left(a_{ij}\right)$)
mit:
\begin{align*}
\abs{\overline{a}_{ij}} & \le\gamma_{n}\left(2+\gamma_{n}\right)\abs{\overline{L}}\abs{\overline{R}}
\end{align*}
Dabei sind $\overline{L}$ und $\overline{R}$ die berechneten Dreiecksmatrizen
($\overline{L}=\left(\overline{l}_{ij}\right);\ \overline{R}=\left(\overline{r}_{ij}\right)$)
und:
\begin{align*}
\abs{\overline{L}} & =\max_{i,j}\abs{\overline{l}_{ij}} & \abs{\overline{R}} & =\max_{i,j}\abs{\overline{r}_{ij}} & \gamma_{n} & :=\frac{n\cdot\text{eps}}{1-n\cdot\text{eps}}
\end{align*}
Dabei sei vorausgesetzt, dass $n\cdot\text{eps}<1$ gilt.


\subsubsection{(ohne Beweis)}

Der Beweis steht im Buch von Deuflhard und Hohmann in Kapitel 2.4.

Üblicherweise ist $n$ sehr viel kleiner als $\frac{1}{\text{eps}}$.

Falls $\overline{L}$ und $\overline{R}$ große Einträge haben, löst
$\overline{x}$ ein Gleichungssystem mit möglicherweise stark veränderter
Matrix $\overline{A}$. (Formuliere dies positiv: Falls $\overline{L}$
und $\overline{R}$ kleine Einträge haben, löst $\overline{x}$ ein
Gleichungssystem mit schwach veränderter Matrix $\overline{A}$.)


\section{Bemerkung}

Die Gauß-Elimination (LR-Zerlegung) angewandt auf die Hilbert-Matrix
\begin{align*}
H & =\left(\frac{1}{i+j-1}\right)_{i,j\in\left\{ 1,\ldots,n\right\} }
\end{align*}
liefert für große $n$ sehr schlechte Ergebnisse. Die Ursache ist,
dass die Kondition
\begin{align*}
\kappa\left(H\right) & =\opnorm H\opnorm{H^{-1}}
\end{align*}
sehr groß ist, also gibt es eine große Fehlerverstärkung (vergleiche
\ref{sec:Kondition-linearer-Gleichungssysteme}).

Satz \ref{sec:Satz-Rueckwaertsanalyse-LGS} sagt: Wenn $\overline{L}$
und $\overline{R}$ keine zu großen Einträge haben, löst $\overline{x}$
eine Gleichung $\overline{H}\cdot\overline{x}=b$, wobei $\overline{H}$
nahe $H$ ist.

Es ist leider eine Tatsache, dass bei der Hilbert-Matrix $\overline{L}$
und $\overline{R}$ große Einträge haben.


\section{Einige Grundregeln, die sich aus diesem Kapitel ergeben.}
\begin{itemize}
\item Kenntnisse über die Kondition eines Problems sind entscheidend für
die Bewertung der Ergebnisse.
\item Multiplikationen und Divisionen sind gut konditioniert.
\item Subtraktion zweier annähernd gleicher Zahlen ist schlecht konditioniert
(Auslöschung). Vermeide dies nach Möglichkeit!
\item Addition von Zahlen mit gleichem Vorzeichen ist gut konditioniert.
\item Unvermeidliche, schlecht konditionierte Elementaroperationen sollte
man meist möglichst früh im Algorithmus durchführen, zum Beispiel
unvermeidliche Subtraktionen. Dann wirkt die Fehlerverstärkung noch
auf kleinere Fehler.
\item Bei einem stabilen Algorithmus bleiben die im Laufe der Rechnung erzeugten
Fehler in der Größenordnung der durch die Kondition bedingten unvermeidlichen
Fehler.
\item Vermeide Abfragen, ob eine Größe gleich Null ist, oder ob zwei Größen
gleich sind. (Dies ist wegen Rundungsfehlern nicht aussagekräftig.)
\item Beachte vorteilhafte Reihenfolge bei Summation: erst die kleinen,
dann die großen.
\end{itemize}

\chapter{\texorpdfstring{$QR$}{QR}-Zerlegung, lineare Ausgleichsprobleme}


\section{Einführende Bemerkungen zur \texorpdfstring{$QR$}{QR}-Zerlegung}

Eine $\left(n\times n\right)$-Matrix $A$ besitze eine Zerlegung
$A=QR$ mit einer orthogonalen $\left(n\times n\right)$-Matrix $Q$
und einer oberen Dreiecksmatrix $R$. $Q$ orthogonal heißt:
\begin{align*}
QQ^{\TT} & =\mathbbm{1}\\
\Leftrightarrow\quad Q^{-1} & =Q^{\TT}
\end{align*}
Löse das Gleichungssystem $Ax=b$ wie folgt:
\begin{enumerate}
\item Löse $Qz=b$, oder äquivalent $z=Q^{-1}b=Q^{\TT}b$. Also ist $z$
hier einfach berechenbar.
\item Löse $Rx=z$. (gestaffeltes Gleichungssystem)
\end{enumerate}
Einfache orthogonale Abbildungen (Matrizen) sind Drehungen und Spiegelungen.

Ziel: Multipliziere $A$ sukzessive mit Drehungen oder Spiegelungen,
bis eine obere Dreiecksmatrix vorliegt.
\begin{align*}
A & \leadsto Q^{\left(1\right)}A\leadsto Q^{\left(2\right)}Q^{\left(1\right)}A\leadsto\ldots\leadsto\underbrace{Q^{\left(m\right)}\ldots Q^{\left(1\right)}}_{=Q^{\TT}}A=R
\end{align*}



\section{Hyperebenen-Spiegelungen}

Sei $u\in\mathbb{R}^{n}$ mit $\norm u=1$.
\begin{align*}
H_{u} & =\left\{ x\in\mathbb{R}^{n}\big|x\cdot u=0\right\} 
\end{align*}
sei die Hyperebene, die zu $u$ senkrecht steht.

\textcolor{green}{TODO: Abbildung 2D-Hyperebene}

Gesucht ist eine Matrix $Q$, die die Spiegelung an $H_{u}$ beschreibt.

Eine Spiegelung $Q$ an der Hyperebene $H_{u}$ ist definiert durch
die Eigenschaften:
\begin{align*}
Qx & =x\qquad\fall_{x\in H_{u}}\\
Qu & =-u
\end{align*}
Sei $x\in\mathbb{R}^{n}$ beliebig, so zerlege in einen Anteil parallel
zu $u$ und einen senkrechten Anteil:
\begin{align*}
x & =\alpha u+\left(x-\alpha u\right)
\end{align*}
Bestimme $\alpha$ so, dass $\left(x-\alpha u\right)\cdot u=0$ ist,
das heißt $\alpha=x\cdot u$, womit folgt:
\begin{align*}
x & =\left(x\cdot u\right)u+\underbrace{\left(x-\left(x\cdot u\right)u\right)}_{\in H_{u}}
\end{align*}
Also gilt:
\begin{align*}
Qx & =-\left(x\cdot u\right)u+\left(x-\left(x\cdot u\right)u\right)=x-2\left(x\cdot u\right)u=\\
 & =\mathbbm{1}x-2\left(uu^{\TT}\right)x
\end{align*}
In Matrixform ist das:
\begin{align*}
Q & =\mathbbm{1}-2\left(uu^{\TT}\right)
\end{align*}
Matrizen dieser Form heißen Householder-Matrizen.

Bemerkung: $u$ und $-u$ definieren die gleiche Spiegelung.


\section{Lemma}

Für $u\in\mathbb{R}^{n}\setminus\left\{ 0\right\} $ mit $\norm u=1$
und $Q=\mathbbm{1}-2uu^{\TT}$ gilt:
\begin{align*}
Q & =Q^{\TT} &  & \text{(symmetrisch)}\\
Q^{-1} & =Q^{\TT} &  & \text{(orthogonal)}\\
Q^{2} & =\mathbbm{1} &  & \text{(involutorisch)}
\end{align*}



\subsubsection*{Beweis}

\begin{align*}
Q^{\TT} & =\mathbbm{1}-2\left(uu^{\TT}\right)^{\TT}=\mathbbm{1}-2\left(uu^{\TT}\right)=Q\\
Q^{2}=QQ^{\TT} & =\left(\mathbbm{1}-2\left(uu^{\TT}\right)\right)\left(\mathbbm{1}-2\left(uu^{\TT}\right)\right)=\\
 & =\mathbbm{1}-2\left(uu^{\TT}\right)-2\left(uu^{\TT}\right)+4u\underbrace{u^{\TT}u}_{=1}u^{\TT}=\mathbbm{1}
\end{align*}
\qqed


\section{\texorpdfstring{$QR$}{QR}-Zerlegung mit Householder-Matrizen}

Sei $A\in\mathbb{R}^{m\times n}$, das heißt:
\begin{align*}
A & =\left(\begin{array}{ccc}
a_{11} & \ldots & a_{1n}\\
\vdots & \ddots & \vdots\\
a_{m1} & \ldots & a_{mn}
\end{array}\right)
\end{align*}
Ziel: Finde Spiegelung $Q^{\left(1\right)}\in\mathbb{R}^{m\times m}$,
sodass gilt:
\begin{align*}
Q^{\left(1\right)}A & =\left(\begin{array}{cccc}
* & \ldots & \ldots & *\\
0 & * &  & \vdots\\
\vdots & \vdots & \ddots & \vdots\\
0 & * & \ldots & *
\end{array}\right)
\end{align*}
Sei
\begin{align*}
a_{j} & =\left(\begin{array}{c}
a_{1j}\\
\vdots\\
a_{nj}
\end{array}\right)
\end{align*}
die $j$-te Spalte der Matrix $A$. Dann muss gelten:
\begin{align*}
Q^{\left(1\right)}a_{1} & =\pm\norm{a_{1}}e_{1}
\end{align*}
$a_{1}$ muss auf einen Vektor der Länge $\norm{a_{1}}$ abgebildet
werden, da $Q^{\left(1\right)}$ orthogonal sein soll.

\textcolor{green}{TODO: Abb2 einfügen}

Definiere:
\begin{align*}
u_{1} & =\pm\frac{a_{1}\pm\norm{a_{1}}e_{1}}{\norm{a_{1}\pm\norm{a_{1}}e_{1}}}
\end{align*}
Wähle:
\begin{align*}
v_{1} & =\frac{a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}}{\norm{a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}}}
\end{align*}
Diese Vorzeichenwahl dient der Vermeidung einer möglicherweise schlechten
Konditionierung. Nun definiere:
\begin{align*}
Q^{\left(1\right)} & =\mathbbm{1}-2v_{1}v_{1}^{\TT}
\end{align*}
%DATE: Mo 12.11.12
\begin{align*}
Q^{\left(1\right)}a_{1} & =a_{1}-2v_{1}v_{1}^{\TT}a_{1}=a_{1}-2\frac{a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}}{\norm{a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}}}\left(\frac{a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}}{\norm{a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}}}\right)^{\TT}a_{1}=\\
 & =a_{1}-2\left(a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}\right)\cdot\frac{\norm{a_{1}}^{2}+\abs{a_{11}}\cdot\norm{a_{1}}}{\norm{\left(a_{11}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}\right)^{2}+\norm{a_{1}}^{2}-a_{11}^{2}}^{2}}=\\
 & =a_{1}-2\left(a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}\right)\cdot\frac{\norm{a_{1}}^{2}+\abs{a_{11}}\cdot\norm{a_{1}}}{2\left(\abs{a_{11}}\norm{a_{1}}+\norm{a_{1}}^{2}\right)}=\\
 & =a_{1}-\left(a_{1}+\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}\right)=-\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}
\end{align*}
Wir erhalten: $A^{\left(2\right)}:=Q^{\left(1\right)}\cdot A^{\left(1\right)}$
mit $A^{\left(1\right)}=A$. Mit
\begin{align*}
\alpha_{1} & =-\text{sgn}\left(a_{11}\right)\norm{a_{1}}e_{1}
\end{align*}
gilt: 
\begin{align*}
A^{\left(2\right)} & =\begin{pmatrix}\alpha_{1}\\
0\\
\vdots & Q^{(1)}a_{2} & \dots & Q^{(1)}a_{n}\\
0
\end{pmatrix}
\end{align*}
Iteration liefert: 
\begin{align*}
A^{(k)} & =\begin{pmatrix}\alpha_{1}\\
0 & \ddots\\
\vdots & \ddots & \ddots\\
\vdots &  & \ddots & \alpha_{k-1}\\
\vdots &  &  & 0\\
\vdots &  &  & \vdots & \tilde{a}_{k+1}^{(k)} & \dots & \tilde{a}_{n}^{(k)}\\
0 & \dots & \dots & 0
\end{pmatrix}=\left(\begin{array}{cccc}
\alpha_{1} &  &  & *\\
 & \ddots\\
 &  & \alpha_{k-1}\\
0 &  &  & B^{\left(k\right)}
\end{array}\right)
\end{align*}
$B^{\left(k\right)}$ ist eine $\left(m-k+1\right)\times\left(n-k+1\right)$-Matrix.
Wähle $\left(m-k+1\right)\times\left(m-k+1\right)$-Spiegelungs-Matrix
$\tilde{Q}^{(k)}$ sodass gilt: 
\begin{align*}
\tilde{Q}^{\left(k\right)}B^{(k)} & =\begin{pmatrix}* & \dots & \dots & *\\
0 & * & \ldots & *\\
\vdots & \vdots & \ddots & \vdots\\
0 & * & \dots & *
\end{pmatrix}
\end{align*}
Dazu sei $v_{k}\in\mathbb{R}^{m-k+1}$ mit:
\begin{align*}
v_{k} & :=\frac{\tilde{a}_{k}^{(k)}-\alpha_{k}e_{k}^{(k)}}{\norm{\tilde{a}_{k}^{(k)}-\alpha_{k}e_{k}^{(k)}}}\\
\alpha_{k} & :=-\text{sgn}\left(\tilde{a}_{kk}^{(k)}\right)\norm{\tilde{a}_{k}^{(k)}}\\
e_{1}^{(k)} & :=\left(1,0,\ldots,0\right)^{\TT}\in\mathbb{R}^{m-k+1}\\
\tilde{Q}^{\left(k\right)} & :=1-2v_{k}v_{k}^{\TT}
\end{align*}
Definiere nun: 
\begin{align*}
Q^{(k)} & =\begin{pmatrix}\mathbbm{1}_{k-1} & 0\\
0 & \underset{n-k+1\text{ Spalten}}{\tilde{Q}^{(k)}}
\end{pmatrix}
\end{align*}
Das Verfahren endet, wenn $k=p:=\min\left(n,m-1\right)$ ist. Wir
erhalten $R=A^{(p)}=Q^{(p)}\cdot\ldots\cdot Q^{(1)}A$. Es gilt: 
\begin{align*}
\left(Q^{(k)}\right)^{-1} & =Q^{k}\\
\Rightarrow\qquad A & =Q^{(1)}\cdot\ldots\cdot Q^{(p)}R=QR
\end{align*}



\section{Algorithmus}

Das Vorgehen bei der Berechnung der $QR$-Zerlegung ist im $k$-ten
Schritt:
\begin{enumerate}
\item $\alpha_{k}=-\text{sgn}\left(\tilde{a}_{kk}^{(k)}\right)\norm{\tilde{a}_{k}^{(k)}}$
\item $h_{k}=\tilde{a}_{k}^{(k)}-\alpha_{k}e_{k}$
\item $\varepsilon:=\norm{h_{k}}^{2}=2\left(\norm{\tilde{a}_{k}^{(k)}}^{2}+\abs{\tilde{a}_{kk}^{(k)}}\norm{\tilde{a}_{k}^{(k)}}\right)$
\item $\tilde{Q}^{(k)}\tilde{a}_{j}^{(k)}=\tilde{a}_{j}^{(k)}-\frac{2}{\varepsilon}h_{k}h_{k}^{T}\cdot a_{j}^{(k)}$
\end{enumerate}
Statt $\varepsilon$ berechne für den Algorithmus $\beta=\alpha_{k}\left(a_{kk}^{(k)}-\alpha_{k}\right)$
und damit $\tilde{Q}^{(k)}\tilde{a}_{j}^{(k)}=a_{j}^{(k)}+\frac{h_{k}\cdot a_{j}^{(k)}}{\beta}h_{k}$. 


\section{Algorithmus\label{sec:Algorithmus-QR-Zerlegung}}
\begin{quote}
\texttt{input($\left(a_{ij}\right)_{i\in\left\{ 1,\ldots,m\right\} ,j\in\left\{ 1,\ldots,n\right\} }$)}

\texttt{for ($k=1,\ldots,\min\left(n,m-1\right)$) \{}

\texttt{\hspace*{1cm}$\alpha_{k}=a_{kk}^{2}$}

\texttt{\hspace*{1cm}for ($j=k+1,\ldots,m$)}

\texttt{\hspace*{2cm}\{$\alpha_{k}=\alpha_{k}+a_{jk}^{2}$\}}

\texttt{\hspace*{1cm}$\alpha_{k}=-\text{sgn}\left(a_{kk}\right)\sqrt{\alpha_{k}}$}

\texttt{\hspace*{1cm}$a_{kk}=a_{kk}-\alpha_{k}$}

\texttt{\hspace*{1cm}$\beta=\alpha_{k}a_{kk}$}

\texttt{\hspace*{1cm}for ($i=k+1,\ldots,n$) \{}

\texttt{\hspace*{2cm}$\gamma=a_{kk}\cdot a_{ki}$}

\texttt{\hspace*{2cm}for ($j=k+1,\ldots,m$)}

\texttt{\hspace*{3cm}\{$\gamma=\gamma+a_{jk}\cdot a_{ji}$\}}

\texttt{\hspace*{2cm}$\delta=\frac{\gamma}{\beta}$}

\texttt{\hspace*{2cm}for ($j=k,\ldots,m$)}

\texttt{\hspace*{3cm}\{$a_{ji}=a_{ji}+\delta\cdot a_{jk}$\}}

\texttt{\hspace*{2cm}\}}

\texttt{\hspace*{1cm}\}}

\texttt{output ($\left(a_{ij}\right)_{i\in\left\{ 1,\ldots,m\right\} ,j\in\left\{ 1,\ldots,n\right\} }$,
$\left(\alpha\right)_{k=1,\ldots,\text{min}\left(n,m-1\right)}$)}
\end{quote}
Die $QR$-Zerlegung nach dem Algorithmus kann wie folgt abgespeichert
werden:
\begin{align*}
\left(\fbox{\ensuremath{\begin{array}{c}
\\
\\
h_{1}\\
\\
\\
\end{array}}}\raisebox{-7pt}{\fbox{\ensuremath{\begin{array}{c}
\\
h_{2}\\
\\
\\
\end{array}}}}\raisebox{-13.5pt}{\fbox{\ensuremath{\begin{array}{c}
\\
h_{3}\\
\\
\end{array}}}}\raisebox{-21pt}{\ensuremath{\begin{array}{c}
\ddots\\
\\
\end{array}}}\raisebox{-28pt}{\fbox{\ensuremath{\begin{array}{c}
h_{n-1}\end{array}}}}\right) & =\left(a_{ij}\right)
\end{align*}
$h_{k}$ geht bis zur Diagonale hinauf. Über der Diagonale wir $R$
in die Matrix $\left(a_{ij}\right)$ gespeichert. Die Diagonale von
$R$ wird in einem Extravektor $\left(\alpha_{k}\right)_{k\in\left\{ 1,\ldots,\min\left(n,m-1\right)\right\} }$
gespeichert, also $\alpha_{k}=r_{kk}$.


\section{Aufwand der $QR$-Zerlegung}

Sei $p=\min\left(n,m-1\right)$. Der Aufwand ist:
\begin{itemize}
\item Additionen:
\begin{align*}
 & \sum_{k=1}^{p}\left(\left(m-k-1\right)+1+\left(n-k-1\right)\left(\left(m-k-1\right)+\left(m-k\right)\right)\right)=\\
 & \qquad=\sum_{k=1}^{p}\left(m-k+\left(n-k-1\right)\left(2\left(m-k\right)-1\right)\right)
\end{align*}

\item Multiplikationen:
\begin{align*}
 & \sum_{k=1}^{p}\left(1+\left(m-k-1\right)+3+\left(n-k-1\right)\left(1+\left(m-k-1\right)+1+\left(m-k\right)\right)\right)=\\
 & \qquad=\sum_{k=1}^{p}\left(m-k+3+\left(n-k-1\right)\left(2\left(m-k\right)+1\right)\right)
\end{align*}

\item Wurzeln: $p$
\end{itemize}
Für $n=m$, also $p=m-1=n-1$, lässt sich der Aufwand mit Hilfe von
Summenformeln berechnen. Die Anzahl von Additionen und Multiplikationen
ist:
\begin{align*}
 & \sum_{k=1}^{p}\left(2\left(m-k\right)+3+4\left(n-k-1\right)\left(m-k\right)\right)=\\
 & \qquad\qquad\qquad=\sum_{k=1}^{n-1}\left(3+2\left(2\left(n-k\right)-1\right)\left(n-k\right)\right)=\\
 & \qquad\qquad\qquad=3\left(n-2\right)+\sum_{k=1}^{n-1}4\left(n-k\right)^{2}-2\left(n-k\right)\stackrel{l=n-k}{=}3\left(n-2\right)+\sum_{l=1}^{n-1}4l^{2}-2l=\\
 & \qquad\qquad\qquad=3\left(n-2\right)+4\frac{\left(n-1\right)n\left(2\left(n-1\right)+1\right)}{6}-2\frac{\left(n-1\right)\left(n-2\right)}{2}=\\
 & \qquad\qquad\qquad=\left(4-n\right)\left(n-2\right)+\frac{2}{3}\left(n-1\right)n\left(2n-1\right)=\\
 & \qquad\qquad\qquad=\left(4n-n^{2}-8+2n\right)+\frac{2}{3}\left(2n^{2}-2n-n+1\right)n=\\
 & \qquad\qquad\qquad=-n^{2}+6n-8+\frac{4}{3}n^{3}-2n^{2}+\frac{2}{3}n=\\
 & \qquad\qquad\qquad=\frac{4}{3}n^{3}-3n^{2}+\frac{20}{3}n-8=\frac{4}{3}n^{3}+\mathcal{O}_{\infty}\left(n^{2}\right)
\end{align*}
Die Anzahl der Wurzeln ist $n-1$.

Ist $m\gg n$, so gilt für den Aufwand in flops, also Multiplikationen
und Additionen:
\begin{align*}
 & \sum_{k=1}^{p}\left(2\left(m-k\right)+3+4\left(n-k-1\right)\left(m-k\right)\right)=\\
 & \qquad\qquad\qquad=\sum_{k=1}^{n}\left(3+2\left(2\left(n-k\right)-1\right)\left(m-k\right)\right)=\\
 & \qquad\qquad\qquad=3\left(n-1\right)+\sum_{k=1}^{n}4\left(n-k\right)\left(m-k\right)-2\left(m-k\right)=\\
 & \qquad\qquad\qquad=3\left(n-1\right)+\sum_{k=1}^{n}4nm-4k\left(m+n\right)+4k^{2}-2m-2k=\\
 & \qquad\qquad\qquad=3\left(n-1\right)+\sum_{k=1}^{n}4k^{2}-2k\left(2\left(m+n\right)+1\right)+4nm-2m=\\
 & \qquad\qquad\qquad=\left(3+2m\left(2n-1\right)\right)\left(n-1\right)+4\frac{n\left(n+1\right)\left(2n+1\right)}{6}-2\frac{n\left(n+1\right)}{2}\left(2\left(m+n\right)+1\right)=
\end{align*}
\begin{align*}
 & \qquad\qquad\qquad=2m\left(2n-1\right)\left(n-1\right)-2n\left(n+1\right)m-2n\left(n+1\right)n+\\
 & \qquad\qquad\qquad\qquad+\frac{2}{3}n\left(n+1\right)\left(2n+1\right)+3\left(n-1\right)-n\left(n+1\right)=\\
 & \qquad\qquad\qquad=2m\left(2n^{2}-n-2n+1-n^{2}-n\right)-2n^{3}+2n^{2}+\\
 & \qquad\qquad\qquad\qquad+\frac{4}{3}n^{3}+\frac{6}{3}n^{2}+\frac{2}{3}n+3n-3-n^{2}-n=\\
 & \qquad\qquad\qquad=2m\left(n^{2}-4n+1\right)-\frac{2}{3}n^{3}+3n^{2}+\frac{8}{3}n-3=2mn^{2}+o_{\infty}\left(mn^{2}\right)
\end{align*}



\section{Konstruktion orthogonaler Matrizen}

Sei $Q\in\mathbb{R}^{n\times n}$ orthogonal. Dann gilt $K_{2}\left(Q\right)=1$.
Dabei sei:
\begin{align*}
K_{2}\left(Q\right) & =\opnorm Q_{2}\cdot\opnorm{Q^{-1}}_{2}
\end{align*}



\subsubsection*{Beweis}

Da $Q$ orthogonal ist, gilt dies auch für $Q^{-1}$. Für $x\in\mathbb{R}^{n}$
gilt somit $\norm{Qx}_{2}=\norm x_{2}=\norm{Q^{-1}x}_{2}$. Damit
folgt:
\begin{align*}
\opnorm Q_{2} & =\max_{x\in\mathbb{R}^{n}}\frac{\norm{Qx}_{2}}{\norm x_{2}}=\max_{x\in\mathbb{R}^{n}}\frac{\norm x_{2}}{\norm x_{2}}=1\\
\opnorm{Q^{-1}}_{2} & =\max_{x\in\mathbb{R}^{n}}\frac{\norm{Q^{-1}x}_{2}}{\norm x_{2}}=\max_{x\in\mathbb{R}^{n}}\frac{\norm x_{2}}{\norm x_{2}}=1
\end{align*}
Also gilt $K_{2}\left(Q\right)=1\cdot1=1$.\qqed

Orthogonale Matrizen sind also sehr gut konditioniert. Die $QR$-Zerlegung
ist daher stabiler als die $LR$-Zerlegung.


\section{Methode der kleinsten Fehlerquadrate}

Gegeben seien $m$ Messpunkte $\left(t_{i},b_{i}\right)\in\mathbb{R}^{2}$
für $i\in\left\{ 1,\ldots,m\right\} $, die Zustände eines Objektes
in verschiedenen Situationen $t_{i}$ beschreibt.

\emph{Annahme:} Den Messungen liegt eine Gesetzmäßigkeit zu Grunde:
\begin{align}
b\left(t\right) & =\varphi\left(t;x_{1},\ldots,x_{n}\right)\label{eq:Gesetz}
\end{align}
Dabei sind $x_{1},\ldots,x_{n}\in\mathbb{R}$ die $n$ unbekannten
freien Parameter.

Falls es Messfehler gibt, wird für $i\in\left\{ 1,\ldots,m\right\} $
nur gelten:
\begin{align*}
b_{i} & \approx\varphi\left(t_{i};x_{1},\ldots,x_{n}\right)
\end{align*}
\emph{Ziel:} Bestimme die Parameter $x_{1},\ldots,x_{n}$ so, dass
die Abstände zwischen den $b_{i}$ und $\varphi\left(t_{i};x_{1},\ldots,x_{n}\right)$
möglichst klein werden.
\begin{align*}
b & =\left(\begin{array}{c}
b_{1}\\
\vdots\\
b_{m}
\end{array}\right) & f\left(x_{1},\ldots,x_{n}\right) & =\left(\begin{array}{c}
f_{1}\\
\vdots\\
f_{m}
\end{array}\right):=\left(\begin{array}{c}
\varphi\left(t_{1};x_{1},\ldots,x_{n}\right)\\
\vdots\\
\varphi\left(t_{m};x_{1},\ldots,x_{n}\right)
\end{array}\right)
\end{align*}
Bestimme nun $x_{1},\ldots,x_{n}$ so, dass $\norm{b-f\left(x_{1},\ldots,x_{n}\right)}$
minimal wird.

Frage: Welche Norm auf $\mathbb{R}^{n}$ sollen wir wählen?

Wir nehmen die euklidische, da die Wahrscheinlichkeitstheorie zeigt,
dass dies eine gute Wahl ist, falls die Fehler normalverteilt sind.
Zudem lässt sich
\begin{align*}
\norm{b-f\left(x_{1},\ldots,x_{n}\right)}^{2}
\end{align*}
einfach differenzieren.

%DATE: Mi 14.11.12

Betrachte die Funktion:
\begin{align*}
b\left(t\right) & =\varphi\left(t;x_{1},\ldots,x_{n}\right)=a_{1}\left(t\right)x_{1}+\ldots+a_{n}\left(t\right)x_{n}
\end{align*}
Die $a_{k}$ können auch nichtlinear in $t$ sein.


\section{Lineare Ausgleichsprobleme}

Es sei $\varphi\left(t;x_{1},\ldots,x_{n}\right)=a_{1}\left(t\right)x_{1}+\ldots+a_{n}\left(t\right)x_{n}$
mit $a_{i}:\mathbb{R}\to\mathbb{R}$ für $i\in\left\{ 1,\ldots,n\right\} $.

Problem (P): Gesucht sind $x_{1},\ldots,x_{n}\in\mathbb{R}$, sodass
\begin{align*}
\norm{b-f\left(x_{1},\ldots,x_{n}\right)}^{2} & =\sum_{i=1}^{m}\left(b_{i}-\sum_{j=1}^{n}a_{j}\left(t_{i}\right)x_{j}\right)^{2}
\end{align*}
minimal wird. (Dabei ist $\norm .$ die euklidische Norm.)

Wir führen die Matrix $A=\left(a_{ij}\right)_{i\in\left\{ 1,\ldots,m\right\} ,j\in\left\{ 1,\ldots,n\right\} }\in\mathbb{R}^{m\times n}$
mit $a_{ij}:=a_{j}\left(t_{i}\right)$ ein. (P) kann umformuliert
werden:

Problem (P'): Gesucht ist ein Vektor $x\in\mathbb{R}^{n}$, sodass
$\norm{b-Ax}^{2}$ minimal ist. Rechnung dazu:
\begin{align*}
\norm{b-Ax}^{2} & =\sum_{i=1}^{m}\left(b_{i}-\sum_{j=1}^{n}a_{ij}x_{j}\right)^{2}=\sum_{i=1}^{m}\left(b_{i}-\sum_{j=1}^{n}a_{j}\left(t_{i}\right)x_{j}\right)^{2}
\end{align*}
Ab jetzt sei $m\ge n$, das heißt es liegen mehr Messungen vor, als
es Parameter gibt.


\section{Geometrische Interpretation des linearen Ausgleichsproblems}

(P') heißt: Suche den Punkt im Bild von $A$, der zu $b$ minimalen
Abstand hat.

Beispiel für $m=2$ und $n=1$: 
\begin{align*}
A:\mathbb{R} & \to\mathbb{R}^{2}\\
x & \mapsto\left(\begin{array}{c}
a_{11}x\\
a_{21}x
\end{array}\right)
\end{align*}
Das Bild von $A$ ist eine Gerade. Die Geometrische Anschauung ist,
dass $b-Ax$ senkrecht auf $\text{im}\left(A\right)$ steht.

\textcolor{green}{TODO: Abb3 einfügen}

Betrachte dies im allgemeinen Kontext.


\section{Satz \textmd{(Projektionssatz)\label{sec:Satz-Projektionssatz}}}

Sei $V$ ein reeller Vektorraum mit Skalarprodukt $\left\langle .,.\right\rangle $,
sei $U\subseteq V$ ein endlich-dimensionaler Unterraum und sei
\begin{align*}
U^{\perp} & :=\left\{ v\in V\big|\left\langle v,u\right\rangle =0\quad\fall_{u\in U}\right\} 
\end{align*}
sein orthogonales Komplement in $V$. Weiter sei $\norm v:=\sqrt{\left\langle v,v\right\rangle }$
die vom Skalarprodukt induzierte Norm.
\begin{enumerate}
\item Dann existiert zu jedem $v\in V$ genau ein $Pv\in U$, sodass gilt:
\begin{align*}
\norm{v-Pv} & =\text{min}_{u\in U}\norm{v-u}
\end{align*}
\textcolor{green}{TODO: Abb4 einfügen}
\item $P:V\to U$ ist linear und $v-Pv\in U^{\perp}$.
\item Zu jedem $v\in V$ gibt es genau ein $\overline{u}\in U$ mit der
Eigenschaft $v-\overline{u}\in U^{\perp}$, das heißt es gilt $Pv=\overline{u}$.
\end{enumerate}
\emph{Bemerkung:} Zur Anschauung betrachte $V=\mathbb{R}^{n}$ mit
euklidischem Skalarprodukt.


\subsubsection*{Beweis}

Sei $v\in V$.
\begin{enumerate}[label=\alph*)]
\item 

\begin{description}
\item [{Behauptung:}] Falls für $\overline{u}\in U$ schon
\begin{align*}
\left\langle v-\overline{u},u\right\rangle  & =0\qquad\fall_{u\in U}
\end{align*}
gilt, so ist $\overline{u}$ eindeutig bestimmtes Minimum:
\begin{align*}
\min_{u\in U}\norm{v-u} & =\norm{v-\overline{u}}
\end{align*}

\item [{Beweis:}] Für alle $\tilde{u}\in U$ gilt:
\begin{align*}
\norm{v-\tilde{u}}^{2} & =\norm{v-\overline{u}+\overline{u}-\tilde{u}}^{2}=\left\langle v-\overline{u}+\overline{u}-\tilde{u},v-\overline{u}+\overline{u}-\tilde{u}\right\rangle =\\
 & =\norm{v-\overline{u}}^{2}+2\underbrace{\left\langle v-\overline{u},\overline{u}-\tilde{u}\right\rangle }_{=0}+\norm{\tilde{u}-\overline{u}}^{2}=\norm{v-\overline{u}}^{2}+\norm{\tilde{u}-\overline{u}}^{2}
\end{align*}
(Dies ist der Satz des Pythagoras.) Es folgt:
\begin{align*}
\norm{v-\tilde{u}}^{2} & -\norm{v-\overline{u}}^{2}=\norm{\tilde{u}-\overline{u}}^{2}\ge0
\end{align*}
Also ist $\overline{u}$ das Minimum und eindeutig.\qqed[Behauptung]\\
\textcolor{green}{TODO: Abb5 einfügen}
\end{description}

\begin{align*}
\left(v-\tilde{u}\right) & \perp\left(\overline{u}-\tilde{u}\right)
\end{align*}
Existiert ein $\overline{u}$ mit obiger Eigenschaft?

\item 

\begin{description}
\item [{Behauptung:}] Es existiert ein $\overline{u}\in U$ sodass für
alle $u\in U$ gilt:
\begin{align*}
\left\langle v-\overline{u},u\right\rangle  & =0
\end{align*}

\item [{Beweis:}] Sei $\left\{ u_{1},\ldots,u_{n}\right\} $ eine Orthonormalbasis
von $U$. Setze
\begin{align*}
\overline{u} & =\sum_{i=1}^{n}\alpha_{i}u_{i}
\end{align*}
mit $\alpha_{i}\in\mathbb{R}$, womit folgt:
\begin{align*}
\left\langle v-\overline{u},u\right\rangle  & =0\quad\fall_{u\in U}\\
\Leftrightarrow\quad\left\langle v,u_{j}\right\rangle  & =\sum_{i=1}^{n}\alpha_{i}\underbrace{\left\langle u_{i},u_{j}\right\rangle }_{=\delta_{ij}}\quad\fall_{j}\\
\left\langle v,u_{j}\right\rangle  & =\alpha_{j}
\end{align*}
Also gilt:
\begin{align*}
\overline{u} & =\sum_{i=1}^{n}\left\langle v,u_{i}\right\rangle u_{i}
\end{align*}
Definiere nun $Pv=\overline{u}$, sodass gilt:
\begin{align*}
\left\langle v-Pv,u\right\rangle  & =0\quad\fall_{u\in U}
\end{align*}
Das heißt $v-Pv\in U^{\perp}$.\\
Zu zeigen ist die Linearität von $P$. Seien $v_{1}$ und $v_{2}\in V$.
\begin{align*}
\left\langle v_{i}-Pv_{i},u\right\rangle  & =0\quad\fall_{u\in U};\ i\in\left\{ 1,2\right\} \\
\Rightarrow\quad\left\langle v_{1}+v_{2}-\left(Pv_{1}+Pv_{2}\right),u\right\rangle  & =0\quad\fall_{u\in U}\\
\Rightarrow\quad P\left(v_{1}+v_{2}\right) & =Pv_{1}+Pv_{2}
\end{align*}
Dies folgt aus der Eindeutigkeit von $P\left(v_{1}+v_{2}\right)$.\qqed[Behauptung]
\end{description}
\end{enumerate}
\qqed


\section{Bemerkung \textmd{(orthogonale Projektion)}}

Die Abbildung $P:V\to U$ heißt \emph{orthogonale Projektion} von
$V$ auf $U$. Es gilt $P^{2}=P$.


\section{Lemma und Definition \textmd{(Normalengleichungen)}}

Die Voraussetzungen seien wie in \ref{sec:Satz-Projektionssatz}.

Sei $v\in V$ und sei $\overline{u}=Pv\in U$. Sei nun $\left\{ w_{1},\ldots,w_{n}\right\} $
eine Basis (nicht notwendig orthogonal) von $U$ und sei:
\begin{align*}
\overline{u} & =\sum_{i=1}^{n}\beta_{i}w_{i}
\end{align*}
Dann erfüllen die $\beta_{i}$ folgendes lineare Gleichungssystem,\emph{
Normalengleichungen} genannt:
\begin{align*}
\sum_{i=1}^{n}\beta_{i}\left\langle w_{i},w_{j}\right\rangle  & =\left\langle v,w_{j}\right\rangle 
\end{align*}
Hintergrund des Begriffs:

\textcolor{green}{TODO: Abb6 einfügen}


\subsubsection*{Beweis}

$\overline{u}$ erfüllt für alle $j\in\left\{ 1,\ldots,n\right\} $:
\begin{align*}
0 & =\left\langle v-\overline{u},w_{j}\right\rangle =\left\langle v,w_{j}\right\rangle -\sum_{i=1}^{n}\beta_{i}\left\langle w_{i},w_{j}\right\rangle 
\end{align*}
\qqed


\section{Satz \textmd{(Existenz von Lösungen zum linearen Ausgleichsproblem)}}

Sei $A\in\mathbb{R}^{m\times n}$, $b\in\mathbb{R}^{m}$ und $m\ge n$.
Dann existiert eine Lösung $\overline{x}\in\mathbb{R}^{n}$ des linearen
Ausgleichsproblems:
\begin{align*}
\norm{A\overline{x}-b}^{2} & =\min_{x\in\mathbb{R}^{n}}\norm{Ax-b}^{2}
\end{align*}
$x$ erfüllt die Normalengleichungen:
\begin{align*}
A^{\TT}A\overline{x} & =A^{\TT}b
\end{align*}
Die Lösung ist genau dann eindeutig bestimmt, wenn $A$ vollen Rang
hat.


\subsubsection{Beweis}

\begin{align*}
\min_{x\in\mathbb{R}^{n}}\norm{Ax-b}^{2} & =\min_{y\in\text{im}\left(A\right)}\norm{y-b}^{2}
\end{align*}
Der Projektionssatz mit $U=\text{im}\left(A\right)$ bedeutet, dass
das Minimum-Problem eine eindeutige Lösung $\overline{y}\in\text{im}\left(A\right)$
hat mit folgender Eigenschaft:
\begin{align*}
\left\langle \overline{y}-b,y\right\rangle  & =0\quad\fall_{y\in\text{im}\left(A\right)}
\end{align*}
Da $\overline{y}\in\text{im}\left(A\right)$ ist, gibt es ein $\overline{x}\in\mathbb{R}^{n}$
mit $A\overline{x}=\overline{y}$. Wegen $y\in\text{im}\left(A\right)$
gibt es ein $x\in\mathbb{R}^{n}$ mit $Ax=y$.

Wegen $\left\langle \overline{y}-b,y\right\rangle =0$ gilt:
\begin{align*}
\left\langle A\overline{x}-b,Ax\right\rangle  & =0\quad\fall_{x\in\mathbb{R}^{n}}\\
\Leftrightarrow\quad\left\langle A^{\TT}A\overline{x}-A^{\TT}b,x\right\rangle  & =0\quad\fall_{x\in\mathbb{R}^{n}}\\
\Leftrightarrow\quad A^{\TT}A\overline{x} & =A^{\TT}b
\end{align*}
Hat die Matrix $A$ vollen Rang, so ist $A$ injektiv. Also gibt es
genau ein $\overline{x}\in\mathbb{R}^{n}$ mit $A\overline{x}=\overline{y}$
und damit folgt die Eindeutigkeit.\qqed


\section{Lösung linearer Ausgleichsprobleme mittels \texorpdfstring{$QR$}{QR}-Zerlegung}

Sei $A\in\mathbb{R}^{m\times n}$, $b\in\mathbb{R}^{m}$ und $m\ge n$.
Gesucht ist ein $\overline{x}\in\mathbb{R}^{n}$ mit:
\begin{align*}
\norm{A\overline{x}-b}^{2} & =\min_{x\in\mathbb{R}^{n}}\norm{Ax-b}^{2}
\end{align*}
Sei nun $A=QR$ die $QR$-zerlegung von $A$. $R$ hat die Form:
\begin{align*}
\left(\begin{array}{cccc}
* & \ldots & \ldots & *\\
0 & \ddots &  & \vdots\\
\vdots & \ddots & \ddots & \vdots\\
0 & \ldots & 0 & *\\
0 & \ldots & \ldots & 0\\
\vdots &  &  & \vdots\\
0 & \ldots & \ldots & 0
\end{array}\right) & =\left(\begin{array}{ccc}
\\
 & \overbrace{R_{1}}^{\in\mathbb{R}^{n\times n}}\\
\\
0 & \ldots & 0\\
\vdots &  & \vdots\\
0 & \ldots & 0
\end{array}\right)
\end{align*}
Dann gilt
\begin{align*}
\norm{Ax-b}^{2} & =\norm{QRx-b}^{2}\stackrel{Q^{\TT}\text{ orthogonal}}{=}\norm{Q^{\TT}\left(QRx-b\right)}^{2}=\norm{Rx-Q^{\TT}b}^{2}
\end{align*}
und:
\begin{align*}
Rx & =\left(\begin{array}{c}
R_{1}x\\
0\\
\vdots\\
0
\end{array}\right)
\end{align*}
Wähle $b_{1}\in\mathbb{R}^{n}$ und $b_{2}\in\mathbb{R}^{m-n}$ mit:
\begin{align*}
\left(\begin{array}{c}
b_{1}\\
b_{2}
\end{array}\right) & =Q^{\TT}b
\end{align*}
Es folgt:
\begin{align*}
\norm{Ax-b}^{2} & =\underbrace{\norm{R_{1}x-b_{1}}^{2}}_{\text{Norm auf }\mathbb{R}^{n}}+\underbrace{\norm{b_{2}}^{2}}_{\text{Norm auf }\mathbb{R}^{m-n}}
\end{align*}
Falls $A$ maximalen Rang hat, so hat auch $R_{1}$ maximalen Rang
und somit ist die $n\times n$-Matrix $R_{1}$ invertierbar. Also
ist
\begin{align*}
x & =R_{1}^{-1}b_{1}
\end{align*}
ein Minimum für $\norm{Ax-b}$.

\emph{Satz:} Hat die Matrix $A$ vollen Rang und besitzt
\begin{align*}
A & =QR=Q\left(\begin{array}{c}
R_{1}\\
0
\end{array}\right)
\end{align*}
als $QR$-Zerlegung, so besitzt das lineare Ausgleichsproblem bezüglich
$b\in\mathbb{R}^{m}$ die eindeutige Lösung:
\begin{align*}
x & =R_{1}^{-1}b_{1}
\end{align*}
Dabei ist mit $b_{1}\in\mathbb{R}^{n}$ und $b_{2}\in\mathbb{R}^{m-n}$:
\begin{align*}
Q^{\TT}b & =\left(\begin{array}{c}
b_{1}\\
b_{2}
\end{array}\right)
\end{align*}



\section{Vorgehen bei der Lösung von linearen Ausgleichsproblemen mit \texorpdfstring{$QR$}{QR}-Zerlegung}

Seien $A\in\mathbb{R}^{m\times n}$, $b\in\mathbb{R}^{m}$ gegeben.
\begin{enumerate}
\item Bestimme $QR$-Zerlegung von $A$: Führe den Algorithmus \ref{sec:Algorithmus-QR-Zerlegung}
für $R=\left(\begin{array}{c}
R_{1}\\
0
\end{array}\right)$ mit $R_{1}\in\mathbb{R}^{n\times n}$ durch.
\item Berechne $\left(b_{1}\right)_{i\in\left\{ 1,\ldots,n\right\} }=\left(Q^{\TT}b\right)_{i\in\left\{ 1,\ldots,n\right\} }$.
\item Löse das gestaffelte Gleichungssystem $R_{1}x=b_{1}$. (vergleiche
Kapitel \ref{chap:Gau=0000DFsches-Eliminationsverfahren})
\end{enumerate}
$x$ ist die gesuchte Lösung.


\section{Bemerkung}

Es gibt noch eine andere Möglichkeit $\overline{x}$ zu berechnen:

Löse die Normalengleichungen ($\left(n\times n\right)$-Gleichungssystem):
\begin{align*}
A^{\TT}A\overline{x} & =A^{\TT}b
\end{align*}
$A^{\TT}A$ ist symmetrisch und das Cholesky-Verfahren kann angewandt
werden. Die $LR$-Zerleung liefert ebenfalls eine Lösung.

\emph{Aber}: Zur Berechnung von $A^{\TT}A$ müssen Skalarprodukte
von Spaltenvektoren berechnet werden, was oft schlecht konditioniert
ist. Das Verfahren mit $QR$-Zerlegung ist stabiler (,,weniger Fehler``).

%DATE: Mo 19.11.12


\section{\texorpdfstring{$QR$}{QR}-Zerlegung mit Givens-Rotation}

\emph{Ziel:} Leite eine $QR$-Zerlegung mit Hilfe von Drehungen her.

Finde eine Drehungsmatrix $Q$, sodass gilt: 
\begin{align}
QA=Q\begin{pmatrix}a & b\\
e & d
\end{pmatrix} & =\begin{pmatrix}r & *\\
0 & *
\end{pmatrix}\label{eq:QA}
\end{align}
Drehungen lassen sich wie folgt schreiben: 
\begin{align*}
Q & =\begin{pmatrix}\cos\left(\varphi\right) & \sin\left(\varphi\right)\\
-\sin\left(\varphi\right) & \cos\left(\varphi\right)
\end{pmatrix}
\end{align*}
Setzte nun $c:=\cos\left(\varphi\right)$ und $s:=\sin\left(\varphi\right)$.
Es gilt $c^{2}+s^{2}=1$. Es muss laut \eqref{eq:QA} gelten: 
\begin{align*}
\begin{pmatrix}c & s\\
-s & c
\end{pmatrix}\begin{pmatrix}a\\
e
\end{pmatrix} & =\begin{pmatrix}r\\
0
\end{pmatrix}
\end{align*}
Wegen $r^{2}=a^{2}+e^{2}$ (Länge bleibt erhalten) folgt: 
\begin{align*}
r & =\pm\sqrt{a^{2}+e^{2}} & c & =\frac{a}{r} & s & =\frac{e}{r}
\end{align*}
Im allgemeiner Fall ist die Matrix von der Form $A\in\mathbb{R}^{m\times n}$.
Wie rotiere ich?\\
Antwort: Bette Givens-Rotation im $\mathbb{R}^{n}$ ein. 
\begin{align*}
G_{i,k} & =\begin{pmatrix}1\\
 & \ddots\\
 &  & 1\\
 &  &  & c_{1} & \dots & s\\
 &  &  & \vdots & 1 & \vdots\\
 &  &  & -s & \dots & c_{2}\\
 &  &  &  &  &  & 1\\
 &  &  &  &  &  &  & \ddots\\
 &  &  &  &  &  &  &  & 1
\end{pmatrix}
\end{align*}
$G_{i,k}$ realisiert Drehung in der von $e_{1}$ und $e_{k}$ aufgespannten
Ebene. Es gilt: 
\begin{align*}
G_{i,k}\begin{pmatrix}x_{1}\\
\vdots\\
x_{n}
\end{pmatrix} & =\begin{pmatrix}x_{1}\\
\vdots\\
x_{i-1}\\
r\\
x_{i+1}\\
\vdots\\
x_{k-1}\\
0\\
x_{k+1}\\
\vdots\\
x_{n}
\end{pmatrix}
\end{align*}
\begin{align*}
r & =\pm\sqrt{x_{i}^{2}+x_{k}^{2}} & c & =\frac{x_{i}}{r} & s & =\frac{x_{k}}{r}
\end{align*}



\subsubsection*{Beispiel}

\begin{align*}
G_{1,2} & =\begin{pmatrix}\frac{4}{5} & -\frac{3}{5} & 0\\
\frac{3}{5} & \frac{4}{5} & 0\\
0 & 0 & 1
\end{pmatrix} & G_{1,2}\begin{pmatrix}4\\
-3\\
1
\end{pmatrix} & =\begin{pmatrix}5\\
0\\
1
\end{pmatrix}
\end{align*}
\begin{align*}
G_{1,3} & =\begin{pmatrix}\frac{5}{\sqrt{26}} & 0 & \frac{1}{\sqrt{26}}\\
0 & 1 & 0\\
-\frac{1}{\sqrt{26}} & 0 & \frac{5}{\sqrt{26}}
\end{pmatrix} & G_{1,3}\begin{pmatrix}5\\
0\\
1
\end{pmatrix} & =\begin{pmatrix}\sqrt{26}\\
0\\
0
\end{pmatrix}
\end{align*}
Wir schreiben diese Vorgehen wie folgt: 
\begin{align*}
\begin{pmatrix}4\\
-1\\
3
\end{pmatrix} & \xrightarrow{G_{1,2}}\begin{pmatrix}5\\
0\\
1
\end{pmatrix}\xrightarrow{G_{1,3}}\begin{pmatrix}\sqrt{26}\\
0\\
0
\end{pmatrix}
\end{align*}


Im Hinblick auf eine speicherschonende Implementierung ist es wichtig,
$G_{i,k}$ durch eine Zahl kodieren zu können.
\begin{align*}
\varrho & =\varrho_{i,k}=\begin{cases}
1 & c=0\\
\frac{1}{2}\text{sgn}\left(c\right)s & \abs s<\abs c\\
2\cdot\frac{\text{sgn}\left(s\right)}{c} & \abs c\leq\abs s
\end{cases}
\end{align*}
Dekodierung:
\begin{align*}
\left(s,c\right) & =\begin{cases}
\left(1,0\right) & \varrho=1\\
\left(2\varrho,\sqrt{1-\left(2\varrho\right)^{2}}\right) & \abs{\varrho}<1\\
\left(\sqrt{1-\left(\frac{2}{\varrho}\right)^{2}},\frac{2}{\varrho}\right) & \abs{\varrho}>1
\end{cases}
\end{align*}
Wende nacheinander Givens-Rotationen an, um die Einträge zu erzeugen.

Achtung: Zerstöre keine Null-Einträge die schon vorher erzeugt wurden.

Wir bekommen Givens-Rotationen $G_{i_{N},k_{N}},\ldots,G_{i_{1},k_{1}}$,
sodass gilt:
\begin{align*}
G_{i_{N},k_{N}}\cdot\ldots\cdot G_{i_{1},k_{1}} & =R\\
A & =\underbrace{G_{i_{1},k_{1}}^{T}\cdot\ldots\cdot G_{i_{N},k_{N}}^{T}}_{=Q}\cdot R
\end{align*}



\subsubsection*{Bemerkung}

$Q$ nie explizit ausrechnen!

Viele Matrizen in der Anwendung sind dünn besetzt, das heißt ganz
viele Einträge sind Null. In diesem Fall braucht man nur noch wenige
Rotationen um eine obere Dreiecksgestalt zu erhalten.




\subsubsection*{Bemerkung zu Givens-Rotationen}

Speicherung von
\begin{align*}
\left(\begin{array}{cc}
c & s\\
-s & c
\end{array}\right)
\end{align*}
mit Hilfe von $\varrho$ kann nach Dekodierung auch
\begin{align*}
-\left(\begin{array}{cc}
c & s\\
-s & c
\end{array}\right)
\end{align*}
liefern. Dies ist kein Problem. Auch diese Matrix sorgt für die richtige
Rotation.
\begin{align*}
\left(\begin{array}{cc}
c & s\\
-s & c
\end{array}\right)\left(\begin{array}{c}
a\\
b
\end{array}\right) & =\left(\begin{array}{c}
r\\
0
\end{array}\right)
\end{align*}
Die andere Matrix ändert nur das Vorzeichen von $r$.


\chapter{Numerische Lösung nichtlinearer Gleichungssysteme}


\section{Problemstellung}


\subsubsection{Beispiele}
\begin{enumerate}
\item Nullstellenbestimmung\\
\emph{Gegeben:} Eine Definitionsmenge $D\subset\mathbb{R}^{n}$ und
eine stetige Funktion $f:D\to\mathbb{R}^{m}$.\\
\emph{Gesucht:} Eine Nullstelle $x\in D$ mit $f\left(x\right)=0$.
\item Fixpunktprobleme\\
\emph{ Gegeben:} Eine Definitionsmenge $D\subset\mathbb{R}^{n}$ und
eine stetige Funktion $g:D\to\mathbb{R}^{n}$.\\
\emph{ Gesucht:} Ein Fixpunkt$x\in D$ mit $g\left(x\right)=x$.\\
 Jede Fixpunktgleichung kann in eine Nullstellengleichung überführt
werden und umgekehrt, indem man setzt:
\begin{align*}
f\left(x\right) & =g\left(x\right)-x
\end{align*}

\end{enumerate}

\section{Beispiele\label{sec:Beispiele-nichtlinear-GLS}}
\begin{enumerate}[label=\alph*)]
\item Bestimme die Nullstellen eines Polynoms $f\left(x\right)=\sum_{i=0}^{m}a_{i}x^{i}$
mit $a_{i}\in\mathbb{R}$. Suche $x\in\mathbb{R}$ mit: 
\begin{align*}
f\left(x\right) & =\sum_{i=0}^{m}a_{i}\cdot x^{i}=0
\end{align*}
Zum Beispiel bestimme $x\in\mathbb{R}$ mit $x^{2}-5=0$.
\item Sei $h:\mathbb{R}^{n}\to\mathbb{R}$ eine Funktion. Gesucht ist das
Minimum $x_{0}$ von $h$. Dann erfüllt $x_{0}$ die Gleichung: 
\begin{align*}
f\left(x_{0}\right) & :=\nabla h\left(x\right)=0
\end{align*}

\item Keplersche Gleichung\\
\emph{Gesucht:} Ein $x\in\mathbb{R}$ mit $x=e\cdot\sin\left(x\right)+a$.\\
\emph{Gesucht:} Ein Fixpunkt $x$ mit $x=g\left(x\right):=e\cdot\sin\left(x\right)+a$.
\item Diskretisierung von Randwertproblemen\\
 Sei $I=\left[a,b\right]$ ein Intervall.\\
\emph{Gesucht:} $u:\left[a,b\right]\to\mathbb{R}$ mit den Eigenschaften:
\begin{align*}
u''\left(x\right) & =f\left(u\left(x\right)\right)\qquad\fall_{x\in\left[a,b\right]}\\
u\left(a\right) & =u_{a}\\
u\left(b\right) & =u_{b}
\end{align*}
Dabei seien $u_{a},u_{b}\in\mathbb{R}$ sowie die stetige Funktion
$f:\mathbb{R}\to\mathbb{R}$ gegeben. Sei $N\in\mathbb{N}$. Definiere
für $i\in\left\{ 0,\ldots,N+1\right\} $: 
\begin{align*}
h & :=\frac{b-a}{N+1}\\
x_{i} & :=a+i\cdot h
\end{align*}
Für $i\in\left\{ 1,\ldots,N\right\} $ kann man die zweite Ableitung
näherungsweise berechnen:
\begin{align*}
u''\left(x_{i}\right) & \approx\frac{u\left(x_{i+1}\right)-2u\left(x_{i}\right)+u\left(x_{i-1}\right)}{h^{2}}
\end{align*}
Ersetze die Differentialgleichung $u''\left(x\right)=f\left(u\left(x_{i}\right)\right)$
mit den Punkten $x_{i}$ durch folgendes Gleichungssystem für $i\in\left\{ 1,\ldots,N\right\} $:
\begin{align*}
\frac{u\left(x_{i+1}\right)-2u\left(x_{1}\right)+u\left(x_{i-1}\right)}{h^{2}} & \approx f\left(u\left(x_{i}\right)\right)
\end{align*}
Führe den Vektor 
\begin{align*}
u & =\begin{pmatrix}u_{1}\\
\vdots\\
u_{N}
\end{pmatrix}
\end{align*}
ein und betrachte das Gleichungssystem: 
\begin{align*}
\begin{pmatrix}2 & -1 & 0 & \dots & 0\\
-1 & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & -1\\
0 & \dots & 0 & -1 & 2
\end{pmatrix}u & =-\begin{pmatrix}h^{2}f\left(u_{1}\right)-u_{a}\\
h^{2}f\left(u_{2}\right)\\
\vdots\\
h^{2}f\left(u_{n-1}\right)\\
h^{2}f\left(u_{n}\right)-u_{b}
\end{pmatrix}
\end{align*}
Dies ist eine nichtlineares Gleichungssystem für die $u_{i}$, die
$u\left(x_{i}\right)$ annähern. (siehe Vorlesung über Numerik von
Differentialgleichungen).
\end{enumerate}

\section{Bisektionsverfahren}

Sei $f:\left[a,b\right]\to\mathbb{R}$ stetig. Es gelte $f\left(a\right)\cdot f\left(b\right)<0$.
Nach dem Mittelwertsatz existiert mindestens eine Nullstellen $\bar{x}\in\mathbb{R}$
mit $f\left(\bar{x}\right)=0$. Ohne Beschränkung der Allgemeinheit
sei $f\left(a\right)<0$ und $f\left(b\right)>0$.\\
\emph{Iterationsverfahren:} Halbiere in jedem Schritt das Intervall.
Setze $x_{-}^{(0)}=a$ und $x_{+}^{(0)}=b$. Für $k=\mathbb{N}_{\ge0}$
berechne: 
\begin{align*}
x_{m}^{(k)} & =\frac{x_{+}^{(k)}+x_{-}^{(k)}}{2}
\end{align*}
Ist $f\left(x_{m}^{(k)}\right)=0$ ist die Nullstelle gefunden und
man kann aufhören. Sonst berechne:
\begin{align*}
x_{+}^{(k+1)} & :=\begin{cases}
x_{+}^{\left(k\right)} & \text{falls }f\left(x_{m}^{\left(k\right)}\right)<0\\
x_{m}^{\left(k\right)} & \text{falls }f\left(x_{m}^{\left(k\right)}\right)>0
\end{cases}\\
x_{-}^{(k+1)} & :=\begin{cases}
x_{m}^{\left(k\right)} & \text{falls }f\left(x_{m}^{\left(k\right)}\right)<0\\
x_{-}^{\left(k\right)} & \text{falls }f\left(x_{m}^{\left(k\right)}\right)>0
\end{cases}
\end{align*}
Die Folgen $\left(x_{+}^{(k)}\right)_{k\in\mathbb{N}}$ und $\left(x_{-}^{(k)}\right)_{k\in\mathbb{N}}$
sind monoton und beschränkt, weshalb beide konvergieren. Aus dem Zwischenwertsatz
folgt, dass die Folgen $\left(x_{+}^{(k)}\right)_{k\in\mathbb{N}}$
und $\left(x_{-}^{(k)}\right)_{k\in\mathbb{N}}$ denselben Grenzwert
$\bar{x}\in\mathbb{R}$ haben. Weiter gilt 
\begin{align*}
\abs{x_{\pm}^{\left(k\right)}-\overline{x}}\le x_{+}^{(k)}-x_{-}^{(k)} & \leq2^{-k}\abs{b-a}
\end{align*}
und: 
\begin{align*}
f\left(\bar{x}\right) & =\lim_{k\rightarrow\infty}f\left(x_{+}^{(k)}\right)=\lim_{k\rightarrow\infty}f\left(x_{-}^{(k)}\right)
\end{align*}
Wegen $f\left(x_{+}^{\left(k\right)}\right)>0$ und $f\left(x_{-}^{\left(k\right)}\right)<0$
bedeutet dies $f\left(\overline{x}\right)=0$:
\begin{align*}
f\left(\overline{x}\right) & \stackrel{f\text{ stetig}}{=}\lim_{k\to\infty}\underbrace{f\left(x_{+}^{\left(k\right)}\right)}_{>0}\ge0\\
 & \stackrel{f\text{ stetig}}{=}\lim_{k\to\infty}\underbrace{f\left(x_{-}^{\left(k\right)}\right)}_{<0}\le0\\
\Rightarrow\quad f\left(\overline{x}\right) & =0
\end{align*}


%DATE: Mi 21.11.12


\section{Konvergenzordnung}

Eine Folge $\left(x^{\left(k\right)}\right)_{k\in\mathbb{N}}$ mit
$x^{\left(k\right)}\in\mathbb{R}^{n}$ konvergiert mit der Ordnung
$p\in\mathbb{R}_{\ge1}$ gegen $\overline{x}$, falls $x^{\left(k\right)}\to\overline{x}$
für $k\to\infty$ konvergiert und falls ein $c\in\mathbb{R}_{>0}$
existiert, sodass gilt:
\begin{align*}
\norm{x^{\left(k+1\right)}-\overline{x}} & \le c\norm{x^{\left(k\right)}-\overline{x}}^{p}
\end{align*}
Im Fall $p=1$ setzen wir zusätzlich $c<1$ voraus und nennen die
Konvergenz \emph{linear}.

Im Fall $p=2$ sprechen wir von quadratischer Konvergenz.

Falls
\begin{align*}
\lim_{k\to\infty}\frac{\norm{x^{\left(k+1\right)}-\overline{x}}}{\norm{x^{\left(k\right)}-\overline{x}}} & =0
\end{align*}
gilt, so sprechen wir von \emph{superlinearer} Konvergenz.


\section{Fixpunktiteration}

Gesucht sei ein Fixpunkt $x\in D\subseteq\mathbb{R}^{n}$ der stetigen
Funktion $g:D\to\mathbb{R}^{n}$, das heißt $g\left(x\right)=x$.

Ansatz: Benutze einfache Iteration, das heißt wähle ein $x^{\left(0\right)}\in D$
und definiere $x^{\left(k+1\right)}=g\left(x^{\left(k\right)}\right)$
für $k\in\mathbb{N}_{0}$.

Falls $x^{\left(k\right)}\to\overline{x}\in D$ konvergiert, so gilt:
\begin{align*}
\overline{x} & =\lim_{k\to\infty}x^{\left(k+1\right)}=\lim_{k\to\infty}g\left(x^{\left(k\right)}\right)=g\left(\overline{x}\right)
\end{align*}


Beispiel 1:

\textcolor{green}{TODO: Abb7 einfügen}

Beispiel 2:

\textcolor{green}{TODO: Abb8 einfügen}


\section{Kontraktion\label{sec:Kontraktion}}

Sei $D\subseteq\mathbb{R}^{n}$ abgeschlossen und $\norm .$ eine
Norm auf $\mathbb{R}^{n}$. Eine Abbildung $g:D\to\mathbb{R}^{n}$
heißt \emph{Kontraktion} (bezüglich $\norm .$), falls ein $\kappa\in\left(0,1\right)$
existiert mit:
\begin{align*}
\norm{g\left(u\right)-g\left(v\right)} & \le\kappa\norm{u-v}\qquad\fall_{u,v\in D}
\end{align*}
Die Zahl $\kappa$ heißt \emph{Kontraktionszahl} von $g$.

\textcolor{green}{TODO: Abb9 einfügen}


\section{Banachscher Fixpunktsatz}

Zusätzlich zu den Eigenschaften in \ref{sec:Kontraktion} gelte $g\left(D\right)\subseteq D$.
Dann gilt:
\begin{enumerate}
\item $g$ hat genau einen Fixpunkt.
\item Für alle $x^{\left(0\right)}\in D$ gilt: Die Folge $\left(x^{\left(k\right)}\right)_{k\in\mathbb{N}}$
definiert durch $x^{\left(k+1\right)}=g\left(x^{\left(k\right)}\right)$
konvergiert gegen $\overline{x}$.
\item Es gilt:
\begin{align*}
\norm{x^{\left(k\right)}-\overline{x}} & \le\frac{\kappa}{1-\kappa}\cdot\norm{x^{\left(k\right)}-x^{\left(k-1\right)}}
\end{align*}
Dies ist eine \emph{a posteriori} Fehlerabschätzung. Es gibt auch
eine \emph{a priori} Abschätzung:
\begin{align*}
\norm{x^{\left(k\right)}-\overline{x}} & \le\frac{\kappa^{k}}{1-\kappa}\norm{x^{\left(1\right)}-x^{\left(0\right)}}
\end{align*}

\end{enumerate}

\subsubsection*{Beweis}

Es gilt:
\begin{align*}
\norm{x^{\left(k+1\right)}-x^{\left(k\right)}} & =\norm{g\left(x^{\left(k\right)}\right)-g\left(x^{\left(k-1\right)}\right)}\stackrel{\text{Kontraktion}}{\le}\kappa\norm{x^{\left(k\right)}-x^{\left(k-1\right)}}\le\kappa^{k}\norm{x^{\left(1\right)}-x^{\left(0\right)}}
\end{align*}
\begin{align*}
\norm{x^{\left(k+l\right)}-x^{\left(k\right)}} & \le\sum_{i=0}^{l-1}\norm{x^{\left(k+i+1\right)}-x^{\left(k+i\right)}}\le\sum_{i=0}^{l-1}\kappa^{i}\norm{x^{\left(k+1\right)}-x^{\left(k\right)}}\stackrel{\text{geo.R.}}{\le}\frac{\kappa^{k}}{1-\kappa}\norm{x^{\left(1\right)}-x^{\left(0\right)}}
\end{align*}
Also ist $x^{\left(k\right)}$ eine Cauchy-Folge und konvergiert daher
gegen $\overline{x}\in D$, da $D$ abgeschlossen ist.
\begin{align*}
\overline{x} & =\lim_{k\to\infty}x^{\left(k+1\right)}=\lim_{k\to\infty}g\left(x^{\left(k\right)}\right)\stackrel{\text{stetig}}{=}g\left(\overline{x}\right)
\end{align*}

\begin{description}
\item [{\textmd{\emph{Eindeutigkeit:}}}] Seien $\overline{x}$ und $\overline{y}$
zwei Fixpunkte.
\begin{align*}
\norm{\overline{x}-\overline{y}} & \le\kappa\norm{g\left(\overline{x}\right)-g\left(\overline{x}\right)}\sr ={g\left(\overline{x}\right)=\overline{x}}{g\left(\overline{y}\right)=\overline{y}}\le\kappa\norm{\overline{x}-\overline{y}}
\end{align*}
Wegen $\kappa<1$ folgt $\overline{x}=\overline{y}$.
\item [{\textmd{\emph{A~priori~Abschätzung:}}}] Es gilt:
\begin{align*}
\norm{x^{\left(k\right)}-\overline{x}} & =\lim_{k\to\infty}\norm{x^{\left(k\right)}-x^{\left(k+l\right)}}\le\frac{\kappa^{k}}{1-k}\norm{x^{\left(1\right)}-x^{\left(0\right)}}
\end{align*}

\item [{\textmd{\emph{A~posteriori~Abschätzung:}}}] Es gilt:
\begin{align*}
\norm{x^{\left(k\right)}-\overline{x}} & =\lim_{k\to\infty}\norm{x^{\left(k\right)}-x^{\left(k+l\right)}}\le\norm{x^{\left(k+1\right)}-x^{\left(k\right)}}\cdot\underbrace{\sum_{i=0}^{l-1}\kappa^{i}}_{\le\frac{1}{1-\kappa}}\le\frac{\kappa}{1-k}\norm{x^{\left(k+1\right)}-x^{\left(k\right)}}
\end{align*}

\end{description}
\qqed


\section{Bemerkungen\label{sec:Bem-BFS}}
\begin{enumerate}
\item Das Iterationsverfahren aus dem Banachschen Fixpunktsatz konvergiert
linear, denn es gilt:
\begin{align*}
\norm{x^{\left(k+1\right)}-\overline{x}} & =\norm{g\left(x^{\left(k\right)}\right)-g\left(\overline{x}\right)}\stackrel{g\left(\overline{x}\right)=\overline{x}}{\le}\kappa\norm{x^{\left(k\right)}-\overline{x}}
\end{align*}
Da $\kappa<1$ ist, folgt die lineare Konvergenz.
\item Sei $D=\overline{\Omega}$ mit $\Omega\subseteq\mathbb{R}^{n}$ offen
und konvex. Dann gilt:\\
Eine stets differenzierbare Abbildung $g:D\to\mathbb{R}^{n}$ ist
kontrahierend, falls gilt:
\begin{align*}
\sup_{x\in D}\opnorm{\DD g\left(x\right)} & <1
\end{align*}


\begin{description}
\item [{Beweis:}] Sei $h\left(t\right)=g\left(x+t\left(y-x\right)\right)$,
das heißt $h\left(1\right)=g\left(y\right)$ und $h\left(0\right)=g\left(x\right)$.
\begin{align*}
\norm{g\left(y\right)-g\left(x\right)} & =\norm{h\left(1\right)-h\left(0\right)}=\norm{\int_{0}^{1}h'\left(t\right)\dd t}=\norm{\int_{0}^{1}\DD g\left(x+t\left(y-x\right)\right)\left(y-x\right)\dd t}\le\\
 & \le\int_{0}^{1}\norm{\DD g\left(x+t\left(y-x\right)\right)\left(y-x\right)}\dd t\le\\
 & \le\norm{\left(y-x\right)}\int_{0}^{1}\opnorm{\DD g\left(x+t\left(y-x\right)\right)}\dd t\le\\
 & \le\norm{\left(y-x\right)}\underbrace{\sup_{z\in D}\opnorm{\DD g\left(z\right)}}_{<1}
\end{align*}
Also folgt, dass $g$ kontrahierend.\qqed[\ref{sec:Bem-BFS}.2]
\end{description}
\item Das Problem bei der Anwendung des Banachschen Fixpunktsatzes ist zu
zeigen, dass $g\left(D\right)\subseteq D$ gilt.
\end{enumerate}

\section{Praktische Formulierung des Banachschen Fixpunktsatzes\label{sec:Praktisch-Banach-Fixpunkt}}

Es sei $g:D\to\mathbb{R}^{n}$ kontrahierend mit Kontraktionszahl
$\kappa$ (und damit Lipschitz-stetig mit Lipschitz-Konstante $\kappa$).\\
Es seien $x_{0}\in D$ und ein $r>0$ mit:
\begin{enumerate}
\item $D_{0}:=\overline{B_{r}\left(x_{0}\right)}\subseteq D$
\item $\norm{g\left(x_{0}\right)-x_{0}}\le\left(1-\kappa\right)r$
\end{enumerate}
Dann gilt $g\left(D_{0}\right)\subseteq D_{0}$ und der Banachsche
Fixpunktsatz ist mit der Menge $D_{0}$ anwendbar, das heißt $g$
besitzt einen Fixpunkt in $D_{0}$.


\subsubsection*{Beweis}

Sei $y\in D_{0}$, so gilt:
\begin{align*}
\norm{g\left(y\right)-x_{0}} & \le\norm{g\left(y\right)-g\left(x_{0}\right)}+\norm{g\left(x_{0}\right)-x_{0}}<\kappa\underbrace{\norm{y-x_{0}}}_{\le r}+\left(1-\kappa\right)r\le r
\end{align*}
\qqed


\section{Das Newton-Verfahren in einer Dimension}

Sei $f:\mathbb{R}\to\mathbb{R}$ stetig differenzierbar. Dann liegt
folgendes Vorgehen nahe, um eine Nullstelle zu bestimmen:

\textcolor{green}{TODO: Abb10 einfügen}
\begin{itemize}
\item Wähle einen Startwert $x^{\left(0\right)}$.
\item Lege eine Tangente an die Funktion $f$ im Punkt $\left(x^{\left(0\right)},f\left(x^{\left(0\right)}\right)\right)$.
\item Berechne die Nullstelle $x^{\left(1\right)}$ der Tangente.
\item Iteriere dieses Verfahren
\end{itemize}
\emph{Bemerkung:} Die Tangente hat genau dann eine Nullstelle, wenn
$f'\left(x^{\left(0\right)}\right)\not=0$ ist.

Wie berechnen sich die Iterierten $x^{\left(k\right)}$? Die Tangente
ist der Graph der Geraden:
\begin{align*}
T\left(x\right) & =f\left(x^{\left(0\right)}\right)+f'\left(x^{\left(0\right)}\right)\left(x-x^{\left(0\right)}\right)
\end{align*}
Löse $T\left(x\right)=0$. Die Nullstelle $x^{\left(1\right)}$ ist
der nächste Schätzwert für die Nullstelle von $f$:
\begin{align*}
x^{\left(1\right)} & =x^{\left(0\right)}-\left(f'\left(x^{\left(0\right)}\right)\right)^{-1}f\left(x^{\left(0\right)}\right)
\end{align*}
Das Verfahren lautet nun:
\begin{enumerate}
\item Wähle ein $x^{\left(0\right)}\in\mathbb{R}$.\\
Für $r\in\mathbb{N}_{0}$:
\item Berechne $f'\left(x^{\left(k\right)}\right)$. Falls $f'\left(x^{\left(k\right)}\right)=0$:
Stoppe!
\item Setze $x^{\left(k+1\right)}=x^{\left(k\right)}-\left(f'\left(x^{\left(0\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)$
und gehe zu 2.
\end{enumerate}

\subsubsection*{Bemerkungen}
\begin{enumerate}
\item Der gewählte Startwert sollte schon eine einigermaßen gute Näherung
sein. Betrachte zum Beispiel den Graphen und schätze Nullstelle. Andere
Möglichkeit: Verwende erst Bisektionsverfahren bis eine gute Näherung
als Startwert für das Newton-Verfahren gefunden ist.%DATE: Mo 26.11.12
\item Das Newton-Verfahren konvergiert nicht immer. Es können zum Beispiel
Oszillationen auftreten.
\end{enumerate}

\section{Satz}

Sei $f\in\mathcal{C}^{2}\left(\left(a,b\right),\mathbb{R}\right)$
und $x^{*}\in\left(a,b\right)$ eine einfache Nullstelle von $f$,
das heißt $f\left(x^{*}\right)=0$ und $f'\left(x^{*}\right)\neq0$.
Dann gibt es ein $\varepsilon>0$, sodass für jedes $x^{(0)}\in\overline{B_{\varepsilon}\left(x^{*}\right)}$
das Newton-Verfahren quadratisch gegen $x^{*}$ konvergiert .


\subsubsection*{Beweis}

Definiere: 
\begin{align*}
g\left(x\right) & =x-\frac{f\left(x\right)}{f'\left(x\right)}
\end{align*}
Das Newton-Verfahren angewendet auf $x^{\left(0\right)}$ liefert:
\begin{align*}
x^{(k+1)} & =g\left(x^{(k)}\right)=g^{k+1}\left(x^{\left(0\right)}\right)
\end{align*}
Weiter gilt: 
\begin{align*}
g'\left(x\right) & =1-\frac{f'\left(x\right)f'\left(x\right)-f\left(x\right)f''\left(x\right)}{f'\left(x\right)^{2}}=\frac{f\left(x\right)f''\left(x\right)}{\abs{f'\left(x\right)}^{2}}
\end{align*}
Also ist $g'\left(x^{*}\right)=0$. Weiterhin gibt es eine Umgebung
$U_{0}$ von $x^{*}$, sodass $f'\left(x\right)\neq0$ für alle $x\in U_{0}$
ist. Da $f''$ stetig ist, existiert also ein $\varepsilon\in\mathbb{R}_{>0}$
und ein $\kappa\in\left(0,1\right)$, sodass $\abs{g'\left(x\right)}\leq\kappa<1$
für alle $x\in\overline{B_{\varepsilon}\left(x^{*}\right)}$ gilt.
Da $g\left(x^{*}\right)=x^{*}$ gilt, liefert der praktische Fixpunktsatz
von Banach \ref{sec:Praktisch-Banach-Fixpunkt} zunächst 
\begin{align*}
g\left(\overline{B_{\varepsilon}\left(x^{*}\right)}\right) & \subset\overline{B_{\varepsilon}\left(x^{*}\right)}
\end{align*}
und damit auch die Konvergenz für alle $x^{(0)}\in\overline{B_{\varepsilon}\left(x^{*}\right)}$.

\emph{Quadratische Konvergenz:}
\begin{align*}
\abs{x^{\left(k+1\right)}-x^{*}} & =\abs{x^{\left(k\right)}-\left(f'\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)-x^{*}}=\\
 & \stackrel{f\left(x^{*}\right)=0}{=}\abs{f'\left(x^{\left(k\right)}\right)}^{-1}\abs{f\left(x^{*}\right)-f\left(x^{\left(k\right)}\right)-f'\left(x^{\left(k\right)}\right)\left(x^{*}-x^{\left(k\right)}\right)}=\\
 & \stackrel{\text{Taylorreihe}}{=}\abs{f'\left(x^{\left(k\right)}\right)}^{-1}\abs{\frac{f''\left(x\right)}{2}\cdot\left(x^{\left(k\right)}-x^{*}\right)^{2}+\mathcal{O}\left(\left(x^{\left(k\right)}-x^{*}\right)^{3}\right)}=\\
 & \sr{\le}{\text{Restgliedabschätzung}}{}\sup_{x\in B_{\varepsilon}\left(x^{*}\right)}\left(\abs{f'\left(x\right)}^{-1}\right)\sup_{x\in B_{\varepsilon}\left(x^{*}\right)}\left(\abs{f''\left(x\right)}\frac{1}{2}\abs{x^{\left(k\right)}-x^{*}}^{2}\right)
\end{align*}
\qqed


\section{Bemerkung}
\begin{enumerate}[label=\alph*)]
\item Bei $m$-fachen Nullstellen 
\begin{align*}
f\left(x^{*}\right) & =f'\left(x^{*}\right)=...=f^{\left(m-1\right)}\left(x^{*}\right)=0 & f^{(m)}\left(x^{*}\right) & \neq0
\end{align*}
liegt im Allgemeinen keine quadratische Konvergenz vor, sondern nur
noch lineare. Man kann quadratische Konvergenz erhalten, indem man
das Newton-Verfahren modifiziert: 
\begin{align*}
x^{(k+1)} & =x^{(k)}-m\frac{f\left(x^{(k)}\right)}{f'\left(x^{(k)}\right)}
\end{align*}

\item Die Ableitung $f'$ muss bekannt sein. 
\item Die Lage und Größe des Konvergenzintervalls ist a priori häufig unbekannt.\\
\emph{Mögliches Vorgehen:} Wende erst das Bisektionsverfahren an,
bis eine halbwegs gute Näherung gefunden wurde. Verwende dann das
Newtonverfahren, um schnell bessere Näherungen zu bekommen. 
\item Beispiele

\begin{enumerate}[label=\roman*)]
\item Die Iteration kann den Definitionsbereich verlassen.\\
\textcolor{green}{TODO: Abbildung einfügen}
\item Die Folge kann oszillieren. 
\item Probleme gibt es oft, falls die Ableitung Nullstelle in der Nähe hat.\\
\textcolor{green}{TODO: Abbildung einfügen}
\end{enumerate}
\end{enumerate}

\section{Newton-Verfahren für Systeme}

Betrachte eine zweimal stetig differenzierbare Funktion $f:D\to\mathbb{R}^{n}$
mit offenem Definitionsbereich $D\subset\mathbb{R}^{n}$. Suche $x\in D$
mit $f\left(x\right)=0$. Führe dazu eine Taylorentwicklung um $\overline{x}\in D$
durch: 
\begin{align*}
f\left(x\right) & =f\left(\bar{x}\right)+\DD f\left(\bar{x}\right)\left(x-\bar{x}\right)+\mathcal{O}_{0}\left(\norm{x-\bar{x}}^{2}\right)
\end{align*}
Die Ableitung ist: 
\begin{align*}
\DD f\left(\bar{x}\right) & =\begin{pmatrix}\frac{\partial f_{1}}{\partial x_{1}} & \dots & \frac{\partial f_{1}}{\partial x_{n}}\\
\vdots & \ddots & \vdots\\
\frac{\partial f_{n}}{\partial x_{1}} & \dots & \frac{\partial f_{n}}{\partial x_{n}}
\end{pmatrix}
\end{align*}
In Komponenten erhalten wir für $x\to\overline{x}$: 
\begin{align*}
f_{i}\left(x\right) & =f_{i}\left(\bar{x}\right)+\sum_{j=1}^{n}\frac{\partial f_{i}}{\partial x_{j}}\left(x_{j}-\bar{x}_{j}\right)+\mathcal{O}_{0}\left(\norm{x-\bar{x}}^{2}\right)
\end{align*}
Suche jetzt Nullstelle der (affin-)linearen Näherung. Ist $x^{\left(k\right)}$
schon berechnet, so löse 
\begin{align*}
0 & =f\left(x^{\left(k\right)}\right)+\DD f\left(x^{\left(k\right)}\right)\left(x^{\left(k+1\right)}-x^{\left(k\right)}\right)
\end{align*}
nach $x^{\left(k+1\right)}$. Falls $\DD f\left(x^{\left(k\right)}\right)$
invertierbar ist, so erhalten wir: 
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}-\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}\left(f\left(x^{\left(k\right)}\right)\right)
\end{align*}
Hier wird jedoch die Inverse gebraucht.

Für $n=1$ entspricht dies dem Verfahren das wir schon kennen. Für
gegebenes $x^{\left(0\right)}$ bestimme iterativ $x^{\left(1\right)},x^{\left(2\right)},...$.\\
In der Praxis gehen wir wie folgt vor: Sei $x^{(k)}$ gegeben, so
löse: 
\begin{align*}
\DD f\left(x^{\left(k\right)}\right)s^{\left(k\right)} & =-f\left(x^{\left(k\right)}\right)
\end{align*}
Setze:
\begin{align*}
x^{(k+1)} & =x^{(k)}+s^{(k)}
\end{align*}
Dies benötigt nur eine Lösung eines linearen Gleichungssystems, während
obige Formel die sehr aufwändige Berechnung einer Inversen benötigt.\\
\emph{Ziel:} Zeige die quadratische Konvergenz.


\section{Satz}

Es sei $\Omega\subset\mathbb{R}^{n}$, $f:\Omega\to\mathbb{R}^{n}$
eine stetig differenzierbare Funktion mit invertierbarer Jacobi-Matrix
$\DD f\left(x\right)$ für alle $x\in\Omega$. Sei $\beta\in\mathbb{R}_{>0}$
mit: 
\begin{align*}
\opnorm{\left(\DD f\left(x\right)\right)^{-1}} & \leq\beta\qquad\fall_{x\in\Omega}
\end{align*}
Weiter sei $\DD f\left(x\right)$ Lipschitz-stetig mit Konstanten
$\gamma$, das heißt: 
\begin{align*}
\opnorm{\DD f\left(x\right)-\DD f\left(y\right)} & \leq\norm{x-y}\qquad\fall_{x,y\in\Omega}
\end{align*}
Weiter existiert eine Lösung $x^{*}$ von $f\left(x^{*}\right)=0$
in $\Omega$. Der Startwert $x^{\left(0\right)}$ erfülle
\begin{align*}
x^{\left(0\right)} & \in B_{\omega}\left(x^{*}\right):=\left\{ x\in\mathbb{R}^{n}\Big|\norm{x^{*}-x}<\omega\right\} 
\end{align*}
Dabei ist $\omega$ so gewählt, dass $B_{\omega}\left(x^{*}\right)\subset\Omega$
ist und $\omega\leq\frac{2}{\gamma\beta}$ gilt.\\
Dann bleibt die durch die Newton-Verfahren definierte Folge $\left(x^{\left(k\right)}\right)_{k\in\mathbb{N}}$
in der Kugel $B_{\omega}\left(x^{*}\right)$ und konvergiert quadratisch
gegen $x^{*}$. Genauer gilt: 
\begin{align*}
\norm{x^{\left(k+1\right)}-x^{*}} & \leq\frac{\beta\gamma}{2}\norm{x^{\left(k\right)}-x^{*}}
\end{align*}



\subsubsection*{Beweis}

Für $x^{\left(k\right)}\in\Omega$ gilt: 
\begin{align*}
x^{\left(k+1\right)}-x^{*} & =x^{\left(k\right)}-x^{*}-\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}\left(f\left(x^{\left(k\right)}\right)-f\left(x^{*}\right)\right)\\
 & =\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}\left(f\left(x^{*}\right)-f\left(x^{\left(k\right)}\right)-\DD f\left(x^{\left(k\right)}\right)\left(x^{*}-x^{\left(k\right)}\right)\right)
\end{align*}
Aus der Abschätzung $\opnorm{\left(\DD f\left(x\right)\right)^{-1}}\le\beta$
folgt: 
\begin{align*}
\norm{x^{\left(k+1\right)}-x^{*}} & \leq\beta\norm{f\left(x^{*}\right)-f\left(x^{\left(k\right)}\right)-\DD f\left(x^{\left(k\right)}\right)\left(x^{*}-x^{\left(k\right)}\right)}
\end{align*}
Betrachte: 
\begin{align*}
\phi\left(t\right) & =f\left(y+t\left(x-y\right)\right)\qquad\fall_{x,y\in B_{\omega}\left(x^{*}\right),\; t\in\left[0,1\right]}
\end{align*}
Es gilt dann:
\begin{align*}
\phi\left(1\right) & =f\left(x\right) & \phi\left(0\right) & =f\left(y\right)
\end{align*}
 Weiter ist:
\begin{align*}
\phi'\left(t\right) & =\DD f\left(y+t\left(x-y\right)\right)\left(x-y\right)
\end{align*}
 Da $\DD f$ Lipschitz ist, folgt: 
\begin{align*}
\norm{\phi'\left(t\right)-\phi'\left(0\right)} & =\norm{\left(\DD f\left(y+t\left(x-y\right)\right)-\DD f\left(y\right)\right)\left(x-y\right)}\\
 & =\opnorm{\DD f\left(y+t\left(x-y\right)\right)-\DD f\left(y\right)}\cdot\norm{x-y}\\
 & \leq\gamma\norm{x-y}^{2}
\end{align*}
Außerdem gilt: 
\begin{align*}
f\left(x\right)-f\left(y\right)-\DD f\left(y\right)\left(x-y\right) & =\phi\left(1\right)-\phi\left(0\right)-\phi'\left(0\right)\stackrel{\text{HDI}}{=}\int_{0}^{1}\left(\phi'\left(t\right)-\phi'\left(0\right)\right)\dd t
\end{align*}
Es folgt: 
\begin{align*}
\norm{f\left(x\right)-f\left(y\right)-\DD f\left(y\right)\left(x-y\right)} & \leq\gamma\norm{x-y}^{2}\int_{0}^{1}t\dd t=\frac{\gamma}{2}\norm{x-y}^{2}
\end{align*}
Damit folgt jetzt: 
\begin{align*}
\norm{x^{\left(k+1\right)}-x^{*}} & \leq\frac{\beta\gamma}{2}\norm{x^{*}-x^{(k)}}^{2}
\end{align*}
Dies zeigt die quadratische Konvergenz. Es bleibt jedoch noch zu zeigen,
dass für alle $k\in\mathbb{N}$ gilt:
\begin{align*}
\norm{x^{\left(k\right)}-x^{*}} & <\omega
\end{align*}
Für $\ensuremath{k=0}$ gilt dies nach Voraussetzung.

Induktionsschritt $k\leadsto k+1$: 
\begin{align*}
\norm{x^{\left(k+1\right)}-x^{*}} & \leq\frac{\beta\gamma}{2}\norm{x^{*}-x^{(k)}}\cdot\norm{x^{*}-x^{(k)}}\leq\frac{\beta\gamma\omega}{2}\omega
\end{align*}
Wegen $\omega<\frac{2}{\beta\gamma}$ folgt die Behauptung.\qqed 


\subsection*{Bemerkung}

Die Parameter $\beta$ und $\gamma$ sind durch das Problem gegeben.
Damit die Voraussetzungen des Satzes erfüllt sind, muss $\omega$
genügend klein gewählt werden.


\section{Beispiel: Diskretisierung eines nicht-linearen Randwertproblems}

Betrachte die Situation aus \ref{sec:Beispiele-nichtlinear-GLS} d).
Seien $I=\left[a,b\right]$, $u_{a},u_{b}\in\mathbb{R}$ und $f:\mathbb{R}\to\mathbb{R}$
stetig differenzierbar. Suche eine Lösung $u:\left[a,b\right]\to\mathbb{R}$
von: 
\begin{align*}
u''\left(x\right)=f\left(u\left(x\right)\right) & \qquad\fall_{x\in I} & u\left(a\right) & =u_{a} & u\left(b\right) & =u_{b}
\end{align*}

\begin{itemize}
\item Zerlege $I$ in $N+1$ äquidistante Teilintervalle mit Länge: 
\begin{align*}
h & =\frac{b-a}{N+1}
\end{align*}

\item 
\begin{align*}
u''\left(x\right) & \approx\frac{1}{h^{2}}\left(u\left(x_{i+1}\right)-2u\left(x_{i}\right)+u\left(x_{i-1}\right)\right)
\end{align*}

\item Verlange: 
\begin{align*}
\frac{1}{h^{2}}\left(u_{i+1}-2u_{i}+u_{i-1}\right) & =f\left(u_{i}\right)
\end{align*}
Gesucht sind $u_{1},...,u_{N}$.\\
Dann erfüllt der Vektor $u=\left(u_{1},...,u_{N}\right)^{T}$ die
nichtlineare Gleichung: 
\begin{align*}
H\left(u\right) & :=\begin{pmatrix}2 & -1 & 0 & \dots & 0\\
-1 & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & -1\\
0 & \dots & 0 & -1 & 2
\end{pmatrix}u+\begin{pmatrix}h^{2}f\left(u_{1}\right)-u_{a}\\
h^{2}f\left(u_{2}\right)\\
\vdots\\
h^{2}f\left(u_{n-1}\right)\\
h^{2}f\left(u_{n}\right)-u_{b}
\end{pmatrix}=\begin{pmatrix}0\\
\vdots\\
0
\end{pmatrix}
\end{align*}
%DATE: Mi 28.11.12\emph{}\\
\emph{Ziel:} Löse diese Gleichung mit dem Newton-Verfahren.\\
\emph{Vorgehen:} Bestimme iterativ: 
\begin{align*}
u^{\left(k+1\right)} & =u^{\left(k\right)}-\left(\DD H\left(u^{(k)}\right)\right)^{-1}H\left(u^{(k)}\right)
\end{align*}
Berechne dafür $v^{(k+1)}$ als Lösung von 
\begin{align*}
\DD H\left(u^{\left(k\right)}\right)v^{\left(k+1\right)} & =-H\left(u^{(k)}\right)
\end{align*}
und setze: 
\begin{align*}
u^{(k+1)} & =u^{(k)}+v^{(k+1)}
\end{align*}
Es gilt: 
\begin{align*}
\DD H\left(u^{\left(k\right)}\right) & =\begin{pmatrix}2 & -1 & 0 & \dots & 0\\
-1 & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & -1\\
0 & \dots & 0 & -1 & 2
\end{pmatrix}+h^{2}\begin{pmatrix}f'\left(u_{1}^{(k)}\right) & 0 & \dots & \dots & 0\\
0 & \ddots & \ddots &  & \vdots\\
\vdots & \ddots & \ddots & \ddots & \vdots\\
\vdots &  & \ddots & \ddots & 0\\
0 & \dots & \dots & 0 & f'\left(u_{n}^{(k)}\right)
\end{pmatrix}
\end{align*}
Das zu lösende Gleichungssystem ist:
\begin{align*}
\begin{pmatrix}2+h^{2}f'\left(u_{1}^{\left(k\right)}\right) & -1 &  & 0\\
-1 & \ddots & \ddots\\
 & \ddots & \ddots & -1\\
0 &  & -1 & 2+h^{2}f'\left(u_{n}^{\left(k\right)}\right)
\end{pmatrix}v^{(k+1)} & =\underbrace{\begin{pmatrix}u_{a}-2u_{1}^{\left(k\right)}+u_{2}^{\left(k\right)}-h^{2}f\left(u_{1}^{\left(k\right)}\right)\\
u_{1}^{\left(k\right)}-2u_{2}^{\left(k\right)}+u_{3}^{\left(k\right)}-h^{2}f\left(u_{2}^{\left(k\right)}\right)\\
\vdots\\
u_{n-1}^{\left(k\right)}-2u_{n}^{\left(k\right)}+u_{b}-h^{2}f\left(u_{n}^{\left(k\right)}\right)
\end{pmatrix}}_{=:b^{\left(k\right)}}
\end{align*}
Für $k\ll1$ ist $\DD H\left(u^{(k)}\right)$ positiv definit und
dann existiert eine eindeutige Lösung dieser Gleichung. (Wie kann
man das zeigen?) Mittels Gauß-Elimination sehen wir, dass $v^{(k+1)}$
durch ein lineares Gleichungssystem der folgenden Form berechnet werden
kann: 
\begin{align*}
\underbrace{\begin{pmatrix}1 & 0 & \dots & \dots & 0\\
l_{2} & \ddots & \ddots &  & \vdots\\
0 & \ddots & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & \ddots & 0\\
0 & \dots & 0 & l_{n} & 1
\end{pmatrix}}_{=:L}\cdot\underbrace{\begin{pmatrix}r_{1} & -1 & 0 & \dots & 0\\
0 & \ddots & \ddots & \ddots & \vdots\\
\vdots & \ddots & \ddots & \ddots & 0\\
\vdots &  & \ddots & \ddots & -1\\
0 & \dots & \dots & 0 & r_{n}
\end{pmatrix}}_{=:R}v^{\left(k+1\right)} & =b^{\left(k\right)}
\end{align*}
Dabei ist: 
\begin{align*}
r_{1} & =2+h^{2}f'\left(u_{1}^{\left(k\right)}\right)\\
r_{i} & =2+h^{2}f'\left(u_{i}^{\left(k\right)}\right)-\frac{1}{r_{i-1}}\\
l_{i} & =-\frac{1}{r_{i-1}}
\end{align*}
Wir lösen nun 
\begin{align*}
LRv^{\left(k+1\right)}=b^{(k)}
\end{align*}
indem wir erst $Ly=b^{(k)}$ lösen, das heißt: 
\begin{align*}
y_{1} & =u_{a}-2u_{1}^{\left(k\right)}+u_{2}^{\left(k\right)}-h^{2}f\left(u_{1}^{\left(k\right)}\right)\\
y_{i} & =u_{i-1}^{\left(k\right)}-2u_{i}^{\left(k\right)}+u_{i+1}^{\left(k\right)}-h^{2}f\left(u_{i}^{\left(k\right)}\right)+\frac{1}{r_{i-1}}y_{i-1}\\
y_{n} & =u_{n-1}^{\left(k\right)}-2u_{n}^{\left(k\right)}+u_{b}-h^{2}f\left(u_{n}^{(k)}\right)+\frac{1}{r_{n-1}}y_{n-1}
\end{align*}
Danach lösen wir $Rv^{(k+1)}=y$ durch Rückwärtssubstitution.\\
\emph{Achtung:} Speichere \textbf{auf keinen Fall} die komplette $\left(n\times n\right)$-Matrix,
da dies eine enorme Speicherplatzverschwendung ist!
\end{itemize}

\section{Abbruchkriterien beim Newton-Verfahren}
\begin{enumerate}
\item Limitiere die Anzahl der Iterationen, schon um Endlosschleifen durch
fehlerhafte Programmierung zu vermeiden.
\item Breche ab, wenn das Verfahren nicht konvergiert, also zum Beispiel
wenn $x^{\left(k\right)}$ nicht im Definitionsbereich liegt.
\item Breche ab, wenn das Newton-Verfahren genau genug ist, das heißt der
Fehler 
\begin{align*}
e^{\left(k\right)} & :=\norm{x^{*}-x^{\left(k\right)}}
\end{align*}
klein genug ist. Realistisch ist es, abzubrechen, wenn $\norm{x^{\left(k+1\right)}-\left(k\right)}<\text{tol}$
ist. Dabei ist tol (Toleranz) eine vorgegebene Zahl.
\end{enumerate}

\section{Das gedämpfte Newton-Verfahren}

\emph{Beobachtung:} Im Newton-Verfahren liefert die ,,Korrektur``
\begin{align*}
S^{\left(k\right)} & =-\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{(k)}\right)
\end{align*}
eine Richtung, in der $\abs f$ abnimmt. Wähle nun $\lambda\in(0,1]$
und definiere:
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}+\lambda S^{\left(k\right)}
\end{align*}
Die Strategie ist, $\lambda$ so gedämpft zu wählen, dass $\abs f$
abnimmt. Dahinter steckt die Idee, dass weniger manchmal mehr ist.

\textcolor{green}{TODO: Abbildung einfügen}

Betrachte zunächst $n=1$. Für $f\left(x^{\left(k\right)}\right)\not=0\not=f'\left(x^{\left(k\right)}\right)$
gilt: 
\begin{align*}
g\left(\lambda\right) & =f\left(x^{\left(k\right)}+\lambda S^{\left(k\right)}\right)\\
g'\left(0\right) & =f'\left(x^{\left(k\right)}\right)\cdot\underbrace{S^{\left(k\right)}}_{=\frac{f\left(x^{\left(k\right)}\right)}{f'\left(x^{\left(k\right)}\right)}}=-f\left(x^{\left(k\right)}\right)\begin{cases}
<0 & \text{falls}\; f\left(x^{\left(k\right)}\right)>0\\
>0 & \text{falls}\; f\left(x^{\left(k\right)}\right)<0
\end{cases}
\end{align*}
Das heißt für eine kleines $\lambda\in(0,1]$ gilt: 
\begin{align*}
\abs{f\left(x^{\left(k\right)}+\lambda S^{\left(k\right)}\right)} & <\abs{f\left(x^{\left(k\right)}\right)}
\end{align*}
\emph{Ziel:} Verallgemeinere dies auf $\mathbb{R}^{n}$. Es seien
$f:\mathbb{R}^{n}\to\mathbb{R}^{n}$ und $x^{\left(k\right)}$ gegeben.
Definiere: 
\begin{align*}
S^{\left(k\right)} & =-\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)
\end{align*}
Wir wollen $f\left(x\right)=0$ erreichen. Ziel ist es $\norm{f\left(x^{\left(k\right)}\right)}$
kleiner zu machen (in einer geeigneten Norm). Versuche $\lambda$
in
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}+\lambda S^{\left(k\right)}
\end{align*}
so zu wählen, dass gilt: 
\begin{align*}
\norm{f\left(x^{\left(k+1\right)}\right)} & <\norm{f\left(x^{\left(k\right)}\right)}\qquad\text{(M)}
\end{align*}
Die Wahl der Norm ist nicht trivial!\\
\emph{Problem:} Das Newton-Verfahren ist affin-invariant. Das Nullstellenproblem
$f\left(x\right)=0$ ist für eine invertierbare Matrix $A$ äquivalent
zu: 
\begin{align*}
f_{A}\left(x\right) & =Af\left(x\right)=0
\end{align*}
Zum Beispiel eine Diagonalmatrix skaliert die Zeilen. Das Newton-Verfahren
bemerkt $A$ gar nicht! 
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}-\left(\DD f_{A}\left(x^{\left(k\right)}\right)\right)^{-1}f_{A}\left(x^{\left(k\right)}\right)=\\
 & =x^{\left(k\right)}-\left(A\DD f\left(x^{\left(k+1\right)}\right)\right)^{-1}Af\left(x^{\left(k\right)}\right)=\\
 & =x^{\left(k\right)}-\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)
\end{align*}
Verlange daher einen modifizierten Monotonietest (M) der affin-invariant
ist.

(AIM): Teste, ob gilt:
\begin{align*}
\norm{\left(\DD f\left(\hat{x}\right)\right)^{-1}f\left(x^{\left(k+1\right)}\right)}_{2} & \le\norm{\left(\DD f\left(\hat{x}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)}_{2}
\end{align*}



\subsection*{Bemerkung}
\begin{enumerate}[label=\roman*)]
\item Die rechte Seite enthält für $\hat{x}=x^{\left(k\right)}$ gerade
die Newton-Korrektur $S^{\left(k\right)}$.
\item Die linke Seite benötigt die Lösung eines linearen Gleichungssystems.
Jetzt sei stets $\hat{x}=x^{\left(k\right)}$ und definiere: 
\begin{align*}
\norm z_{\left(k\right)} & :=\norm{\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}z}_{2}
\end{align*}

\end{enumerate}

\section{Lemma}

Es sei $x^{(k)}\in\mathbb{R}^{n}$ mit $f\left(x^{\left(k\right)}\right)\neq0$
gegeben, $f$ sei in einer Umgebung von $x^{\left(k\right)}$ zweimal
stetig differenzierbar und $\DD f\left(x^{\left(k\right)}\right)$
sei invertierbar. Betrachte 
\begin{align*}
S^{\left(k\right)} & :=-\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)\\
x^{\left(k+1\right)} & :=x^{\left(k\right)}+\lambda S^{\left(k\right)}
\end{align*}
und $\norm{\cdot}_{\left(k\right)}$ wie oben. Dann existiert ein
$x\in\left(0,1\right)$ mit:
\begin{align*}
\norm{f\left(x^{\left(k+1\right)}\right)}_{\left(k\right)} & <\norm{f\left(x^{\left(k\right)}\right)}_{\left(k\right)}
\end{align*}



\subsubsection*{Beweis}

Definiere:
\begin{align*}
B & :=\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}\\
f_{B}\left(x\right) & :=Bf\left(x\right)
\end{align*}
\begin{align*}
g\left(x\right) & :=\left(f_{B}\left(x\right)\right)^{\TT}\cdot f_{B}\left(x\right)=\norm{\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x\right)}_{2}^{2}=\norm{f\left(x\right)}_{\left(k\right)}^{2}
\end{align*}
Die Taylor-Entwicklung von $g$ um $\lambda=0$ ist:
\begin{align*}
g\left(x^{\left(k\right)}+\lambda S^{\left(k\right)}\right) & =g\left(x^{\left(k\right)}\right)+\lambda\nabla g\left(x^{\left(k\right)}\right)\cdot S^{\left(k\right)}+\mathcal{O}_{0}\left(\lambda^{2}\right)
\end{align*}
Wegen
\begin{align*}
\nabla g\left(x\right) & =2\left(f_{B}\left(x\right)\right)^{\TT}\DD\left(f_{B}\left(x\right)\right)\\
\DD f_{B}\left(x\right) & =B\DD f\left(x\right)
\end{align*}
folgt:
\begin{align*}
\left(\nabla g\left(x^{k}\right)\right)\circ S^{\left(k\right)} & =-2\left(f_{B}\left(x^{k}\right)\right)^{\TT}\DD\left(f_{B}\left(x^{k}\right)\right)\cdot\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)=\\
 & =-2\left(Bf\left(x^{k}\right)\right)^{\TT}B\DD f\left(x^{\left(k\right)}\right)\cdot\left(\DD f\left(x^{\left(k\right)}\right)\right)^{-1}f\left(x^{\left(k\right)}\right)=\\
 & =-2\left(f\left(x^{k}\right)\right)^{\TT}B^{\TT}Bf\left(x^{\left(k\right)}\right)=-2\norm{Bf\left(x^{\left(k\right)}\right)}_{2}^{2}<0
\end{align*}
Daraus folgt für genügend kleines $\lambda\in\mathbb{R}_{>0}$:
\begin{align*}
g\left(x^{\left(k\right)}+\lambda S^{\left(k\right)}\right) & <g\left(x^{\left(k\right)}\right)
\end{align*}
\qqed


\section{Algorithmisches Vorgehen beim gedämpften Newton-Verfahren}

Lasse maximal $k_{\text{max}}$ Schleifen laufen, wähle als kleinsten
Dämpfungsparameter $\lambda_{\text{min}}$ und gebe eine kleine Toleranz
tol vor.

\begin{figure}[H]
\noindent \begin{centering}
\tikzstyle{line} = [draw, -latex']
\tikzstyle{startend} = [draw, ellipse, fill=green!20, text centered, minimum height=2em]
\tikzstyle{inout} = [draw, trapezium, trapezium left angle=70, trapezium right angle=-70, fill=brown!30, text centered, minimum height=2em]
\tikzstyle{operation} = [draw, rectangle, fill=blue!20, text centered, minimum height=2em]
\tikzstyle{decision} = [draw, diamond, aspect=2, fill=red!30, inner sep=1pt, text centered, minimum height=2em]
\begin{tikzpicture}[node distance = 2cm, auto]
  % Nodes
  \node [startend] (start) {Start};
  \node [inout, below of=start] (input) {$x^{(0)}$};
  \node [operation, left of=input,node distance=3cm] (init) {$k=0$};
  \node [operation, below of=input, text width=20em] (korrektur) {Berechne Korrektur $S^{(k)}$ als Lösung von:\\ $f'\left(x^{(k)}\right)S^{(k)}=-f\left(x^{(k)}\right)$\\Initialisiere Dämpfung: $\lambda:=1$};
  \node [operation, below of=korrektur, text width=20em,node distance=3cm] (damping) {Dämpfung:\\ 	$x:=x^{(k)} + \lambda S^{(k)}$\\ 	$C_\lambda := 1-\frac{\lambda}{4}$};
  \node [decision, below left of=damping, node distance=4cm] (condition) {$\norm{f(x)}_{(k)}\stackrel{?}{\le} C_\lambda \norm{f\left(x^{(k)}\right)}_{(k)}$};
  \node [decision, right of=condition,node distance=8cm,text width=5em] (cond2) {$\lambda :=\frac{\lambda}{2}$\\ $\lambda\stackrel{?}{\ge}\lambda_{\textnormal{min}}$};
  \node [decision, below right of=condition,node distance=5cm,text width=18em] (cond3) {$x^{(k+1)} :=x$\\\hspace*{1mm} \\ $\Bigg(k\ge k_{\textnormal{max}}$ oder $\norm{f\left(x^{k+1}\right)}_{(k)}\le\textnormal{tol}\Bigg) ?$};
  \node[operation, left of=korrektur, node distance=17em] (increase) {$k=k+1$};
  \node [startend, below of=cond3, node distance=10em] (end) {Ende};
  % Lines
  \path [line] (start) -- (input);
  \path [line] (input) -- (init);
  \path [line] (init) -- (korrektur);
  \path [line] (increase) -- (korrektur);
  \path [line] (korrektur) -- (damping);
  \path [line] (damping) -- (condition.90);
  \path [line] (condition) -- node[near start] {nein} (cond2);
  \path [line] (condition.270) -- node[near start,below] {ja} (cond3.90);
  \path [line] (cond2) |-  node[near start] {ja} (damping);
  \path [line] (cond2.0) |-  node[near start] {nein} (end);
  \path [line] (cond3) --  node[near start] {ja} (end);
  \path [line] (cond3) -|  node[near start] {nein} (increase);
\end{tikzpicture}\caption{Flussdiagramm des Algorithmus}

\par\end{centering}

\end{figure}


%DATE: Mo 3.12.12


\section{Das Sekantenverfahren}

Idee: Zu zwei Schätzwerten $x,y\in\mathbb{R}$ einer Nullstelle von
$f$ wollen wir eine bessere Näherung bestimmen. Dabei wählen wir
$z$ als Schnittpunkt der Geraden durch $\left(x,f\left(x\right)\right)$
und $\left(y,f\left(y\right)\right)$ mit der $x$-Achse. Die Bestimmung
von $z$ geht daher wie folgt:
\begin{align*}
m & :=\frac{f\left(x\right)-f\left(y\right)}{x-y}\\
m & =\frac{f\left(z\right)-f\left(x\right)}{z-x}\approx\frac{-f\left(x\right)}{z-x}\\
z & \approx x-\frac{f\left(x\right)}{m}=x-\frac{x-y}{f\left(x\right)-f\left(y\right)}\cdot f\left(x\right)
\end{align*}
Sekantenverfahren:
\begin{enumerate}
\item Wähle Startwerte $x^{\left(0\right)}$ und $x^{\left(1\right)}$.
\item Setze für $k\in\mathbb{N}$, falls $f\left(x^{\left(k\right)}\right)\not=f\left(x^{\left(k-1\right)}\right)$
ist:
\begin{align*}
x^{\left(k+1\right)} & :=x^{\left(k\right)}-\frac{x^{\left(k\right)}-x^{\left(k-1\right)}}{f\left(x^{\left(k\right)}\right)-f\left(x^{\left(k-1\right)}\right)}\cdot f\left(x^{\left(k\right)}\right)
\end{align*}

\end{enumerate}

\section{Bemerkung}
\begin{enumerate}[label=\alph*)]
\item Das Sekantenverfahren lässt sich aus dem Newton-Verfahren ableiten,
indem man Ableitung durch Differenzenquotient ersetzt.
\item Dies liefert auch Verfahren für Systeme: Ersetze in
\begin{align*}
\DD f\left(x^{\left(k\right)}\right) & =\left(\frac{\partial f_{i}}{\partial x_{j}}\right)_{i,i\in\left\{ 1,\ldots,n\right\} }\left(x^{\left(k\right)}\right)
\end{align*}
die partielle Ableitung $\frac{\partial f_{i}}{\partial x_{j}}\left(x^{\left(k\right)}\right)$
durch den Differenzenquotient:
\begin{align*}
\frac{f_{i}\left(x^{\left(k\right)}\right)-f_{i}\left(x^{\left(k\right)}-\left(0,\ldots,0,x_{j}^{\left(k\right)}-x_{j}^{\left(k-1\right)},0,\ldots,0\right)\right)}{x_{j}^{\left(k\right)}-x_{j}^{\left(k-1\right)}}
\end{align*}

\item Das Sekantenverfahren konvergiert superlinear mit Konvergenzordnung:
\begin{align*}
p & =\frac{1+\sqrt{5}}{2}\approx1,618
\end{align*}
\begin{align*}
\Rightarrow\qquad\norm{x^{\left(k-1\right)}-x^{*}} & \le C\norm{x^{\left(k\right)}-x^{*}}^{p}
\end{align*}

\item Trotz der langsameren Konvergenz als beim Newton-Verfahren ist das
Sekantenverfahren oft effizienter, da keine Ableitung berechnet werden
muss.
\end{enumerate}

\chapter{Interpolation}


\section{Einführung}

Schon in \ref{sec:Fragestellungen} 4. wurde folgendes Problem formuliert:

Gegeben seien Punkte $\left(x_{i},f_{i}\right)$ für $i\in\left\{ 0,\ldots,n\right\} $
mit paarweise verschiedenen $x_{i}$.

Finde eine Funktion $p$ so, dass $p\left(x_{i}\right)=f_{i}$ ist.

Dieses Problem taucht zum Beispiel auf, wenn $\left(x_{i},f_{i}\right)$
Messdaten sind. Dann ist man daran interessiert, was an allen $x$.
In diesem Fall sucht man eine interpolierende Funktion $p$ mit $p\left(x_{i}\right)=f_{i}$.
Dann wertet man $p$ an der Stelle $x$ aus. Weiter Anwendungsfelder:
\selectlanguage{english}%
\begin{itemize}
\item Computer-Aided Design\foreignlanguage{ngerman}{ (CAD)}
\item Computer-Aided Geometric Design\foreignlanguage{ngerman}{ (CAGD)}
\end{itemize}
\selectlanguage{ngerman}%
In diesem Fall soll die Funktion möglichst glatt, das heißt möglichst
oft differenzierbar sein.


\section{Anforderungen an die Interpolationsaufgabe}
\begin{enumerate}
\item Die Funktion soll sich aus einfachen Funktionen zusammensetzen, also
zum Beispiel aus Polynome, stückweisen Polynome, sin, cos, etc. oder
Potenzen von exp bestehen.
\item Zu gegebenen Werten $\left(x_{i},f_{i}\right)$ sollte die Interpolierende
leicht zu berechnen sein.
\item Wenn ich die Daten $\left(x_{i},f_{i}\right)$ leicht ändere, sollte
sich $p$ nur leicht ändern.
\item Eindeutige Lösung
\end{enumerate}

\section{Allgemeine Interpolationsaufgabe}

Sei $I\subseteq\mathbb{R}$ ein Intervall und $g_{i}:I\to\mathbb{R}$
für $i\in\left\{ 0,\ldots,n\right\} $ seien $n+1$ gegebene Funktionen,
so definiere $V:=\text{span}\left\{ g_{i}\big|i\in\left\{ 0,\ldots,n\right\} \right\} $.

Zu Punkten $\left(x_{i},f_{i}\right)$ für $i\in\left\{ 0,\ldots,n\right\} $
mit paarweise verschiedenen $x_{i}$ suchen wir ein $p\in V$, sodass
für alle $i\in\left\{ 0,\ldots,n\right\} $ gilt:
\begin{align*}
p\left(x_{i}\right) & =f_{i}
\end{align*}
Da jedes $p\in V$ eine Darstellung 
\begin{align*}
p & =\sum_{j=0}^{n}a_{j}g_{j}
\end{align*}
besitzt, ist die Aufgabe im Allgemeinen äquivalent dazu, Zahlen $a_{0},...,a_{n}\in\mathbb{R}$
mit 
\begin{align}
\sum_{j=0}^{n}a_{j}g_{j}\left(x_{i}\right) & =f_{i}\label{eq:IA}
\end{align}
für $i\in\left\{ 0,\ldots,n\right\} $ zu finden. Dabei handelt es
sich um ein lineares Gleichungssystem. Für die Koeffizienten $a_{j}$
definieren wir die Matrix: 
\begin{align*}
f & =\begin{pmatrix}g_{0}\left(x_{0}\right) & \ldots & g_{n}\left(x_{0}\right)\\
\vdots &  & \vdots\\
g_{0}\left(x_{n}\right) & \dots & g_{n}\left(x_{n}\right)
\end{pmatrix}=\left(g_{j}\left(x_{i}\right)\right)_{i,j\in\left\{ 0,\ldots,n\right\} }
\end{align*}
Dann ist \eqref{eq:IA} äquivalent zu: 
\begin{align}
A\begin{pmatrix}a_{0}\\
\vdots\\
a_{n}
\end{pmatrix} & =\begin{pmatrix}f_{0}\\
\vdots\\
f_{n}
\end{pmatrix}\label{eq:Mat-IA}
\end{align}



\section{Satz\label{sec:Satz-DetA}}

Die Interpolationsaufgabe hat genau dann eine Lösung, wenn $\det A\not=0$
ist.


\subsubsection*{Beweis}

Im Allgemeinen ist das lineare Gleichungssystem \eqref{eq:Mat-IA}
genau dann eindeutig Lösbar, wenn $\det\left(A\right)\neq0$ ist.\qqed


\section{Polynominterpolation}

Mit der Notation von oben wähle $g_{j}\left(x\right):=x^{j}$ für
$j\in\left\{ 0,\ldots,n\right\} $. Das heißt $p$ ist ein Polynom
$n$-ten Grades. Der Raum aller Polynome maximale $n$-ten Grades
wird mit $\mathbb{P}_{n}$ bezeichnet.


\section{Satz}

Zu gegebenen Punkten $\left(x_{i},f_{i}\right)$ für $i\in\left\{ 0,\ldots,n\right\} $
mit paarweise verschiedenen $x_{i}$ gibt es genau ein $p\in\mathbb{P}_{n}$,
das die Interpolationsaufgabe löst, das heißt:
\begin{align*}
p\left(x_{i}\right) & =f_{i}
\end{align*}



\subsubsection*{Beweis}

\textbf{Eindeutigkeit:} Seien $p,q\in\mathbb{P}_{n}$ mit $p\left(x_{i}\right)=f_{i}=q\left(x_{i}\right)$
für alle $i\in\left\{ 0,...,n\right\} $. Dann wäre $p-q$ ein Polynom
$n$-ten Grades mit $n+1$ vielen Nullstellen. Aus dem Fundamentalsatz
der Algebra folgt dann bereits $p=q$ (betrachte Zerlegung in Linearfaktoren).\\
\textbf{Existenz:} Satz \ref{sec:Satz-DetA} besagt, dass die eindeutige
Lösbarkeit äquivalent ist zu: 
\begin{align*}
\det\begin{pmatrix}1 & x_{0} & \dots & x_{0}^{n}\\
\vdots & \vdots &  & \vdots\\
1 & x_{n} & \dots & x_{n}^{n}
\end{pmatrix} & \sr ={\text{Vandermonde-}}{\text{Determinante}}\prod_{i=0}^{n}\prod_{j=i+1}^{n}\left(x_{i}-x_{j}\right)\stackrel{x_{i}\not=x_{j}}{\neq}0
\end{align*}
Daraus folgt die Behauptung.\qqed

Also löse das Gleichungssystem mit der Vandermonde-Matrix.


\section{Bemerkung}

Will man zu Punkten $\left(x_{i},y_{i}\right)$ für $i\in\left\{ 0,\ldots,n\right\} $
mit paarweise verschiedenen $x_{i}$ das Interpolationspolynom
\begin{align*}
p\left(x\right) & =a_{n}x^{n}+\ldots+a_{0}
\end{align*}
berechnen, so löst man:
\begin{align*}
\left(\begin{array}{cccc}
1 & x_{0} & \ldots & x_{0}^{n}\\
\vdots & \vdots &  & \vdots\\
1 & x_{n} & \ldots & x_{n}^{n}
\end{array}\right)\left(\begin{array}{c}
a_{0}\\
\vdots\\
a_{n}
\end{array}\right) & =\left(\begin{array}{c}
f_{0}\\
\vdots\\
f_{n}
\end{array}\right)
\end{align*}
Problem: Mit dem Gauß-Verfahren benötigt man $\mathcal{O}\left(n^{3}\right)$
Rechenoperationen und dies ist schlecht. Andere Verfahren benötigen
nur $\mathcal{O}\left(n^{2}\right)$ Operationen.


\section{Lagrange-Interpolationsformel}

\emph{Idee:} Wähle die Basis statt aus Monomen aus Polynomen, sodass
\begin{align*}
A & =\left(w_{j}\left(x_{i}\right)\right)_{i,j}
\end{align*}
die Einheitsmatrix ist. Dann gilt: 
\begin{align*}
\sum_{j=0}^{n}a_{j}w_{j}\left(x_{i}\right) & =f_{i}\quad\Leftrightarrow\quad a_{i}=f_{i}
\end{align*}
Wie sind die $w_{j}$ zu wählen? Es muss für $i,j\in\left\{ 0,...,n\right\} $
gelten: 
\begin{align*}
w_{j}\left(x_{i}\right) & =\delta_{i,j}
\end{align*}
Daher hat $w_{j}$ eine Nullstelle in $x_{i}$ für $i\not=j$ und
es folgt: 
\begin{align*}
w_{j}\left(x\right) & =k_{j}\prod_{\substack{i=0\\
i\neq j
}
}\left(x-x_{i}\right)
\end{align*}
Damit $w_{j}\left(x_{j}\right)=1$ ist, muss gelten: 
\begin{align*}
k_{j}\prod_{\substack{i=0\\
i\neq j
}
}^{n}\left(x_{j}-x_{i}\right) & =1
\end{align*}
Daraus folgt: 
\begin{align*}
w_{j}\left(x\right) & =\prod_{\substack{i=0\\
i\neq j
}
}^{n}\frac{x-x_{i}}{x_{j}-x_{i}}
\end{align*}



\section{Satz \textmd{(Lagrange-Interpolationspolynom)\label{sec:Satz-Lagrange-Interpolationspolynom}}}

Das Interpolationspolynom $p\in\mathbb{P}_{n}$ zu den Punkten aus
$\left\{ \left(x_{i},f_{i}\right)\bigg|i\in\left\{ 0,\ldots,n\right\} \right\} $
mit paarweise verschiedenen $x_{i}$ hat die Darstellung: 
\begin{align*}
p\left(x\right) & =\sum_{j=0}^{n}f_{j}w_{j}\left(x\right)
\end{align*}



\subsubsection*{Beweis}

Da $A$ die Einheitsmatrix ist, gilt:
\begin{align*}
\sum_{j=0}^{n}a_{j}w_{j}\left(x_{i}\right) & =f_{i}\quad\Leftrightarrow\quad a_{i}=f_{i}
\end{align*}
\qqed


\section{Bemerkung}

Es gibt effizientere Verfahren, aber zur theoretischen Untersuchung
ist diese Darstellung gut geeignet.

%DATE: Mi 5.12.12

%Vertretung durch Daniel Depner


\section{Definition \textmd{(Interpolationspolynom)}}

Zu vorgegebenen Punkten $\left(x_{i},f_{i}\right)$ für $i\in\left\{ 0,\ldots,n\right\} $
mit paarweise verschiedenen $x_{i}$ bezeichne
\begin{align*}
\left(Pf\big|x_{0},\ldots,x_{n}\right) & \in\mathbb{P}_{n}
\end{align*}
das eindeutig bestimmte \emph{Interpolationspolynom} vom Grad $\le n$.
Später schreiben wir oft
\begin{align*}
f_{i} & :=f\left(x_{i}\right)
\end{align*}
für eine Funktion $f$. Das Interpolationspolynom ist nach Satz \ref{sec:Satz-Lagrange-Interpolationspolynom}:
\begin{align*}
\left(Pf\big|x_{0},\ldots,x_{n}\right)\left(x\right) & =\sum_{j=0}^{n}f_{j}\underbrace{\prod_{\sr{}{i=0}{i\not=j}}^{n}\frac{x-x_{i}}{x_{j}-x_{i}}}_{=w_{j}\left(x\right)}
\end{align*}
Dies ist anfällig gegenüber Auslöschung.


\section{Rekursionsformel von Neville und Aitken}

Will man die Interpolierende nur an einer Stelle $x$ auswerten, so
bietet sich eine rekursive Berechnung an.


\section{Lemma \textmd{(Rekursionsformel)}\label{sec:Lem-Interpolations-Rekursion}}

\begin{align*}
\left(Pf\big|x_{0},\ldots,x_{n}\right)\left(x\right) & =\frac{x-x_{0}}{x_{n}-x_{0}}\left(Pf\big|x_{1},\ldots,x_{n}\right)\left(x\right)+\frac{x_{n}-x}{x_{n}-x_{0}}\left(Pf\big|x_{0},\ldots,x_{n-1}\right)\left(x\right)=:\varphi\left(x\right)
\end{align*}



\subsubsection*{Beweis}

Wegen der Eindeutigkeit des Interpolationspolynoms genügt es, dass
für alle $i\in\left\{ 0,\ldots,n\right\} $ schon $\varphi\left(x_{i}\right)=f_{i}$
gilt. Dazu berechne:
\begin{align*}
\varphi\left(x_{0}\right) & =0+\frac{x_{n}-x_{0}}{x_{n}-x_{0}}\underbrace{\left(Pf\big|x_{0},\ldots,x_{n-1}\right)\left(x_{0}\right)}_{=f_{0}}=f_{0}
\end{align*}
Analog folgt $\varphi\left(x_{n}\right)=f_{n}$. Für $1<i<n$ gilt:
\begin{align*}
\varphi\left(x_{i}\right) & =\frac{x_{i}-x_{0}}{x_{n}-x_{0}}\underbrace{\left(P_{f}\big|_{x_{1},\ldots,x_{n}}\right)\left(x_{i}\right)}_{=f_{i}}+\frac{x_{n}-x_{i}}{x_{n}-x_{0}}\underbrace{\left(P_{f}\big|_{x_{0},\ldots,x_{n-1}}\right)\left(x_{i}\right)}_{=f_{i}}=\\
 & =\frac{f_{i}}{x_{n}-x_{0}}\left(x_{i}-x_{0}+x_{n}-x_{i}\right)=f_{i}
\end{align*}
\qqed


\section{Algorithmus von Neville-Aitken}

Sei $x\in\mathbb{R}$ fest und seien $i_{0},\ldots,i_{k}\in\left\{ 0,\ldots,n\right\} $
paarweise verschieden. Setze:
\begin{align*}
P_{i_{0},\ldots,i_{k}}\left(x\right) & :=\left(Pf\big|x_{i_{0}},\ldots,x_{i_{k}}\right)\left(x\right)
\end{align*}
Dann gilt nach Lemma \ref{sec:Lem-Interpolations-Rekursion}:
\begin{align*}
P_{i_{0},\ldots,i_{k}} & =\frac{x-x_{i_{0}}}{x_{i_{k}}-x_{i_{0}}}P_{i_{1},\ldots,i_{k}}+\frac{x_{i_{k}}-x}{x_{i_{k}}-x_{i_{0}}}P_{i_{0},\ldots,i_{k-1}}=\\
 & =P_{i_{1},\ldots,i_{k}}+\frac{x_{i_{k}}-x}{x_{i_{k}}-x_{i_{0}}}\left(P_{i_{0},\ldots,i_{k-1}}-P_{i_{1},\ldots,i_{k}}\right)
\end{align*}
Damit folgt das Neville-Aitken-Schema:

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture} [node distance=10mm]
  \node (x0) {$x_0$};
  \node [below of=x0] (x1) {$x_1$};
  \node [below of=x1] (x2) {$x_2$};
  \node [below of=x2] (xdots) {$\vdots$};
  \node [below of=xdots,node distance=12mm] (xn) {$x_n$};
  \draw ($(x0) + (0.5,0.5)$) -- ($(xn) + (0.5,-0.5)$);
  % Polynome
  \node [right of =x0, node distance=15mm] (p0) {$f_0=p_0$};
  \node [right of =x1, node distance=15mm] (p1) {$f_1=p_1$};
  \node [right of =x2, node distance=15mm] (p2) {$f_2=p_2$};
  \node [right of =xdots, node distance=15mm] (pdots) {$\vdots$};
  \node [right of =xn, node distance=15mm] (pn) {$f_n=p_n$};
  % zweite Spalte
  \node (p01) at ($(p0)+(2,-0.5)$) {$p_{01}$};
  \node (p12) at ($(p1)+(2,-0.5)$) {$p_{12}$};
  \node (pn-1n) at ($(pn)+(2,0.5)$) {$p_{n-1,n}$};
  \draw (p0) -- (p01) -- (p1);
  \draw (p1) -- (p12) -- (p2);
  \draw (pn) -- (pn-1n);
  % dritte Spalte
  \node (p012) at ($(p01)+(2,-0.5)$) {$p_{012}$};
  \node (pn-2n-1n) at ($(pn-1n)+(2,0.5)$) {$p_{n-2,n-1,n}$};
  \draw (p01) -- (p012) -- (p12);
  \draw (pn-1n) -- (pn-2n-1n);
  % vierte Spalte
  \node (p0n) at ($(p012)!0.5!(pn-2n-1n) +(2,0)$) {$p_{01\ldots n}$};  
  \draw[dotted] (p012) -- (p0n) -- (pn-2n-1n);
\end{tikzpicture}
\par\end{centering}

\caption{Neville-Aitken-Schema}


\end{figure}


Beispiel: $n=3$

\noindent \begin{center}
\begin{tabular}{c||c|c|c|c}
$x_{i}$ & 0 & 1 & 2 & 4\tabularnewline
\hline 
$f_{i}$ & 0 & 1 & 8 & 64\tabularnewline
\end{tabular}
\par\end{center}


\section{Newtonsche Interpolationsformel}

\emph{Idee:} Finde bei bekanntem Interpolationspolynom $\left(Pf\big|x_{0},\ldots,x_{n-1}\right)$
einen Korrekturterm, durch dessen Ergänzung $\left(Pf\big|x_{0},\ldots,x_{n}\right)$
entsteht. Dazu betrachte zunächst folgende Darstellung:


\section{Satz\label{sec:Satz-Newton-dividierte-Differenzen}}

$p_{n}=\left(Pf\big|x_{0},\ldots,x_{n}\right)\in\mathbb{P}_{n}$ lässt
sich schreiben als:
\begin{align*}
p_{n}\left(x\right) & =d_{0}+d_{1}\left(x-x_{0}\right)+d_{2}\left(x-x_{0}\right)\left(x-x_{1}\right)+\ldots+d_{n}\left(x-x_{0}\right)\cdot\ldots\cdot\left(x-x_{n-1}\right)
\end{align*}
Dabei sind die Koeffizienten $d_{k}\in\mathbb{R}$ eindeutig bestimmt
durch:
\begin{align*}
d_{k} & =\frac{f_{k}-p_{k-1}\left(x_{k}\right)}{\left(x_{k}-x_{0}\right)\cdot\ldots\cdot\left(x_{k}-x_{k-1}\right)}
\end{align*}



\subsubsection*{Beweis}

Führe eine Induktion über $n$ durch:

\emph{Induktionsanfang bei $n=0$:} Notwendigerweise gilt $d_{0}=f_{0}$.

\emph{Induktionsschritt $n-1\leadsto n$:} Sei
\begin{align*}
p_{n-1}\left(x\right) & =d_{0}+d_{1}\left(x-x_{0}\right)+d_{2}\left(x-x_{0}\right)\left(x-x_{1}\right)+\ldots+d_{n-1}\left(x-x_{0}\right)\cdot\ldots\cdot\left(x-x_{n-2}\right)\in\mathbb{P}_{n-1}
\end{align*}
mit $p_{n-1}\left(x_{i}\right)=f_{i}$ für alle $i\in\left\{ 0,\ldots,n-1\right\} $,
so verwende den Ansatz:
\begin{align*}
p_{n}\left(x\right) & =p_{n-1}\left(x\right)+d_{n}\left(x-x_{0}\right)\cdot\ldots\cdot\left(x-x_{n-1}\right)
\end{align*}
Da $p_{n}\left(x_{n}\right)=f_{n}$ sein muss, folgt:
\begin{align*}
d_{n} & =\frac{f_{n}-p_{n-1}\left(x_{n}\right)}{\left(x_{n}-x_{0}\right)\cdot\ldots\cdot\left(x_{n}-x_{n-1}\right)}
\end{align*}
\qqed


\section{Bemerkung}

Obige Darstellung hat den Vorteil, dass bei Hinzunahme eines weiteren
Knotens $x_{n+1}$ nur ein Koeffizient neu berechnet werden muss.


\section{Newtonsche dividierte Differenzen\label{sec:Newtonsche-dividierte-Differenzen}}

Wir definieren die Newtonsche dividierte Differenz durch:
\begin{align*}
\left[x_{0},\ldots,x_{n}\right]f & :=d_{n}
\end{align*}
Dabei ist $d_{n}$ wie in Satz \ref{sec:Satz-Newton-dividierte-Differenzen}
der Koeffizient von $x^{n}$ des Interpolationspolynoms. Rekursive
Anwendung liefert folgende Form der Newtonschen Interpolationsformel:
\begin{align*}
\left(Pf\big|x_{0},\ldots,x_{n}\right)\left(x\right) & =\left[x_{0}\right]f+\left(x-x_{0}\right)\left[x_{0},x_{1}\right]f+\left(x-x_{0}\right)\left(x-x_{1}\right)\left[x_{0},x_{1},x_{2}\right]f+\ldots+\\
 & \qquad+\left(x-x_{0}\right)\cdot\ldots\cdot\left(x-x_{n-1}\right)\left[x_{0},\ldots,x_{n}\right]f
\end{align*}
Für eine Funktion $f:\left[a,b\right]\to\mathbb{R}$ und paarweise
verschiedene $x_{i}\in\left[a,b\right]$ für $i\in\left\{ 0,\ldots,n\right\} $
setzen wir immer $f_{i}:=f\left(x_{i}\right)$.

Frage: Wie berechnen wir $\left[x_{0},\ldots,x_{n}\right]f$ und woher
kommt der Name?


\section{Korollar\label{sec:Korollar-Rekursion-Newtion-dividierte-Differenzen}}

Es gilt:
\begin{align*}
\left[x_{0},\ldots,x_{n}\right]f & =\frac{\left[x_{1},\ldots,x_{n}\right]f-\left[x_{0},\ldots,x_{n-1}\right]f}{x_{n}-x_{0}}
\end{align*}



\subsubsection*{Beweis}

Nach Lemma \ref{sec:Lem-Interpolations-Rekursion} gilt:
\begin{align*}
\left(Pf\big|x_{0},\ldots,x_{n}\right)\left(x\right) & =\frac{x-x_{0}}{x_{n}-x_{0}}\left(Pf\big|x_{1},\ldots,x_{n}\right)\left(x\right)+\frac{x_{n}-x}{x_{n}-x_{0}}\left(Pf\big|x_{0},\ldots,x_{n-1}\right)\left(x\right)=:\varphi\left(x\right)
\end{align*}
$\left[x_{0},\ldots,x_{n}\right]f$ ist der Koeffizient vor $x^{n}$
in $\varphi\left(x\right)$.

$\left[x_{1},\ldots,x_{n}\right]f$ ist der Koeffizient vor $x^{n-1}$
in $\left(Pf\big|x_{1},\ldots,x_{n}\right)$.

$\left[x_{0},\ldots,x_{n-1}\right]f$ ist der Koeffizient vor $x^{n-1}$
in $\left(Pf\big|x_{0},\ldots,x_{n-1}\right)$.

Ein Vergleich der Koeffizienten liefert die Behauptung.\qqed


\section{Dreiecksschema zur Berechnung der Newtonschen dividierten Differenzen}

Mit $\left[x_{i}\right]f=f_{i}$ (Interpolationspolynom ist eine Konstante)
und Korollar \ref{sec:Korollar-Rekursion-Newtion-dividierte-Differenzen}
können wir die dividierten Differenzen nach folgendem rekursivem Schema
berechnen:

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture} [node distance=10mm]
  \node (x0) {$x_0$};
  \node [below of=x0] (x1) {$x_1$};
  \node [below of=x1] (x2) {$x_2$};
  \node [below of=x2] (xdots) {$\vdots$};
  \node [below of=xdots,node distance=12mm] (xn) {$x_n$};
  \draw ($(x0) + (0.5,0.5)$) -- ($(xn) + (0.5,-0.5)$);
  % Polynome
  \node [right of =x0, node distance=15mm] (p0) {$f_0=[x_0]f$};
  \node [right of =x1, node distance=15mm] (p1) {$f_1=[x_1]f$};
  \node [right of =x2, node distance=15mm] (p2) {$f_2=[x_2]f$};
  \node [right of =xdots, node distance=15mm] (pdots) {$\vdots$};
  \node [right of =xn, node distance=15mm] (pn) {$f_n=[x_n]f$};
  % zweite Spalte
  \node (p01) at ($(p0)+(3,-0.5)$) {$[x_0,x_1]f$};
  \node (p12) at ($(p1)+(3,-0.5)$) {$[x_1,x_2]f$};
  \node (pn-1n) at ($(pn)+(3,0.5)$) {$[x_{n-1},x_n]f$};
  \draw (p0) -- (p01) -- (p1);
  \draw (p1) -- (p12) -- (p2);
  \draw (pn) -- (pn-1n);
  % dritte Spalte
  \node (p012) at ($(p01)+(3,-0.5)$) {$[x_0,x_1,x_2]f$};
  \node (pn-2n-1n) at ($(pn-1n)+(3,0.5)$) {$[x_{n-2},x_{n-1},x_n]f$};
  \draw (p01) -- (p012) -- (p12);
  \draw (pn-1n) -- (pn-2n-1n);
  % vierte Spalte
  \node (p0n) at ($(p012)!0.5!(pn-2n-1n) +(3,0)$) {$[x_0,x_1,\ldots,x_n]f$};  
  \draw[dotted] (p012) -- (p0n) -- (pn-2n-1n);
\end{tikzpicture}
\par\end{centering}

\caption{Newtonsche dividierte Differenzen}
\end{figure}


Der obere Rand liefert die Koeffizienten in der Newton-Darstellung
aus \ref{sec:Satz-Newton-dividierte-Differenzen} beziehungsweise
\ref{sec:Korollar-Rekursion-Newtion-dividierte-Differenzen}.\qqed


\section{Das Horner-Schema}

Wir wollen die Polynome
\begin{align*}
p\left(x\right) & =\sum_{k=0}^{n}a_{k}x^{k}
\end{align*}
beziehungsweise
\begin{align*}
q\left(x\right) & =\sum_{k=0}^{n}d_{k}\cdot\prod_{j=0}^{k-1}\left(x-x_{j}\right)
\end{align*}
auswerten.


\subsubsection*{Vorwärtssubstitution}

Berechne die Potenzen $x^{k}$, die Produkte $a_{k}x^{k}$ und summiere
auf. Berechne also 
\begin{align*}
u_{0} & =1 & v_{0} & =a_{0}
\end{align*}
und für $k\in\left\{ 1,\ldots,n\right\} $:
\begin{align*}
u_{k} & :=x\cdot u_{k-1} & v_{k} & :=v_{k-1}+a_{k}u_{k}
\end{align*}
Der Wert $v_{n}$ ist dann gerade $p\left(x\right)$.

Hier werden $2n$ Multiplikationen und $n$ Additionen benötigt.


\subsubsection*{Rückwärtssubsitution \textmd{(Horner-Schema)}}

Beispiel:
\begin{align*}
p\left(x\right) & =1+2x+3x^{2}+4x^{3}=1+x\left(2+x\left(3+4x\right)\right)
\end{align*}
Allgemein gilt:
\begin{align*}
p\left(x\right) & =\sum_{k=0}^{n}a_{k}x^{k}=a_{0}+x\left(a_{1}+x\left(a_{2}+\ldots+x\left(a_{n-1}+a_{n}x\right)\ldots\right)\right)
\end{align*}
Definieren wir:
\begin{align*}
w_{n+1} & =0\\
w_{k} & =a_{k}+x\cdot w_{k+1}
\end{align*}
So liefert $w_{0}$ den Wert $p\left(x\right)$.

Dieses Vorgehen ist ein Algorithmus zur Auswertung von $p$ an der
Stelle $x$ und benötigt $n+1$ Additionen und $n+1$ Multiplikationen.
Er spart also $n-2$ Operationen im Vergleich zur Vorwärtssubstitution.

Will man $q\left(x\right)$ auswerten, so geht man entsprechend vor:
\begin{align*}
q\left(x\right) & =d_{0}+d_{1}\left(x-x_{0}\right)+\ldots+d_{n}\left(x-x_{0}\right)\cdot\ldots\cdot\left(x-x_{n-1}\right)=\\
 & =d_{0}+\left(x-x_{0}\right)\left(d_{1}+\left(x-x_{1}\right)\left(d_{2}+\ldots+\left(x-x_{n-2}\right)\left(d_{n-1}+\left(x-x_{n-1}\right)d_{n}\right)\ldots\right)\right)
\end{align*}
In diesem Fall lautet das Horner-Schema für $k\in\left\{ n,n-1,\ldots,0\right\} $:
\begin{align*}
w_{n+1} & :=0\\
w_{k} & =d_{k}+\left(x-x_{n}\right)w_{k+1}
\end{align*}


%DATE: Mo 10.12.12


\section{Satz\label{sec:Satz-Newton-dividierte-Differenzen-Eigenschaften}}

Die Newtonschen dividierten Differenzen, haben die folgenden Eigenschaften:
\begin{enumerate}
\item Die Abbildung $\left[x_{0},\ldots,x_{n}\right]:\mathcal{C}^{0}\left(\mathbb{R}\right)\to\mathbb{R}$
mit $f\mapsto\left[x_{0},\ldots,x_{n}\right]f$ ist linear.
\item Ist $f\left(x\right)=x^{k}$ für $k\in\left\{ 0,\ldots,n\right\} $,
so gilt $\left[x_{0},\ldots,x_{n}\right]f=\delta_{k,n}$.
\item $\left[x_{0},\ldots,x_{n}\right]f$ ist unabhängig von der Reihenfolge
der Argumente $x_{0},\ldots,x_{n}$.
\item Ist $f\in\mathcal{C}^{n}\left(\left[a,b\right],\mathbb{R}\right)$,
so gibt es ein $\xi\in\left(\min\left(x_{0},\ldots,x_{n}\right),\max\left(x_{0},\ldots,x_{n}\right)\right)$
mit: 
\begin{align*}
\left[x_{0},\ldots,x_{n}\right]f & =\frac{f^{\left(n\right)}\left(\xi\right)}{n!}
\end{align*}

\end{enumerate}

\subsubsection*{Beweis}
\begin{enumerate}
\item Seien $f,g\in\mathcal{C}^{0}\left(\mathbb{R}\right)$ und $\alpha\in\mathbb{R}$.
Wähle $p,q\in\mathbb{P}_{n}$ so, dass für alle $i\in\left\{ 0,\ldots,n\right\} $
gilt:
\begin{align*}
p\left(x_{i}\right) & =f\left(x_{i}\right)\\
q\left(x_{i}\right) & =g\left(x_{i}\right)
\end{align*}
Dann folgt für alle $i\in\left\{ 0,\ldots,n\right\} $: 
\begin{align*}
\left(\alpha p+q\right)\left(x_{i}\right) & =\left(\alpha f+g\right)\left(x_{i}\right)
\end{align*}
Das bedeutet, dass $\alpha p+q$ das Interpolationspolynom zu $\alpha f+g$
interpoliert an den Stellen $x_{0},\ldots,x_{n}$ ist. Nach Definition
ist $\left[x_{0},\ldots,x_{n}\right]f$ der Koeffizient von $x^{n}$
des Polynoms $\alpha p+q$. Da $\deg\left(p\right)=\deg\left(g\right)$
ist, ergibt sich dieser als das $\alpha$-fache des Koeffizient von
$x^{n}$ in $p$ addiert zum Koeffizient von $x^{n}$ in $q$ Daraus
folgt: 
\begin{align*}
\left[x_{0},\ldots,x_{n}\right]\left(\alpha f+g\right) & =\alpha\left[x_{0},\ldots,x_{n}\right]f+\left(x_{0},\ldots,x_{n}\right)g
\end{align*}
Also ist $\left[x_{0},\ldots,x_{n}\right]$ eine lineare Abbildung.\qqed[\arabic{enumi})]
\item Wegen $k\in\left\{ 0,\ldots,n\right\} $ ist das Interpolationspolynom
von $f$:
\begin{align*}
\left(Pf|x_{0},\ldots,n\right) & =x^{k}
\end{align*}
Daher ist der Koeffizient von $x^{n}$ nur von Null verschieden, wenn
$k=n$ gilt, das heißt:
\begin{align*}
\left[x_{0},\ldots,x_{k}\right]f & =\begin{cases}
0 & k<n\\
1 & k=n
\end{cases}
\end{align*}
\qqed[\arabic{enumi})]
\item Die Eigenschaft Interpolationspolynom zu sein ist unabhängig von der
Reihenfolge der $x_{0},\ldots,x_{n}$. Also ist der Koeffizient von
$x^{n}$ unabhängig von der Reihenfolge.\qqed[\arabic{enumi})]
\item Betrachte $p\left(x\right)\in\mathbb{P}_{n}$ mit $p\left(x_{i}\right)=f\left(x_{i}\right)$
für $i\in\left\{ 0,\ldots,n\right\} $ und der Darstellung: 
\begin{align*}
p\left(x\right) & =\sum_{j=0}^{n}a_{j}x^{j}
\end{align*}
Dann hat $q:=p-f$ die $n+1$ Nullstellen $x_{0},\ldots,x_{n}$. Ohne
Einschränkung (vergleiche 3.) sei $x_{0}<x_{1}<\ldots<x_{n}$. Nach
dem Satz von Rolle existieren mindestens $n$ Nullstellen von $q'$.
Mindestens eine liegt in jedem offenen Intervall $\left(x_{i-1},x_{i}\right)$
für $i\in\left\{ 0,\ldots,n\right\} $.\\
Somit hat $q''$ mindestens $n-1$ Nullstellen, und iterativ folgt,
dass $q^{\left(n\right)}$ mindestens eine Nullstelle hat. Das heißt,
es existiert ein $\xi\in\left(x_{0},x_{n}\right)$ mit: 
\begin{align*}
0 & =q^{\left(n\right)}\left(\xi\right)=p^{\left(n\right)}\left(\xi\right)-f^{\left(n\right)}\left(\xi\right)=a_{n}n!-f^{\left(n\right)}\left(\xi\right)
\end{align*}
Wegen $a_{n}=\left[x_{0},...,x_{n}\right]f$ folgt: 
\begin{align*}
\left[x_{0},...,x_{n}\right]f & =\frac{f^{\left(n\right)}\left(\xi\right)}{n!}
\end{align*}
\qqed[\arabic{enumi})]
\end{enumerate}
\qqed


\section{Fehlerdarstellung\label{sec:Fehlerdarstellung-IA}}

Sei $I\subseteq\mathbb{R}$ ein Intervall, $f\in\mathcal{C}^{n+1}\left(I\right)$
und $p\in\mathbb{P}_{n}$ das Interpolationspolynom zu $\left(x_{i},f\left(x_{i}\right)\right)$
mit paarweise verschiedenen $x_{i}$, das heißt $p\left(x_{i}\right)=f\left(x_{i}\right)$
für $i\in\left\{ 0,\ldots,n\right\} $.\\
Dann existiert zu jedem Punkt $\bar{x}\in I$ ein Punkt $\xi\left(\bar{x}\right)\in\left(\min\left(x_{0},...,x_{n}\right),\max\left(x_{0},...,x_{n}\right)\right)$
mit: 
\begin{align*}
f\left(\bar{x}\right)-p\left(\bar{x}\right) & =\left(\prod_{j=0}^{n}\left(\bar{x}-x_{j}\right)\right)\cdot\frac{1}{\left(n+1\right)!}f^{\left(n+1\right)}\left(\xi\left(\bar{x}\right)\right)
\end{align*}



\subsubsection*{Beweis}

Falls $\bar{x}=x_{i}$ für ein $i\in\left\{ 0,\ldots,n\right\} $
gilt, so ist die Aussage trivialerweise erfüllt. Sei nun $\bar{x}\neq x_{i}$
für alle $i\in\left\{ 0,\ldots,n\right\} $. Dann sei $\bar{p}\in\mathbb{P}_{n+1}$
das Interpolationspolynom mit folgenden Eigenschaften: 
\begin{align*}
\fall_{i\in\left\{ 0,\ldots,n\right\} }:\,\bar{p}\left(x_{i}\right) & =f\left(x_{i}\right) & p\left(\bar{x}\right) & =f\left(\bar{x}\right)
\end{align*}
Daraus folgt: 
\begin{align}
\bar{p}\left(x\right) & =p\left(x\right)+\prod_{j=0}^{n}\left(x-x_{j}\right)\left[\bar{x},x_{0},...,x_{n}\right]f\label{eq:p-quer}
\end{align}
Aus \ref{sec:Satz-Newton-dividierte-Differenzen-Eigenschaften} 4.
folgt die Existenz eines $\xi\in\left(\min\left(\bar{x},x_{0},...,x_{n}\right),\max\left(\bar{x},x_{0},...,x_{n}\right)\right)$
mit: 
\begin{align}
\left[\bar{x},x_{0},...,x_{n}\right]f & =\frac{f^{\left(n+1\right)}\left(\xi\right)}{\left(n+1\right)!}\label{eq:NDDf}
\end{align}
Wegen $\bar{p}\left(\bar{x}\right)=f\left(\bar{x}\right)$ folgt aus
\eqref{eq:p-quer} und \eqref{eq:NDDf}: 
\begin{align*}
f\left(\overline{x}\right) & =\bar{p}\left(\bar{x}\right)=p\left(\bar{x}\right)+\prod_{j=0}^{n}\left(x-x_{j}\right)\frac{f^{\left(n+1\right)}\left(\xi\right)}{\left(n+1\right)!}
\end{align*}
\qqed


\section{Fehlerabschätzung}

Sei $f\in\mathcal{C}^{n+1}\left(\left[a,b\right]\right)$ und $p\in\mathbb{P}_{n}$
das Interpolationspolynom zu paarweise verschiedenen Stützstellen
$x_{0},\ldots,x_{n}\in\left[a,b\right]$, das heißt für $i\in\left\{ 0,\ldots,n\right\} $
gilt: 
\begin{align*}
p\left(x_{i}\right) & =f\left(x_{i}\right)
\end{align*}
Dann gilt: 
\begin{align*}
\norm{f-p}_{\infty,\left[a,b\right]} & :=\sup_{x\in\left[a,b\right]}\abs{f\left(x\right)-p\left(x\right)}\leq\frac{\norm{f^{\left(n+1\right)}}_{\infty,\left[a,b\right]}}{\left(n+1\right)!}\left(b-a\right)^{n+1}
\end{align*}



\subsubsection*{Beweis}

Mit \ref{sec:Fehlerdarstellung-IA} folgt: 
\begin{align*}
\abs{f\left(x\right)-p\left(x\right)}=\abs{\prod_{j=0}^{n}\underbrace{\left(x-x_{i}\right)}_{\leq\abs{b-a}}\cdot\frac{f^{\left(n+1\right)}\left(\xi\left(x\right)\right)}{\left(n+1\right)!}} & \leq\left(b-a\right)^{n+1}\frac{\norm{f^{n+1}}_{\infty,\left[a,b\right]}}{\left(n+1\right)!}
\end{align*}
\qqed


\section{Bemerkungen}
\begin{enumerate}
\item In der Fehlerdarstellung wurde folgendes gezeigt: 
\begin{align*}
\abs{f\left(x\right)-p\left(x\right)} & =\abs{\frac{f^{\left(n+1\right)}\left(\xi\left(x\right)\right)}{\left(n+1\right)!}\cdot\prod_{j=0}^{n}\left(x-x_{i}\right)}
\end{align*}
Versuche die rechte Seite klein zu machen!\\
Den Wert $f^{\left(n+1\right)}\left(\xi\left(x\right)\right)$ kennen
wir in der Regel nicht!\\
\emph{Ziel:} Mache den Ausdruck $\prod_{j=0}^{n}\left(x-x_{j}\right)$
klein, das heißt wähle $x_{j}\in\left[a,b\right]$ geeignet.\\
Betrachte zunächst $\left[a,b\right]=\left[-1,1\right]$ und wähle
$x_{j}$ als die Nullstellen des Tschebyschow-Polynoms: 
\begin{align*}
p_{n+1}\left(x\right) & =\cos\left(\left(n+1\right)\arccos\left(x\right)\right)
\end{align*}
Also sind die Nullstellen: 
\begin{align*}
x_{j} & =\cos\left(\frac{2j+1}{n+1}\cdot\frac{\pi}{2}\right)
\end{align*}
Für allgemeine Intervalle $\left[a,b\right]$ müssen die Nullstellen
transformiert werden: 
\begin{align*}
\bar{x}_{j} & =2\cdot\frac{x_{j}-a}{b-a}-1
\end{align*}

\item Seien zu jedem $n\in\mathbb{N}$ die paarweise verschiedenen Knoten
\begin{align*}
\left\{ x_{0}^{\left(n\right)},\ldots,x_{n}^{\left(n\right)}\right\}  & \subseteq\left[a,b\right]
\end{align*}
gegeben.\\
\textcolor{green}{TODO: Abbildungen}\\
Das heißt für jedes $n$ wähle $n+1$ Stützstellen für die Interpolation.
Sei $f\in\mathcal{C}\left(\left[a,b\right],\mathbb{R}\right)$ gegeben
und sei $p\in\mathbb{P}_{n}$ so gewählt, dass für alle $i\in\left\{ 0,\ldots,n\right\} $
gilt: 
\begin{align*}
p_{n}\left(x_{i}^{\left(n\right)}\right) & =f\left(x_{i}^{\left(n\right)}\right)
\end{align*}
Frage: Konvergiert $p_{n}\to f_{n}$ für $n\to\infty$?\\
Es gelten folgende Sätze:

\begin{description}
\item [{Satz~von~Marcinkiewicz}] \hspace*{1mm}\\
Zu jedem $f\in\mathcal{C}\left(\left[a,b\right],\mathbb{R}\right)$
können Knoten $\left(x_{0}^{\left(n\right)},\ldots,x_{n}^{\left(n\right)}\right)\subseteq\left[a,b\right]$
gewählt werden, sodass die Folge der Interpolationspolynome $p_{n}$
gegen $f$ konvergiert, das heißt: 
\begin{align*}
\norm{f-p_{n}}_{\infty,\left[a,b\right]} & \xrightarrow{n\to\infty}0
\end{align*}

\item [{Satz~von~Faber}] \hspace*{1mm}\\
Zu jeder vorgegeben Folge von Knotenmengen $\left\{ x_{0}^{\left(n\right)},\ldots,x_{n}^{\left(n\right)}\right\} \subseteq\left[a,b\right]$
existiert eine Funktion $f\in\mathcal{C}\left(\left[a,b\right],\mathbb{R}\right)$,
sodass gilt: 
\begin{align*}
\norm{f-p_{n}}_{\infty,\left[a,b\right]} & \xrightarrow{n\to\infty}\infty
\end{align*}

\end{description}
\end{enumerate}

\section{Definition \textmd{(Hermite-Interpolation)}}

Gegeben seien die Knoten $x_{0}<x_{1}<\ldots<x_{m}$ und Werte $f_{i}^{\left(k\right)}$
für $k\in\left\{ 0,\ldots,n_{i}-1\right\} $ und $i\in\left\{ 0,\ldots,m\right\} $.
Damit sind 
\begin{align*}
n+1 & =\sum_{i=0}^{m}n_{i}
\end{align*}
Werte gegeben.

Ein Polynom $p\in\mathbb{P}_{n}$ mit der Eigenschaft 
\begin{align}
p^{\left(k\right)}\left(x_{i}\right) & =f_{i}^{\left(k\right)}\label{eq:hermite-interpolation}
\end{align}
für alle $i\in\left\{ 0,\ldots,m\right\} $ und $k\in\left\{ 0,\ldots,n_{i}-1\right\} $
heißt \emph{Hermite-Interpolationspolynom}.


\section{Satz}

Das Hermite-Interpolationspolynom existiert und ist eindeutig bestimmt.


\subsubsection*{Beweis}
\begin{description}
\item [{Eindeutigkeit:}] Seien $p_{1},p_{2}\in\mathbb{P}_{n}$ mit der
Eigenschaft \eqref{eq:hermite-interpolation}. Dann gilt 
\begin{align*}
q & :=p_{1}-p_{2}\in\mathbb{P}_{n}
\end{align*}
und $q$ hat $n+1$ Nullstellen (mit Vielfachheit gezählt). Daraus
folgt $q=0$ und somit $p_{1}=p_{2}$.
\item [{Existenz:}] Betrachte die lineare Abbildung: 
\begin{align*}
L:\mathbb{P}_{n} & \to\mathbb{R}^{n+1}\\
p & \mapsto L\left(p\right):=\underbrace{\left(\begin{array}{c}
p\left(x_{0}\right)\\
p'\left(x_{0}\right)\\
\vdots\\
p^{\left(n_{i}-1\right)}\left(x_{0}\right)\\
p\left(x_{1}\right)\\
\vdots\\
p\left(x_{m}\right)\\
\vdots\\
p^{\left(n_{m}-1\right)}\left(x_{m}\right)
\end{array}\right)}_{=:v}
\end{align*}
Aus den obigen Überlegungen folgt, dass $L$ injektiv ist. Wegen $\dim\left(\mathbb{P}_{n}\right)=n+1$
ist $L$ auch surjektiv. Dies zeigt die eindeutige Existenz eines
Interpolationspolynoms als Urbild $p=L^{-1}\left(v\right)$. \qqed
\end{description}
%DATE: Mi 12.12.12

Jetzt führen wir folgende Notation ein:
\begin{align*}
\overbrace{\underbrace{x_{0}}_{=:t_{0}}=\ldots=\underbrace{x_{0}}_{=:t_{n_{0}-1}}}^{n_{0}\text{-mal}}<\overbrace{\underbrace{x_{1}}_{=:t_{n_{0}}}=\ldots=\underbrace{x_{1}}_{=:t_{n_{0}+n_{1}-1}}}^{n_{1}\text{-mal}}<x_{2}\ldots<\overbrace{x_{m}=\ldots=\underbrace{x_{1}}_{=:t_{n}}}^{n_{m}\text{-mal}}
\end{align*}
Die Interpolationsbedingungen sind für $j\in\left\{ 0,\ldots,n\right\} $:
\begin{align}
p^{\left(s_{j}\right)}\left(t_{j}\right) & =f^{\left(s_{j}\right)}\left(t_{j}\right)\label{eq:int-bed}
\end{align}
\begin{align*}
s_{j} & =\max\left\{ x\big|t_{j}=t_{j-x}\right\} 
\end{align*}
Wir bezeichnen mit $P\left(f|t_{0},\ldots,t_{n}\right)$ das Interpolationspolynom
in $\mathbb{P}_{n}$, welches \eqref{eq:int-bed} erfüllt.


\section{Bestimmung des Hermite-Interpolationspolynoms}


\subsubsection*{Definition}

Wir bezeichnen für beliebige Stützstellen $t_{i}$ mit $i\in\left\{ 0,\ldots,n\right\} $
jeweils den Koeffizienten von $x^{k-i}$ des Hermite-Polynoms $P\left(f\big|t_{i},\ldots,t_{k}\right)\in\mathbb{P}_{k-i}$
wieder mit $\left[t_{i},\ldots,t_{k}\right]f$ . Für $t_{0}=\ldots=t_{k}$
ergibt sich als Folgerung:
\begin{align*}
\left[t_{0},\ldots,t_{k}\right]f & =\frac{f^{\left(k\right)}\left(t_{0}\right)}{k!}
\end{align*}
Betrachte dafür das Taylor-Polynom:
\begin{align*}
f\left(t_{0}\right)+f'\left(t_{0}\right)\left(t-t_{0}\right)+\ldots+\frac{f^{\left(k\right)}\left(t_{0}\right)}{k!}\left(t-t_{0}\right)^{k} & =P\left(f\big|t_{0},\ldots,t_{k}\right)
\end{align*}



\subsubsection*{Lemma}

Gegeben seien $t_{0},\ldots,t_{k}\in\mathbb{R}$. Dann gilt für $t_{i},t_{j}\in\left\{ t_{0},\ldots,t_{k}\right\} $:
\begin{align*}
\left[t_{0},\ldots,t_{k}\right]f & =\begin{cases}
{\displaystyle \frac{\left[t_{0},\ldots,t_{i-1},t_{i+1},\ldots,t_{k}\right]f-\left[t_{0},\ldots,t_{j-1},t_{j+1},\ldots,t_{k}\right]f}{t_{i}-t_{j}}} & \text{für }t_{i}\not=t_{j}\\
\frac{f^{\left(k\right)}\left(t_{0}\right)}{k!} & \text{falls }t_{0}=\ldots=t_{k}
\end{cases}
\end{align*}



\subsubsection*{Beweis}

Dies kann unmittelbar nachgerechnet werden: Für $t_{i}\not=t_{j}$
gilt nach dem Lemma \eqref{sec:Lem-Interpolations-Rekursion} von
Aitken:
\begin{align*}
P\left(f\big|t_{0},\ldots,t_{k}\right)\left(x\right) & =\frac{\left(x-t_{i}\right)P\left(f\big|t_{0},\ldots,t_{i-1},t_{i+1},\ldots,t_{k}\right)\left(x\right)}{t_{j}-t_{i}}-\\
 & \qquad-\frac{\left(x-t_{j}\right)P\left(f\big|t_{0},\ldots,t_{j-1},t_{j+1},\ldots,t_{k}\right)\left(x\right)}{t_{j}-t_{i}}
\end{align*}
Ein Vergleich der Koeffizienten von $x^{k}$ liefert die Behauptung.\qqed[Lemma]


\subsubsection*{Beispiel}

Für $t_{0}=0$, $t_{1}=t_{2}=t_{3}=\frac{1}{2}$, $t_{4}=1$ bestimme
$P\in\mathbb{P}_{4}$ so, dass gilt:
\begin{align*}
P\left(0\right) & =1 & P\left(1\right) & =\frac{5}{2}\\
P\left(\frac{1}{2}\right) & =\frac{3}{2} & P'\left(\frac{1}{2}\right) & =\frac{1}{2} & P''\left(\frac{1}{2}\right) & =0
\end{align*}


\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture} [node distance=10mm]
  \node (ti) {$t_i$};
  \node [below of=ti] (t0) {$0$};
  \node [below of=t0] (t1) {$\frac{1}{2}$};
  \node [below of=t1] (t2) {$\frac{1}{2}$};
  \node [below of=t2] (t3) {$\frac{1}{2}$};
  \node [below of=t3] (t4) {$1$};
  \draw ($(ti) + (0.5,0.5)$) -- ($(t4) + (0.5,-0.5)$);
  \draw ($(ti) + (-0.5,-0.5)$) -- ($(ti) + (15,-0.5)$);
  % Polynome
  \node [right of =ti, node distance=15mm] (fi) {$f_i=[t_i]f$};
  \node [right of =t0, node distance=15mm] (f0) {$1$};
  \node [right of =t1, node distance=15mm] (f1) {$\frac{3}{2}$};
  \node [right of =t2, node distance=15mm] (f2) {$\frac{3}{2}$};
  \node [right of =t3, node distance=15mm] (f3) {$\frac{3}{2}$};
  \node [right of =t4, node distance=15mm] (f4) {$\frac{5}{2}$};
  % zweite Spalte
  \node [right of =fi, node distance=25mm] (pii+1) {$[t_i,t_{i+1}]f$};
  \node [right of =f0, node distance=25mm] (p01) {$\frac{\frac{3}{2}-1}{\frac{1}{2}-0}=1$};
  \node [right of =f1, node distance=25mm,blue] (p12) {$f'\left(\frac{1}{2}\right)=\frac{1}{2}$};
  \node [right of =f2, node distance=25mm,blue] (p23) {$f'\left(\frac{1}{2}\right)=\frac{1}{2}$};
  \node [right of =f3, node distance=25mm] (p34) {$\frac{\frac{5}{2}-\frac{3}{2}}{1-\frac{1}{2}}=2$};
  % dritte Spalte
  \node [right of =pii+1, node distance=25mm] (pii+1i+2) {$[t_i,t_{i+1},t_{i+2}]f$};
  \node [right of =p01, node distance=25mm] (p012) {$\frac{\frac{1}{2}-1}{\frac{1}{2}-0}=-1$};
  \node [right of =p12, node distance=25mm,red] (p123) {$\frac{1}{2}f''\left(\frac{1}{2}\right)=0$};
  \node [right of =p23, node distance=25mm] (p234) {$\frac{2-\frac{1}{2}}{1-\frac{1}{2}}=3$};
  % vierte Spalte
  \node [right of =pii+1i+2, node distance=30mm] (pii+1i+2i+3) {$[t_i,t_{i+1},t_{i+2},t_{i+3}]f$};
  \node [right of =p012, node distance=30mm] (p0123) {$\frac{0-(-1)}{\frac{1}{2}-0}=2$};
  \node [right of =p123, node distance=30mm] (p1234) {$\frac{3-0}{1-\frac{1}{2}}=6$};
  % vierte Spalte
  \node [right of =pii+1i+2i+3, node distance=35mm] (pii+1i+2i+3i+4) {$[t_i,t_{i+1},t_{i+2},t_{i+3},t_{i+4}]f$};
  \node [right of =p0123, node distance=35mm] (p01234) {$\frac{6-2}{1-0}=4$};
  % Linien
 \draw (f0) -- (p01) -- (f1);
 \draw (f1) -- (p12) -- (f2);
 \draw (f2) -- (p23) -- (f3);
 \draw (f3) -- (p34) -- (f4);

 \draw (p01) -- (p012) -- (p12);
 \draw (p12) -- (p123) -- (p23);
 \draw (p23) -- (p234) -- (p34);

 \draw (p012) -- (p0123) -- (p123);
 \draw (p123) -- (p1234) -- (p234);

 \draw (p0123) -- (p01234) -- (p1234);
\end{tikzpicture}
\par\end{centering}

\caption{Hermite-Interpolationskoeffizienten}
\end{figure}


Das Interpolationspolynom ist also:
\begin{align*}
P\left(x\right) & =\underline{1}+\underline{1}\cdot\left(x-0\right)\underline{-1}\cdot x\left(x-\frac{1}{2}\right)+\underline{2}\cdot x\left(x-\frac{1}{2}\right)^{2}+\underline{4}\cdot x\left(x-\frac{1}{2}\right)^{3}
\end{align*}



\chapter{Numerische Integration}


\section{Einführung}

Häufig sind Integrale $\int_{a}^{b}f\left(x\right)\dd x=:I\left(f\right)$
nicht analytisch lösbar (keine explizite Stammfunktion berechenbar),
oder aber die Stammfunktion ist sehr kompliziert.

\emph{Ziel:} Nähere $\int_{a}^{b}f\left(x\right)\dd x$ durch einen
einfachen Ausdruck an. Diese Aufgabe heißt \emph{numerische Quadratur}.

Wir wollen also die Abbildung
\begin{align*}
I:f & \mapsto\int_{a}^{b}f\left(x\right)\dd x
\end{align*}
durch eine Abbildung
\begin{align*}
J:f & \mapsto J\left(f\right)
\end{align*}
ersetzen. $J$ sollte
\begin{itemize}
\item elementar zu berechnen sein.
\item möglichst viele Eigenschaften von $I$ haben.
\item die Werte $I\left(f\right)$ möglichst gut annähern.
\end{itemize}

\section{Eigenschaften des Integrals}

$f,g$ seien integrierbare Funktionen, zum Beispiel $f,g:\left[a,b\right]\to\mathbb{R}$
Riemann integrierbar.
\begin{enumerate}[label=\alph*)]
\item $I$ ist linear:
\begin{align*}
I\left(\alpha f+\beta g\right) & =\alpha I\left(f\right)+\beta I\left(g\right)
\end{align*}

\item $I$ ist monoton:
\begin{align*}
f\le g\quad\Rightarrow\quad I\left(f\right)\le I\left(g\right)
\end{align*}

\end{enumerate}
Nach Möglichkeit soll $J$ auch die Eigenschaften a) und b) haben.


\section{Einfache Quadraturformeln\label{sec:Einfache-Quadraturformeln}}

Zerlege das Intervall $\left[a,b\right]$ in $n$ Teilintervalle $\left[x_{i},x_{i+1}\right]$
mit $i\in\left\{ 0,\ldots,n-1\right\} $ und Länge $h_{i}:=x_{i+1}-x_{i}$:
\begin{align*}
a=x_{0}<x_{1}<\ldots<x_{n}=b
\end{align*}
Ersetze dann auf jeden Teilintervall die Funktion $f$ durch eine
einfach zu integrierende Funktion.
\begin{enumerate}[label=\alph*)]
\item \emph{Rechteckregel:} Wähle eine konstante Funktion auf jedem Teilintervall.\\
\textcolor{green}{TODO: Abb14 einfügen}\\
Übliche Wahlen für die Konstante sind der linke/rechte/mittlere Intervallpunkt.\\
Wählen wir den mittleren Punkt, so ergibt sich:
\begin{align*}
R_{M}\left(f\right) & :=\sum_{j=0}^{n-1}f\left(\frac{x_{j+1}+x_{j}}{2}\right)\left(x_{j+1}-x_{j}\right)
\end{align*}
Sind die Intervalle gleich groß, so definiere:
\begin{align*}
h & :=\frac{b-a}{n}=x_{j+1}-x_{j}
\end{align*}
Dann gilt:
\begin{align*}
R_{M}\left(f\right) & =h\sum_{j=0}^{n-1}f\left(x_{j}+\frac{h}{2}\right)
\end{align*}

\item \emph{Trapezregel:} Wähle auf jedem Teilintervall eine (affin-)lineare
Funktion. Eine (übliche) Möglichkeit ist die Wahl einer affin-linearen
Funktion, die die Werte an den Randpunkten verbindet.\\
\textcolor{green}{TODO: Abb15 einfügen}\\
Auf jedem Teilintervall ergibt sich als Integral:
\begin{align*}
\frac{f\left(x_{i}\right)+f\left(x_{i+1}\right)}{2}\left(x_{i+1}-x_{i}\right)
\end{align*}
Damit folgt:
\begin{align*}
T\left(f\right) & =\sum_{i=0}^{n-1}\frac{f\left(x_{i}\right)+f\left(x_{i+1}\right)}{2}\left(x_{i+1}-x_{i}\right)
\end{align*}
Ist die Breite
\begin{align*}
h & =x_{i+1}-x_{i}
\end{align*}
der Intervalle konstant, so folgt:
\begin{align*}
T\left(f\right) & =h\left(\frac{1}{2}f\left(x_{0}\right)+\sum_{i=1}^{n-1}f\left(x_{i}\right)+\frac{1}{2}f\left(x_{n}\right)\right)
\end{align*}

\end{enumerate}

\section{Interpolatorische Integrationsformeln}

Nähere $f$ jetzt durch Polynome höherer Ordnung an.

Seien $x_{0},\ldots,x_{n}$ Stützstellen mit $a\le x_{0}<x_{1}<\ldots<x_{n}\le b$
und sei $f:=\left[a,b\right]\to\mathbb{R}$.

Sei nun $p\in\mathbb{P}_{n}$ die Funktion, die $f$ an den Stellen
$x_{0},\ldots,x_{n}$ interpoliert, das heißt:
\begin{align*}
p\left(x_{i}\right) & =f\left(x_{i}\right)
\end{align*}
In Lagrange-Darstellung heißt das:
\begin{align*}
p\left(x\right) & =\sum_{j=0}^{n}f\left(x_{j}\right)w_{j}\left(x\right)
\end{align*}
Dabei ist:
\begin{align*}
w_{j}\left(x\right) & =\prod_{\sr{}{i=0}{i\not=j}}\frac{x-x_{i}}{x_{j}-x_{i}}
\end{align*}
Statt
\begin{align*}
\int_{a}^{b}f\left(x\right)\dd x
\end{align*}
berechne:
\begin{align*}
J\left(f\right) & =\int_{a}^{b}p\left(x\right)\dd x=\int_{a}^{b}\sum_{j=0}^{n}f\left(x_{j}\right)w_{j}\left(x\right)\dd x=\sum_{j=0}^{n}f\left(x_{j}\right)\underbrace{\int_{a}^{b}w_{j}\left(x\right)\dd x}_{=:\alpha_{j}}
\end{align*}
Die $\alpha_{0},\ldots,\alpha_{n}$ sind unabhängig von $f$ und müssen
nur einmal berechnet werden.


\section{Definition \textmd{(Integrationsformel, Quadraturformel)}}
\begin{enumerate}[label=\roman*)]
\item Eine Abbildung $J:C^{0}\left(\left[a,b\right];\mathbb{R}\right)\to\mathbb{R}$
heißt \emph{Integrationsformel} (\emph{Quadraturformel}), falls
\begin{align*}
J\left(f\right) & =\sum_{j=0}^{n}\alpha_{j}f\left(x_{j}\right)
\end{align*}
mit $\alpha_{j}\in\mathbb{R}$ und $a\le x_{0}<x_{1}<\ldots<x_{n}\le b$
gilt.
\item $J$ heißt \emph{exakt} für eine Menge von Funktionen $K$, falls
für alle $f\in K$ gilt:
\begin{align*}
J\left(f\right) & =\int_{a}^{b}f\left(x\right)\dd x
\end{align*}

\item $J$ heißt genau dann \emph{abgeschlossen} (\emph{offen}), wenn $a=x_{0}$
und $x_{n}=b$ ($a<x_{0}$ und $x_{n}<b$) gilt.
\end{enumerate}
%DATE: Mo 17.12.12


\section{Satz \textmd{(Charakterisierung interpolatorischer Quadraturformeln)}}

Sei $J\left(f\right)=\sum_{j=0}^{n}a_{j}f\left(x_{j}\right)$ eine
Quadraturformel. Dann sind äquivalent:
\begin{enumerate}[label=\roman*)]
\item $J$ ist exakt für Polynome vom Grad $n$.
\item Es gilt $a_{i}=\int_{a}^{b}\omega_{i}\left(x\right)\dd x$ mit:
\begin{align*}
\omega_{i}\left(x\right) & =\prod_{\sr{}{j=0}{j\not=i}}^{n}\frac{\left(x-x_{j}\right)}{\left(x_{i}-x_{j}\right)}
\end{align*}

\item Es gilt $J=I\circ P$ mit
\begin{align*}
I\left(f\right) & =\int_{a}^{b}f\left(x\right)\dd x\\
P:C^{0}\left(\left[a,b\right]\right) & \to P_{n}
\end{align*}
und $P\left(f\right)$ ist das Interpolationspolynom zu $x_{1},\ldots,x_{n}$.
\end{enumerate}

\subsubsection*{Beweis}

\textcolor{green}{TODO: Rest (Beweis) einfügen}


\section{Abgeschlossene Newton-Cotes Formeln}

Eine abgeschlossene, interpolatorische Quadraturformel, bei denen
die Stützstellen äquidistant gewählt sind, heißen \emph{Newton-Cotes
Formel}.

Wir setzen $h=\frac{b-a}{n}$ und $x_{j}=a+jh$ mit $j\in\left\{ 0,\ldots,n\right\} $.
Dann ist
\begin{align*}
Q_{n}\left(f\right) & =\sum_{j=0}^{n}\alpha_{j}f\left(x_{j}\right)
\end{align*}
mit
\begin{align*}
\alpha_{j} & =\int_{a}^{b}\omega_{j}\left(x\right)\dd x
\end{align*}
die eindeutig bestimmte interpolatorische Quadraturformel, die Polynome
vom Grad $n$ exakt integriert. Diese nennen wir Newton-Cotes Formeln
der Ordnung $n$. Es gilt für $k\in\left\{ 0,\ldots,n\right\} $:
\begin{align*}
Q_{n}\left(x^{k}\right) & =\int_{a}^{b}x^{k}\dd x=\frac{1}{k+1}\left(b^{k+1}-a^{k+1}\right)=\\
 & =\sum_{j=0}^{n}\alpha_{j}^{\left(n\right)}x_{j}^{k}
\end{align*}
Die $\alpha_{j}^{\left(n\right)}$ können also als Lösung des folgenden
Gleichungssystems berechnet werden:
\begin{align*}
\left(\begin{array}{ccc}
1 & \ldots & 1\\
x_{0} & \ldots & x_{n}\\
\vdots &  & \vdots\\
x_{0}^{n} & \ldots & x_{n}^{n}
\end{array}\right)\left(\begin{array}{c}
\alpha_{0}^{\left(n\right)}\\
\vdots\\
\vdots\\
\alpha_{n}^{\left(n\right)}
\end{array}\right) & =\left(\begin{array}{c}
b-a\\
\frac{b^{2}-a^{2}}{2}\\
\vdots\\
\frac{b^{n+1}-a^{n+1}}{n+1}
\end{array}\right)
\end{align*}



\section{Bemerkung}
\begin{enumerate}[label=\roman*)]
\item Quadraturformeln sind linear.
\item Bei abgeschlossenen Newton-Cotes-Formeln sind einige Werte $a_{i}^{\left(n\right)}$
negativ, falls $n$ groß ist, das heißt aus $f\le g$ folgt nicht
immer $Q_{n}\left(f\right)\le Q_{n}\left(g\right)$.
\end{enumerate}

\section{Satz}

Sei $Q_{n}\left(f\right)=\sum_{i=0}^{n}\alpha_{i}f\left(x_{i}\right)$
eine abgeschlossene Newton-Cotes Formel der Ordnung $n$. Dann gilt:
\begin{enumerate}[label=\roman*)]
\item ${\displaystyle \sum_{i=0}^{n}\alpha_{i}=b-a}$
\item $\alpha_{i}=\alpha_{n-i}$
\item Ist $n$ ungerade, so integriert $Q_{n}$ Polynome bis zum Grad $n+1$
exakt.
\end{enumerate}

\subsubsection*{Beweis}

\textcolor{green}{TODO: Rest (Beweis) einfügen}


\section{Darstellungsformel für den Integrationsfehler}

Sei ${\displaystyle J\left(f\right)=\sum_{i=0}^{n}\alpha_{i}f\left(x_{i}\right)}$
mit $\alpha_{i}\in\mathbb{R}$ und $0\le x_{0}<x_{1}<\ldots<x_{n}\le b$.
Dann ist der Integrationsfehler:
\begin{align*}
R\left(f\right) & :=J\left(f\right)-\int_{a}^{b}f\left(x\right)\dd x
\end{align*}
eine lineare Abbildung. Es gilt folgende Fehlerabschätzung.


\subsubsection*{Satz \textmd{(Peano)}}

Für alle $p\in\mathbb{P}_{n}$ gelte $R\left(p\right)=0$, das heißt
alle Polynome $p\in\mathbb{P}_{n}$ werden exakt integriert. Dann
gilt für $f\in C^{n+1}\left(\left[a,b\right]\right)$:
\begin{align*}
R\left(f\right) & =\int_{a}^{b}f^{\left(n+1\right)}\left(t\right)K\left(t\right)\dd t\\
K\left(t\right) & :=\frac{1}{n!}R\left[\left(.-t\right)^{n}\Theta\left(.-t\right)\right]=\\
 & =\frac{1}{n!}\left(\sum_{i=1}^{n}\alpha_{i}\left(x_{i}-t\right)^{n}\Theta\left(x_{i}-t\right)-\int_{a}^{b}\left(x-t\right)^{n}\Theta\left(x-t\right)\dd x\right)
\end{align*}
Hierbei ist $\Theta$ die Heaviside-Funktion:
\begin{align*}
\Theta\left(x\right) & =\begin{cases}
1 & x\ge0\\
0 & x<0
\end{cases}
\end{align*}
Verwende im Folgenden die Kurzschreibweise:
\begin{align*}
\left(.-t\right)_{+}^{n} & :=\left(.-t\right)^{n}\Theta\left(.-t\right)
\end{align*}



\subsubsection*{Beweis}

\textcolor{green}{TODO: Rest (Beweis) einfügen}


\section{Fehlerdarstellung bei den Newton-Cotes Formeln}

Sei $Q_{n}\left(f\right)=\sum_{i=0}^{n}\alpha_{i}f\left(x_{i}\right)$
die Newton-Cotes Formel der Ordnung $n$.
\begin{align*}
R_{n}\left(f\right) & :=Q_{n}\left(f\right)-\int_{a}^{b}f\left(x\right)\dd x
\end{align*}
Ist $n$ ungerade (gerade), so werden Polynome bis Grad $n$ ($n+1$)
exakt integriert.

%DATE: Mi 19.12.12

Für $Q_{n}\left(f\right)$ sei der Kern $K_{n}$:
\begin{align}
R_{n}\left(f\right) & =\int_{a}^{b}f^{\left(n+1+l\right)}\left(t\right)K_{n}\left(t\right)\dd t\label{eq:stern}
\end{align}
Dabei ist:
\begin{align*}
l & :=\begin{cases}
0 & \text{falls }n\text{ ungerade}\\
1 & \text{falls }n\text{ gerade}
\end{cases}
\end{align*}
Dann gilt:
\begin{align*}
K_{n}\left(t\right) & =\frac{1}{\left(n+l\right)!}R_{n}\left(\left(.-t\right)_{+}^{n+l}\right)
\end{align*}
Bei den Newton-Cotes-Formeln hat $K_{n}$ ein konstantes Vorzeichen.
Der verallgemeinerte Mittelwertsatz der Integralrechnung liefert (vergleiche
\eqref{eq:stern}):
\begin{align*}
R_{n}\left(f\right) & =f^{\left(n+1+l\right)}\left(\xi\right)\cdot\int_{a}^{b}K_{n}\left(t\right)\dd t
\end{align*}
Hierbei ist $\xi\in\left(a,b\right)$. Für die Wahl
\begin{align*}
f & =x^{n+1+l}
\end{align*}
ergibt sich:
\begin{align*}
R_{n}\left(x^{n+1+l}\right) & =\left(n+1+l\right)!\int_{a}^{b}K_{n}\left(t\right)\dd t
\end{align*}



\subsubsection*{Satz}

Im Fall der Newton-Cotes-Formeln haben die Peano-Kerne $K_{n}$ ein
konstantes Vorzeichen. Somit existiert für $f\in C^{n+l+1}\left(\left[a,b\right]\right)$
mit $l$ wie oben ein $\xi\in\left(a,b\right)$, sodass gilt:
\begin{align*}
R_{n}\left(f\right) & =Q_{n}\left(f\right)-\int_{a}^{b}f\left(x\right)\dd x=\frac{R_{n}\left(x^{n+l+1}\right)}{\left(n+l+1\right)!}f^{\left(n+l+1\right)}\left(\xi\right)
\end{align*}



\subsubsection*{Beweis}

Im Buch \emph{Interpolation} von \noun{Johan Steffensen} (1965) wird
$K_{n}\ge0$ gezeigt. Der Rest folgt unter Ausnutzung der Diskussion
oben. Siehe dazu \noun{Stoer} oder \noun{Hämmerlin}, \noun{Hoffmann}.


\section{Beispiel: Die Newton-Cotes Formel für \texorpdfstring{$n=1$}{n=1}}

Für $n=1$ gilt:
\begin{align*}
Q_{n}\left(f\right) & =Q_{1}\left(f\right)=\frac{f\left(a\right)+f\left(b\right)}{2}\left(b-a\right)
\end{align*}
Jetzt sei $t\in\left[a,b\right]$.
\begin{align*}
K\left(t\right) & =\frac{1}{n!}R\left[\left(.-t\right)_{+}^{n}\right]=R_{x}\left(\left(x-t\right)_{+}\right)=Q_{1,x}\left(\left(x-t\right)_{+}\right)-\int_{a}^{b}\left(x-t\right)_{+}\dd x=\\
 & =\left(b-a\right)\cdot\frac{\overbrace{\left(a-t\right)_{+}}^{=0}+\left(b-t\right)_{+}}{2}-\int_{t}^{b}\left(x-t\right)\dd x=\\
 & =\frac{\left(b-a\right)\left(b-t\right)}{2}-\left(\frac{b^{2}}{2}-\frac{t^{2}}{2}-\left(b-t\right)t\right)=\frac{1}{2}\left(b-t\right)\left(t-a\right)\ge0
\end{align*}
Es folgt, dass der Peano-Kern ein konstantes Vorzeichen hat, und somit
ergibt sich für ein $\xi\in\left(a,b\right)$:
\begin{align*}
R_{1}\left(f\right) & =Q_{1}\left(f\right)-\int_{a}^{b}f\left(x\right)\dd x=f^{\left(2\right)}\left(\xi\right)\frac{R_{1}\left(x^{2}\right)}{2}\\
R_{1}\left(x^{2}\right) & =\left(b-a\right)\frac{b^{2}+a^{2}}{2}-\int_{a}^{b}x^{2}\dd x=\frac{\left(b-a\right)^{3}}{6}
\end{align*}
Es folgt:
\begin{align*}
R_{1}\left(f\right) & =f^{\left(2\right)}\left(\xi\right)\frac{\left(b-a\right)^{3}}{12}=f^{\left(2\right)}\left(\xi\right)\frac{h^{3}}{12}
\end{align*}
Weitere Fehlerdarstellungen sind:
\begin{align*}
R_{2}\left(f\right) & =\frac{h^{5}}{90}f^{\left(4\right)}\left(\xi\right) &  & \text{Keplersche Fassregel (Simpson Regel)}\\
R_{3}\left(f\right) & =\frac{3h^{5}}{80}f^{\left(4\right)}\left(\xi\right) &  & \text{Newtonsche }\frac{3}{8}\text{-Regel}
\end{align*}



\section{Iterierte Newton-Cotes Formeln}

Idee: Verwende die Newton-Cotes Formel (oder verwandte Formeln) nicht
auf $\left[a,b\right]$, sondern unterteile $\left[a,b\right]$ in
Teilintervalle. (vergleiche \ref{sec:Einfache-Quadraturformeln})

Trapezregel: Sei $a=x_{0}<x_{1}<\ldots<x_{n}=b$ mit $x_{i}=a+ih$
und $h=\frac{b-a}{n}$ eine äquidistante Unterteilung von $\left[a,b\right]$.
Für $i\in\left\{ 1,\ldots,n\right\} $ ist der Wert der Trapezregel
auf $\left[x_{i-1},x_{i}\right]$:
\begin{align*}
T_{i}\left(f\right) & =\frac{h}{2}\left(f\left(x_{i-1}\right)+f\left(x_{i}\right)\right)
\end{align*}
Dann gilt für geeignete $\xi_{i}\in\left(x_{i-1},x_{i}\right)$:
\begin{align*}
R_{i}\left(f\right) & =T_{i}\left(f\right)-\int_{x_{i-1}}^{x_{i}}f\left(x\right)\dd x=\frac{h^{3}}{12}f^{\left(2\right)}\left(\xi_{i}\right)
\end{align*}
Die Iterierte Trapezregel für $f\in C^{2}\left(\left[a,b\right]\right)$
ist:
\begin{align*}
\hat{T}_{n}\left(f\right) & =\sum_{i=1}^{n}T_{i}\left(f\right)
\end{align*}
Der dabei gemachte Fehler besitzt die Darstellung:
\begin{align*}
\hat{T}_{n}\left(f\right)-\underbrace{\int_{a}^{b}f\left(x\right)\dd x}_{=\sum_{i=1}^{n}\int_{x_{i-1}}^{x_{i}}f\left(x\right)\dd x} & =\sum_{i=1}^{n}\frac{h^{3}}{12}f^{\left(2\right)}\left(\xi_{i}\right)=\left(b-a\right)\frac{h^{2}}{12}\frac{1}{n}\sum_{i=1}^{n}f^{\left(2\right)}\left(\xi_{i}\right)=\\
 & =\left(b-a\right)\frac{h^{2}}{12}f^{\left(2\right)}\left(\xi\right)
\end{align*}
Hier ist $\xi\in\left(a,b\right)$ geeignet und:
\begin{align*}
h & =\frac{b-a}{n}
\end{align*}
Die letzte Identität folgt aus dem Zwischenwertsatz:
\begin{align*}
\text{min}\left(f^{\left(2\right)}\right) & \le\frac{1}{n}\sum_{i=1}^{n}f^{\left(2\right)}\left(\xi_{i}\right)\le\max\left(f^{\left(2\right)}\right)
\end{align*}
Die iterierte Trapezregel konvergiert für $h=\frac{b-a}{n}\to0$ quadratisch
gegen Null.


\section{Euler-MacLaurinsche Summenformel}

Für Funktionen $f\in C^{2m+2}\left(\left[a,b\right]\right)$ gilt
die folgende Entwicklung für die iterierte Trapezregel:


\subsubsection{Satz}

Es sei $f\in C^{2m+2}\left(\left[a,b\right]\right)$ und $\hat{T}_{n}\left(f\right)$
die iterierte Trapezregel für äquidistante Knoten $x_{i}=a+ih$ mit
$h=\frac{b-a}{n}$, das heißt:
\begin{align*}
\hat{T}_{n}\left(f\right) & =h\left(\frac{1}{2}f\left(a\right)+\sum_{i=1}^{n-1}f\left(a+ih\right)+\frac{1}{2}f\left(b\right)\right)
\end{align*}
Dann gilt:
\begin{align*}
\hat{T}\left(f\right) & =\underbrace{\int_{a}^{b}f\left(x\right)\dd x}_{=:\tau_{0}}+\tau_{1}h^{2}+\tau_{2}h^{4}+\ldots+\tau_{m}h^{2m}+\alpha_{m+1}\left(h\right)h^{2m+2}
\end{align*}
Dabei ist für $k\in\left\{ 1,\ldots,m\right\} $ nun
\begin{align*}
\tau_{k} & =\frac{B_{2k}}{\left(2k\right)!}\left(f^{\left(2k-1\right)}\left(b\right)-f^{\left(2k-1\right)}\left(a\right)\right)
\end{align*}
mit den Bernulli-Zahlen $B_{2k}$ und
\begin{align}
\alpha_{m+1}\left(h\right) & =\frac{B_{2m+2}}{\left(2m+2\right)!}\left(b-a\right)f^{\left(2m+2\right)}\left(\xi\left(h\right)\right)\label{eq:Fehlerdarstellung}
\end{align}
mit $\xi\left(h\right)\in\left(a,b\right)$.


\subsubsection*{Beweis}

Siehe \noun{Stoer} oder \noun{Hämmerlin}, \noun{Hoffmann}.


\subsubsection*{Beispiel}

\begin{align*}
\hat{T}_{h}\left(f\right) & =\tau_{0}+\tau_{1}h^{2}+\tau_{2}h^{4}+\ldots\\
\hat{T}_{\frac{h}{2}}\left(f\right) & =\tau_{0}+\tau_{1}\left(\frac{h}{2}\right)^{2}+\tau_{2}\left(\frac{h}{2}\right)^{4}+\ldots
\end{align*}
Versuche eine bessere Näherung durch eine Linearkombination zu erhalten:
\begin{align*}
\alpha_{0}\hat{T}_{h}+\alpha_{1}\hat{T}_{\frac{h}{2}} & =\left(\alpha_{0}+\alpha_{1}\right)\tau_{0}+\tau_{1}h^{2}\left(\alpha_{0}+\frac{\alpha_{1}}{4}\right)+\ldots
\end{align*}
Wähle dazu:
\begin{align*}
\alpha_{0}+\alpha_{1} & =1\\
\alpha_{0}+\frac{\alpha_{1}}{4} & =0
\end{align*}
\begin{align*}
\Rightarrow\quad\alpha_{0}\hat{T}_{h}+\alpha_{1}\hat{T}_{\frac{h}{2}} & =\int_{a}^{b}f\left(x\right)\dd x+Ch^{4}+\ldots
\end{align*}



\section{Idee der Extrapolation}

Definiere:
\begin{align*}
q\left(x\right) & =\tau_{0}+\tau_{1}x+\tau_{2}x^{2}+\ldots+\tau_{m}x^{m}
\end{align*}
Es gilt:
\begin{enumerate}[label=\roman*)]
\item ${\displaystyle \hat{T}_{h}\left(f\right)=q\left(h^{2}\right)+\mathcal{O}_{0}\left(h^{2m+2}\right)}$
\item ${\displaystyle q\left(0\right)=\tau_{0}=\int_{a}^{b}f\left(x\right)\dd x}$
\end{enumerate}
\emph{Anmerkung:} Angenommen wir kennen $\hat{T}_{h}$ (als Funktion
von $h$) an den Stellen 
\begin{align*}
h_{0} & =\frac{b-a}{n_{0}}>h_{1}=\frac{b-a}{n_{1}}>\ldots>h_{m}=\frac{b-a}{n_{m}}
\end{align*}
mit $n_{0},\ldots,n_{m}\in\mathbb{N}$. Dann existiert genau ein Polynom
$p\in\mathbb{P}_{m}$, sodass für $i\in\left\{ 0,\ldots,m\right\} $
gilt:
\begin{align*}
p\left(h_{i}^{2}\right) & =\hat{T}_{h_{i}}\left(f\right)
\end{align*}
Erwartung:
\begin{itemize}
\item $p$ und $q$ liegen nahe beieinander, da sie an den Stellen $h_{0},\ldots,h_{m}$
bis auf $\mathcal{O}_{0}\left(h_{j}^{2m+2}\right)$ für $j\in\left\{ 0,\ldots,m\right\} $
übereinstimmen.
\item $p\left(0\right)\approx\int_{a}^{b}f\left(x\right)\dd x$
\end{itemize}
Wir interessieren uns für $p\left(0\right)$. Nutze das Neville-Aitken
Verfahren, um $p\left(0\right)$ zu berechnen.


\subsubsection*{Bemerkung}

$m=1$ entspricht obigem Vorgehen mit $\alpha_{0},\alpha_{1}$.

\textcolor{green}{TODO: Abb17 einfügen}


\section{Romberg-Verfahren}

Zur Erinnerung: $P\left(g|x_{j},\ldots,x_{k}\right)$ ist das Interpolationspolynom
zu den Punkten $\left(x_{i},g_{i}\right)$ mit $g_{i}=g\left(x_{i}\right)$.
Dann gilt:
\begin{align*}
P\left(g|x_{j},\ldots,x_{k}\right)\left(x\right) & =\frac{\left(x-x_{j}\right)P\left(g|x_{j+1},\ldots,x_{k}\right)\left(x\right)+\left(x_{k}-x\right)P\left(g|x_{j},\ldots,x_{k-1}\right)\left(x\right)}{x_{k}-x_{j}}
\end{align*}
Hier gilt:
\begin{align*}
x_{i} & =h_{i}^{2} & g_{i} & =T_{i}:=\hat{T}_{h_{i}}\left(f\right) & x & =0
\end{align*}
Zur Abkürzung:
\begin{align*}
T_{k,i} & :=P\left(g|x_{k-i},\ldots,x_{k}\right)\left(0\right)
\end{align*}
Uns interessiert $T_{m,m}$. Neville-Aitken liefert (oben $j=k-i$):
\begin{align*}
T_{k,i} & =\frac{-x_{k-i}T_{k,i-1}+x_{k}T_{k-i,i-1}}{x_{k}-x_{k-i}}=\\
 & =T_{k,i-1}+\frac{-x_{k}}{x_{k}-x_{k-i}}\left(T_{k,i-1}-T_{k-i,i-1}\right)=\\
 & =T_{k,i-1}+\frac{1}{\frac{x_{k-i}}{x_{k}}-1}\left(T_{k,i-1}-T_{k-i,i-1}\right)
\end{align*}
Es folgt für $i\in\left\{ 1,\ldots,m\right\} $ und $k\in\left\{ 1,\ldots,m\right\} $:
\begin{align*}
T_{k,i} & =T_{k,i-1}+\frac{1}{\left(\frac{h_{k-i}}{h_{k}}\right)^{2}-1}\left(T_{k,i-1}-T_{k-1,i-1}\right)
\end{align*}
Nutze das Dreiecksschema:

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture} [node distance=10mm]
  \node (x0) {$x_0$};
  \node [below of=x0] (x1) {$x_1$};
  \node [below of=x1] (x2) {$x_2$};
  \node [below of=x2] (xdots) {$\vdots$};
  \node [below of=xdots,node distance=12mm] (xn) {$x_n$};
  \draw ($(x0) + (0.5,0.5)$) -- ($(xn) + (0.5,-0.5)$);
  % Polynome
  \node [right of =x0, node distance=15mm] (p0) {$T_{0,0}$};
  \node [right of =x1, node distance=15mm] (p1) {$T_{1,0}$};
  \node [right of =x2, node distance=15mm] (p2) {$T_{2,0}$};
  \node [right of =xdots, node distance=15mm] (pdots) {$\vdots$};
  \node [right of =xn, node distance=15mm] (pn) {$T_{m-1,0}$};
  % zweite Spalte
  \node (p01) at ($(p0)+(3,-0.5)$) {$T_{1,1}$};
  \node (p12) at ($(p1)+(3,-0.5)$) {$T_{2,1}$};
  \node (pn-1n) at ($(pn)+(3,0.5)$) {$T_{m,1}$};
  \draw (p0) -- (p01) -- (p1);
  \draw (p1) -- (p12) -- (p2);
  \draw (pn) -- (pn-1n);
  % dritte Spalte
  \node (p012) at ($(p01)+(3,-0.5)$) {$T_{2,2}$};
  \node (pn-2n-1n) at ($(pn-1n)+(3,0.5)$) {$T_{m,2}$};
  \draw (p01) -- (p012) -- (p12);
  \draw (pn-1n) -- (pn-2n-1n);
  % vierte Spalte
  \node (p0n) at ($(p012)!0.5!(pn-2n-1n) +(3,0)$) {$T_{m,m}$};  
  \draw[dotted] (p012) -- (p0n) -- (pn-2n-1n);
\end{tikzpicture}
\par\end{centering}

\caption{Extrapolationsschema}
\end{figure}



\section{Wahl der Schrittweite}
\begin{enumerate}[label=\alph*)]
\item Romberg Folge: Die einfachste Wahl ist die iterative Intervallhalbierung
in $2^{j}$ gleichgroße Teilintervalle für $j\in\left\{ 0,\ldots,m\right\} $.
Dann hat man die \emph{Romberg-Folge}:
\begin{align*}
h_{j} & =\frac{b-a}{2^{j}}
\end{align*}
\begin{align*}
T_{k,i} & =T_{k,i-1}+\underbrace{\frac{1}{2^{2i}-1}}_{=\frac{1}{4^{i}-1}}\left(T_{k,i-1}-T_{k-1,i-1}\right)
\end{align*}
Für $T_{h_{j}}$ nutze $T_{h_{j-1}}$.\\
\textcolor{green}{TODO: Abb18 einfügen}
\begin{align*}
\hat{T}_{\frac{h}{2}} & =\frac{h}{4}\left(f\left(a\right)+2\sum_{i=1}^{2n-1}f\left(a+i\frac{h}{2}\right)+f\left(b\right)\right)=\\
 & =\frac{h}{4}\left(f\left(a\right)+2\sum_{k=1}^{n-1}f\left(a+kh\right)+f\left(b\right)\right)+\frac{h}{2}\sum_{k=1}^{n}f\left(a+\frac{2k-1}{2}h\right)=\\
 & =\frac{1}{2}\hat{T}_{h}+\frac{h}{2}\sum_{k=1}^{n}f\left(a+\frac{2k-1}{2}h\right)
\end{align*}
$\hat{T}_{h}$ ist schon bekannt!%DATE: Mo 7.1.13
\item Bulirsch-Folge: Die zweite übliche Wahl ist:
\begin{align*}
h_{2j} & =2^{-j}\left(b-a\right)\\
h_{2j+1} & =\frac{1}{3}\cdot2^{-j}\cdot\left(b-a\right)
\end{align*}
Die Folge lautet:
\begin{align*}
\left(b-a\right),\frac{b-a}{3},\frac{b-a}{4},\frac{b-a}{6},\frac{b-a}{8},\frac{b-a}{12},\ldots
\end{align*}
Der Vorteil dabei ist, dass die Schrittweite nicht so schnell klein
wird. Bei der Berechnung von $T\left(h_{j}\right)$ kann $T\left(h_{j-2}\right)$
benutzt werden (vergleiche a)).
\item Für glatte Funktionen $f$ wird in der Praxis das Romberg-Verfahren
benutzt, um Integrale näherungsweise zu berechnen. $T_{m,m}$ ist
in der Regel deutlich näher an dem Integral $\int_{a}^{b}f\left(x\right)\dd x$,
als $T_{m,0}$.
\end{enumerate}

\section{Verfahren und Abbruch}

In der Praxis ist $f$ häufig nicht beliebig oft differenzierbar.
In die Fehlerdarstellung \eqref{eq:Fehlerdarstellung} geht aber die
Differenzierbarkeitsordnung $2m+2$ ein. Falls $m$ unbekannt oder
falls $f$ nicht so oft differenzierbar ist, gehe wie folgt vor: Wähle
$m_{\text{max}}$, das heißt wir bestimmen iterativ $T_{0,0},T_{1,1},\ldots,T_{m_{\text{max}},m_{\text{max}}},\ldots,T_{i,m_{\text{max}}}$.

\noindent \begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
$T_{0,0}$ &  &  & \tabularnewline
\hline 
$T_{1,0}$ & $T_{1,1}$ &  & \tabularnewline
\hline 
$\vdots$ &  & $\ddots$ & \tabularnewline
\hline 
$T_{m_{\text{max}},0}$ & $\ldots$ & $\ldots$ & $T_{m_{\text{max}},m_{\text{max}}}$\tabularnewline
\hline 
$T_{m_{\text{max}}+1,0}$ & $\ldots$ & $\ldots$ & $T_{m_{\text{max}}+1,m_{\text{max}}}$\tabularnewline
\hline 
$\vdots$ &  &  & $\vdots$\tabularnewline
\hline 
$T_{i,0}$ & $\ldots$ & $\ldots$ & $T_{i,m_{\text{max}}}$\tabularnewline
\hline 
\end{tabular}
\par\end{center}

$T_{i,m_{\text{max}}}$ wird deutlich besser sein, als $T_{m_{\text{max}},m_{\text{max}}}$,
da kleinere $h_{j}$-Werte genutzt werden.

Zum Beispiel: Setze $m_{\text{max}}=7$ und breche ab, falls:
\begin{enumerate}[label=\alph*)]
\item $i$ zu groß wird.
\item genügend viele Ziffern „stehen“, das heißt genügend viele führende
Ziffern sich nicht mehr ändern, oder
\begin{align*}
\abs{T_{i,m_{\text{max}}}-T_{i+1,m_{\text{max}}}} & \le\varepsilon\cdot\abs{\tau}
\end{align*}
für sehr kleines $\varepsilon\in\mathbb{R}_{>0}$ gilt. Dabei ist
$\tau$ ein Schätzwert für das Integral, zum Beispiel $\tau=T_{i+1,m_{\text{max}}}$.
\end{enumerate}

\section{Idee der Gauß-Quadratur}

Bisher waren die Stützstellen $x_{j}$ äquidistant.

Jetzt wählen wir in $J\left(f\right)=\sum_{j=0}^{n}\alpha_{j}f\left(x_{j}\right)$
auch die $x_{0},\ldots,x_{j}$ geeignet. Die Gaußquadratur wählt $\alpha_{0},\ldots,\alpha_{n}$
und $x_{0},\ldots,x_{n}$, sodass Polynome möglichst hohen Grades
exakt integriert werden. Wir hoffen, Polynome des Grades $2n+1$ exakt
integrieren zu können.


\subsubsection*{Verallgemeinere die Aufgabenstellung}

Betrachte gewichtete Integrale:
\begin{align*}
I\left(f\right) & :=\int_{a}^{b}f\left(x\right)\omega\left(x\right)\dd x
\end{align*}
Dabei können $a$ und $b$ auch $-\infty$ oder $\infty$ sein. $\omega$
ist eine nicht negative Gewichtsfunktion.


\section{Gewichtsfunktionen\label{sec:Gewichtsfunktionen}}

Wir stellen folgende Voraussetzungen an $\omega$:
\begin{enumerate}[label=\alph*)]
\item $\omega$ ist auf $\left(a,b\right)$ nicht negativ und stückweise
stetig (messbar).
\item Alle Momente
\begin{align*}
\mu_{k} & :=\int_{a}^{b}x^{k}\omega\left(x\right)\dd x
\end{align*}
für $k\in\mathbb{N}_{\ge0}$ sind endlich.
\item Für jedes Polynom $p$ mit $P\left(x\right)\ge0$ für alle $x\in\left[a,b\right]$
und
\begin{align*}
\int_{a}^{b}\omega\left(x\right)P\left(x\right)\dd x & =0
\end{align*}
folgt $P\left(x\right)=0$ für alle $x\in\mathbb{R}$.
\end{enumerate}
Diese Voraussetzungen sind für positives und stetiges $\omega$ auf
einem endlichen Intervall erfüllt.

Falls a) und b) erfüllt sind, lässt sich c) äquivalent durch $\mu_{0}>0$
ausdrücken.

Typische Gewichtsfunktionen sind:

\noindent \begin{center}
\begin{tabular}{|c|c|}
\hline 
$\omega\left(x\right)$ & Intervall\tabularnewline
\hline 
\hline 
$1$ & $\left[-1,1\right]$\tabularnewline
\hline 
$e^{-x}$ & $[0,\infty)$\tabularnewline
\hline 
$\frac{1}{\sqrt{1-x^{2}}}$ & $\left(-1,1\right)$\tabularnewline
\hline 
$e^{-x^{2}}$ & $\left(-\infty,\infty\right)$\tabularnewline
\hline 
\end{tabular}
\par\end{center}

Zu einem gegebenen $\omega$ führen wir das Skalarprodukt
\begin{align*}
\left\langle f,g\right\rangle _{\omega} & :=\int_{a}^{b}\omega\left(x\right)f\left(x\right)g\left(x\right)\dd x
\end{align*}
ein. Dies ist definiert für alle messbaren Funktionen $f,g:\left[a,b\right]\to\mathbb{R}$,
für die die Norm
\begin{align*}
\norm f & :=\sqrt{\int_{a}^{b}\omega\left(x\right)f^{2}\left(x\right)\dd x}
\end{align*}
endlich ist. Funktionen $f,g$ heißen \emph{orthogonal}, falls $\left\langle f,g\right\rangle _{\omega}=0$
gilt.

\emph{Ziel:} Finde zu gegebenen $\omega$ und $n$ die Werte $\left(x_{i},\alpha_{i}\right)_{i\in\left\{ 0,\ldots,n\right\} }$,
sodass Polynome möglichst hohen Grades durch $\sum_{i=0}^{n}\alpha_{i}f\left(x_{i}\right)$
exakt integriert werden.

Das Mittel dazu sind Orthogonalpolynome.


\section{Lemma}

Es sei
\begin{align*}
\hat{I}_{n}\left(f\right) & =\sum_{i=0}^{n}\alpha_{i,n}f\left(x_{i,n}\right)
\end{align*}
für alle $p\in\mathbb{P}_{2n+1}$ exakt, das heißt für alle $p\in\mathbb{P}_{2n+1}$
gilt:
\begin{align*}
I\left(p\right) & =\int_{a}^{b}p\left(x\right)\omega\left(x\right)\dd x=\hat{I}_{n}\left(p\right)
\end{align*}
Dann ist
\begin{align*}
p_{n+1}\left(x\right) & :=\left(x-x_{0,n}\right)\cdot\ldots\cdot\left(x-x_{n,n}\right)\in\mathbb{P}_{n+1}
\end{align*}
orthogonal zu allen $p\in\mathbb{P}_{n}$ bezüglich $\left\langle .,.\right\rangle _{\omega}$.


\subsubsection*{Beweis}

Sei $p\in\mathbb{P}_{n}$, also $p\cdot p_{n+1}\in\mathbb{P}_{2n+1}$,
dann folgt:
\begin{align*}
\left\langle p,p_{n+1}\right\rangle _{\omega} & =I\left(p\cdot p_{n+1}\right)=\hat{I}_{n}\left(p\cdot p_{n+1}\right)=\sum_{i=0}^{n}\alpha_{i,n}p\left(x_{i,n}\right)\underbrace{p_{n+1}\left(x_{i,n}\right)}_{=0}=0
\end{align*}
\qqed

Die gesuchten Knoten $\left(x_{i,n}\right)_{n\in\mathbb{N},i\in\left\{ 1,\ldots,n\right\} }$
müssen also Nullstellen zueinander orthogonaler Polynome sein.

Wir suchen also orthogonale Polynome $p_{0},p_{1},p_{2},\ldots$ mit
$\deg\left(P_{j}\right)=j$.

Fragen:
\begin{enumerate}
\item Existieren solche $P_{j}$?
\item Sind die Nullstellen einfach und reell?
\item Wie sind die $\alpha_{i,n}$ zu wählen?
\item Ist dann bei guter Wahl $\hat{I}_{n}\left(p\right)$ exakt für alle
$\mathbb{P}_{2n+1}$?
\end{enumerate}

\section{Satz \textmd{(Existenz von Orthogonalpolynomen)}}

Es gibt für $j\in\mathbb{N}_{\ge0}$ eindeutig bestimmte, normierte
Polynome $p_{j}\in\mathbb{P}_{j}$ mit der Eigenschaft $\left\langle p_{j},p_{k}\right\rangle _{\omega}=0$
für $j\not=k$.

Diese Polynome genügen der 3-Term Rekursionsformel mit $p_{-1}:=0$
und $\gamma_{0}:=1$:
\begin{enumerate}
\item $p_{0}\left(x\right)=1$
\item $p_{j+1}\left(x\right)=\left(x-\delta_{i}\right)p_{i}\left(x\right)-\gamma_{i}p_{i-1}\left(x\right)$
\begin{align*}
\delta_{i} & :=\frac{\left\langle x\cdot p_{i},p_{i}\right\rangle _{\omega}}{\left\langle p_{i},p_{i}\right\rangle _{\omega}} & \gamma_{i} & :=\frac{\left\langle p_{i},p_{i}\right\rangle _{\omega}}{\left\langle p_{i-1},p_{i-1}\right\rangle _{\omega}}
\end{align*}

\end{enumerate}

\subsubsection*{Beweis}

Bestimme die Polynome rekursiv analog zum Gram-Schmidschen Orthogonalisierungsverfahren.
Offensichtlich ist $p_{0}=1$.

Nehme an, dass $p_{j}\in\mathbb{P}_{j}$ normiert sind mit $j\ge i$
und den gewünschten Eigenschaften existiert.

Zeige, dass ein normiertes $p_{i+1}\in\mathbb{P}_{i+1}$ existiert,
sodass für $j\le i$ nun
\begin{align*}
\left\langle p_{i+1},p_{j}\right\rangle _{\omega} & =0
\end{align*}
und die Rekursionsformel gilt. Da die Polynome $p_{0},\ldots,p_{i}$
normiert sind, gilt
\begin{align*}
p_{i+1}\left(x\right) & =\left(x-\delta_{i+1}\right)p_{i}\left(x\right)+c_{i-1}p_{i-1}\left(x\right)+\ldots+c_{0}p_{0}\left(x\right)
\end{align*}
mit eindeutig bestimmten $\delta_{i+1}$ und $c_{k}$ für $k\le i-1$.
Da für alle $j\not=k$ und $j,k\le i$ nun $\left\langle p_{j},p_{k}\right\rangle _{\omega}=0$
gilt $\left\langle p_{i+1},p_{j}\right\rangle _{\omega}=0$ für $j\le i$
genau dann ,wenn gilt:
\begin{align*}
\left\langle p_{i+1},p_{i}\right\rangle _{\omega} & =\left\langle xp_{i},p_{i}\right\rangle _{\omega}-\delta_{i+1}\left\langle p_{i},p_{i}\right\rangle _{\omega}=0
\end{align*}
Für $j\le i$ gilt:
\begin{align*}
\left\langle p_{i+1},p_{j-1}\right\rangle _{\omega} & =\underbrace{\left\langle x\cdot p_{j-1},p_{i}\right\rangle _{\omega}}_{=0\text{ für }j<i}+c_{j-1}\left\langle p_{j-1},p_{j-1}\right\rangle _{\omega}\stackrel{!}{=}0
\end{align*}
Damit folgt für $j<i$:
\begin{align*}
c_{j-1} & =0
\end{align*}
Außerdem gilt:
\begin{align*}
\left\langle xp_{i-1},p_{i}\right\rangle _{\omega} & =-c_{i-1}\left\langle p_{i-1},p_{i-1}\right\rangle _{\omega}
\end{align*}
Es folgt die Formel für $\gamma_{i}=-c_{i-1}$ und somit:
\begin{align*}
p_{i+1} & =\left(x-\delta_{i+1}\right)p_{i}-\gamma_{i}p_{i-1}
\end{align*}
Dabei ist $\delta_{i+1}$ wie in der Behauptung.\qqed


\subsubsection*{Bemerkung}
\begin{enumerate}[label=\roman*)]
\item Da die Polynome $p_{0},\ldots,p_{n}$ orthogonal und ungleich Null
sind, sind sie auch linear unabhängig und bilden eine Basis von $\mathbb{P}_{n}$.
\end{enumerate}
\textcolor{green}{TODO: Rest einfügen}

%DATE: Mi 9.1.13


\section{Satz}

Die Nullstellen $x_{0},\ldots,x_{n-1}$ von $p_{n}$ sind reell, einfach
und liegen im offenen Intervall $\left(a,b\right)$.


\subsubsection*{Beweis}

\textcolor{green}{TODO: Abb19 einfügen}

Seien $a<x_{0}<x_{1}<\ldots<x_{l}<b$ die Nullstellen von $p_{n}$,
an denen $p_{n}$ das Vorzeichen wechselt, das heißt wir betrachten
reelle Nullstellen mit ungerader Vielfachheit in $\left(a,b\right)$.
Zu zeigen ist $l=n-1$. 

\emph{Annahme:} $l=n-1$

Definiere:
\begin{align*}
q\left(x\right) & :=\prod_{j=0}^{l}\left(x-x_{j}\right)\in\mathbb{P}_{l}
\end{align*}
Wegen $\deg\left(q\right)<n$ folgt $\left\langle p_{n},q\right\rangle _{\omega}=0$.
Andererseits ändert $p_{n}\cdot q$ das Vorzeichen nicht. Es folgt:
\begin{align*}
\left\langle p_{n},q\right\rangle _{\omega} & =\int_{a}^{b}\underbrace{\omega\left(x\right)}_{\ge0}\underbrace{p_{n}\left(x\right)q\left(x\right)}_{\text{sgn}\left(\ldots\right)\,=\text{ konst.}}\dd x\not=0
\end{align*}
Dies ist ein Widerspruch zu \ref{sec:Gewichtsfunktionen} c).\qqed


\section{Beispiele für orthogonale Polynomsysteme}
\begin{enumerate}[label=\alph*)]
\item Für $\omega\left(x\right)=1$ und $\left[a,b\right]=\left[-1,1\right]$
sind die Legendre-Polynome orthogonal. Sie sind durch folgende Formel
definiert:
\begin{align*}
p_{k}\left(x\right) & =\frac{k!}{\left(2k\right)!}\frac{\dd^{k}}{\dd x^{k}}\left(x^{2}-1\right)^{k}
\end{align*}
Es gilt die Rekursionsformel:
\begin{align*}
p_{k+1}\left(x\right) & =xp_{k}\left(x\right)-\frac{k^{2}}{4k^{2}-1}p_{k-1}\left(x\right)
\end{align*}
\begin{align*}
p_{0}\left(x\right) & =1 & p_{1}\left(x\right) & =x
\end{align*}

\item Für $\omega\left(x\right)=\frac{1}{\sqrt{1-x^{2}}}$ auf $\left[-1,1\right]$
erhält man die Tschebyscheff-Polynome:
\begin{align*}
T_{n}\left(x\right) & =\cos\left(n\arccos\left(x\right)\right)
\end{align*}
Rekursion:
\begin{align*}
T_{n+1}\left(x\right) & =2xT_{n}\left(x\right)-T_{n-1}\left(x\right)
\end{align*}

\item Für $\omega\left(x\right)=e^{-x}$ ist und $[a,b)=[0,\infty)$ erhält
man die Laguerre-Polynome:
\begin{align*}
\Lambda_{n}\left(x\right) & =\left(-1\right)^{n}e^{x}\frac{\dd^{n}}{\dd x^{n}}\left(x^{n}e^{-x}\right)
\end{align*}

\item Für $\omega\left(x\right)=e^{-x^{2}}$ und $\left(a,b\right)=\left(-\infty,\infty\right)$
erhalten wir die Hermite-Polynome:
\begin{align*}
H_{n}\left(x\right) & =\frac{\left(-1\right)^{n}}{2^{n}}e^{x^{2}}\frac{\dd^{n}}{\dd x^{n}}\left(e^{-x^{2}}\right)
\end{align*}

\end{enumerate}
Betrachten wir im Fall a) die Nullstellen des Legendre-Polynoms für
die Integrationsformel, so spricht man von Gauß-Legendre Quadratur.
(Analog: b) Gauß-Tschebyscheff, c) Gauß-Laguerre, d) Gauß-Tschebyscheff)


\subsubsection*{Bemerkung}
\begin{enumerate}[label=\roman*)]
\item Für $\omega\left(x\right)=1$ und ein endliches Intervall $\left(a,b\right)$
ist das Romberg-Verfahren für glatte Funktionen ohne Singularität
in der Regel besser als das Gauß-Legendre-Verfahren.
\item Das Gauß-Tschebyscheff-Verfahren bietet sich an, wenn der Integrand
bei $x=\pm1$ (schwach) singulär ist. Zerlege dazu die Funktion $g$
in $g=f\omega$.
\item Das Gauß-Laguerre-Verfahren und das Gauß-Tschebyscheff-Verfahren sind
wichtig für Integrale auf unbeschränkten Intervallen.
\end{enumerate}

\section{Bestimmung der Gewichte $\alpha_{i}$}

Die Gewichte $\left(\alpha_{i}\right)_{i\in\left\{ 0,\ldots,n\right\} }$
für die Gauß-Quadratur
\begin{align*}
\sum_{i=0}^{n}\alpha_{i}f\left(x_{i}\right)
\end{align*}
werden für die Nullstellen $x_{0},\ldots,x_{n}$ des Orthogonalpolynoms
eindeutig bestimmt durch:
\begin{align}
\alpha_{i} & =\int_{a}^{b}\omega\left(x\right)L_{i}\left(x\right)\dd x\label{eq:alphai-Integral}
\end{align}
Dabei sind die $\left(L_{i}\right)_{i\in\left\{ 0,\ldots,n\right\} }$
die zu $x_{0},\ldots,x_{n}$ gehörenden Lagrange-Interpolationspolynome.
(Setze $L_{i}$ in die Gauß-Quadraturformel ein und nutze aus, dass
die $L_{i}\in\mathbb{P}_{n}$ exakt integrierbar sind.) Genaue Werte
für die Gewichte finden sie in der Literatur. (\noun{Stoer} oder \noun{Hämmerlin},
\noun{Hoffmann})

Wir bestimmen die $\alpha_{i}$ nun etwas anders.


\subsubsection*{Bemerkungen}

Für die Quadratur auf dem Intervall $\left[a,b\right]$ mit $a,b\in\mathbb{R}$
transformiere die Stützstellen auf $\left[-1,1\right]$ durch eine
affin-lineare Abbildung nach $\left[a,b\right]$. Entsprechend transformiere
$[a,\infty)$ nach $[0,\infty)$.


\section{Satz}

Es sei $\left(p_{i}\right)_{i\in\mathbb{N}}$ ein normiertes orthogonales
Polynomsystem.
\begin{enumerate}
\item Seien $x_{0},\ldots,x_{n}$ die Nullstellen von $p_{n+1}$ und $\alpha_{0},\ldots,\alpha_{n}$
seien die Lösungen des linearen Gleichungssystems:
\begin{align}
\sum_{i=0}^{n}\alpha_{i}p_{k}\left(x_{i}\right) & =\begin{cases}
\left\langle p_{0},p_{0}\right\rangle _{\omega} & \text{falls }k=0\\
0 & \text{falls }k\in\left\{ 1,\ldots,n\right\} 
\end{cases}\label{eq:GLS-alphai}
\end{align}
Dann gilt für $i\in\left\{ 0,\ldots,n\right\} $
\begin{align*}
\alpha_{i} & >0
\end{align*}
und für alle $p\in\mathbb{P}_{2n+1}$:
\begin{align}
\int_{a}^{b}\omega\left(x\right)p\left(x\right)\dd x & =\sum_{i=0}^{n}\alpha_{i}p\left(x_{i}\right)\label{eq:Formel-Polynomintegration}
\end{align}

\item Gilt umgekehrt \eqref{eq:Formel-Polynomintegration} für Zahlen $x_{i},\,\alpha_{i}$
für $i\in\left\{ 0,\ldots,n\right\} $, so sind die $x_{i}$ Nullstellen
von $p_{n+1}$ und die $\alpha_{i}$ erfüllen \eqref{eq:GLS-alphai}.
\item Es gibt keine reellen Zahlen $x_{i},\,\alpha_{i}$, sodass \eqref{eq:Formel-Polynomintegration}
für alle $p\in\mathbb{P}_{2n+2}$ gilt.
\end{enumerate}

\subsubsection*{Beweis}
\begin{enumerate}
\item Es gilt:
\begin{align*}
I\left(p_{k}\right) & =\left\langle p_{k},p_{0}\right\rangle _{\omega}=\begin{cases}
\left\langle p_{0},p_{0}\right\rangle _{\omega} & \text{für }k=0\\
0 & \text{für }k\not=0
\end{cases}
\end{align*}
\begin{align*}
\hat{I}_{n}\left(p_{k}\right) & =\sum_{i=0}^{n}\alpha_{i}p_{k}\left(x_{i}\right)
\end{align*}
Da $\left(p_{0},\ldots,p_{n}\right)$ eine Basis von $\mathbb{P}_{n}$
ist, folgt:\\
\eqref{eq:GLS-alphai} ist äquivalent dazu, dass $\hat{I}_{n}$ für
alle $p\in\mathbb{P}_{n}$ exakt ist.\\
Damit ist \eqref{eq:GLS-alphai} äquivalent zu \eqref{eq:alphai-Integral}
und eindeutig lösbar. Sei nun $p\in\mathbb{P}_{2n+1}$, so folgt durch
Polynomdivision die Restklassendarstellung mit geeigneten $q,r\in\mathbb{P}_{n}$:
\begin{align*}
p & =p_{n+1}q+r
\end{align*}
Daher existieren $\alpha_{k},\beta_{k}\in\mathbb{R}$ mit:
\begin{align*}
q\left(x\right) & =\sum_{k=0}^{n}\alpha_{k}p_{k}\left(x\right)\\
r\left(x\right) & =\sum_{k=0}^{n}\beta_{k}p_{k}\left(x\right)
\end{align*}
Wegen $p_{0}=1$ und der Orthogonalität folgt:
\begin{align*}
\int_{a}^{b}\omega\left(x\right)p\left(x\right)\dd x & =\underbrace{\left\langle p_{n+1},q\right\rangle _{\omega}}_{=0}+\left\langle r,p_{0}\right\rangle _{\omega}=\beta_{0}\left\langle p_{0},p_{0}\right\rangle _{\omega}
\end{align*}
Andererseits gilt wegen $p_{n+1}\left(x_{i}\right)=0$ für $i\in\left\{ 0,\ldots,n\right\} $
nun:
\begin{align*}
\sum_{i=0}^{n}\alpha_{i}p\left(x_{i}\right) & =\sum_{i=0}^{n}\alpha_{i}r\left(x_{i}\right)=\sum_{k=0}^{n}\beta_{k}\sum_{i=0}^{n}\alpha_{i}p_{k}\left(x_{i}\right)\stackrel{\eqref{eq:GLS-alphai}}{=}\beta_{0}\left\langle p_{0},p_{0}\right\rangle _{\omega}
\end{align*}
Dies zeigt \eqref{eq:Formel-Polynomintegration}.\\
Sei nun für $l\in\left\{ 0,\ldots,n\right\} $:
\begin{align*}
0 & \le\overline{p}_{l}\left(x\right):=\prod_{\sr{}{j\not=l}{j=0}}^{n}\left(x-x_{j}\right)^{2}\in\mathbb{P}_{2n}\left(x\right)
\end{align*}
Es folgt:
\begin{align*}
0 & \sr <{\text{vergleiche Voraussetzung c)}}{\text{ an die Gewichte}}\int_{a}^{b}\omega\left(x\right)\overline{p}_{l}\left(x\right)\dd x=\sum_{i=0}^{n}\alpha_{i}\overline{p}_{l}\left(x_{i}\right)=\alpha_{l}\underbrace{\overline{p}_{l}\left(x_{l}\right)}_{>0}
\end{align*}
\qqed[\arabic{enumi}.]
\item [3.]Nehme an, es existieren $x_{i},\,\alpha_{i}$ so, dass die Quadratur
für alle $p\in\mathbb{P}_{2n+2}$ exakt ist. Wähle:
\begin{align*}
0\le\overline{p}\left(x\right) & :=\prod_{j=0}^{n}\left(x-x_{j}\right)^{2}\in\mathbb{P}_{2n+2}\left(x\right)
\end{align*}
Es folgt:
\begin{align*}
0 & <I\left(\overline{p}\right)=\hat{I}_{n}\left(\overline{p}\right)=\sum_{i=0}^{n}\alpha\overline{p}\left(x_{i}\right)=0
\end{align*}
Dies ist ein Widerspruch.\qqed[\arabic{enumi}.]
\item [2.]Da $p_{k}\in\mathbb{P}_{2n+1}$ für $k\in\left\{ 0,\ldots,n\right\} $
gilt wie in 1.), dass \eqref{eq:GLS-alphai} richtig ist.\\
Außerdem folgt wie in 1. $\alpha_{i}>0$. Zu zeigen ist , dass die
$x_{i}$ Nullstellen von $p_{n+1}$ sind. Es gilt:
\begin{align}
0 & \stackrel{k\in\left\{ 0,\ldots,n\right\} }{=}\left\langle p_{n+1},p_{k}\right\rangle _{\omega}=\int_{a}^{b}\omega\left(x\right)\underbrace{p_{n+1}\left(x\right)p_{k}\left(x\right)}_{\in\mathbb{P}_{2n+1}}\dd x=\sum_{i=0}^{n}\alpha_{i}p_{n+1}\left(x_{i}\right)p_{k}\left(x_{i}\right)\label{eq:sum_alphai}
\end{align}
Setze:
\begin{align*}
A & =\left(p_{k}\left(x_{i}\right)\right)_{k,i\in\left\{ 0,\ldots,n\right\} }\\
c & =\left(\begin{array}{c}
\alpha_{0}p_{n+1}\left(x_{0}\right)\\
\vdots\\
\alpha_{n}p_{n+1}\left(x_{n}\right)
\end{array}\right)
\end{align*}
Also ist \eqref{eq:GLS-alphai} äquivalent zu $Ac=0$. Falls $A$
regulär ist, folgt $c=0$ und wegen $\alpha_{i}>0$ folgt $p_{n+1}\left(x_{i}\right)=0$.\\
Beobachte zunächst $x_{0}<x_{1}<\ldots<x_{n}$ (siehe oben). Dass
$A$ regulär ist, folgt aus dem folgenden Satz.\qqed[\arabic{enumi}.]
\end{enumerate}
\qqed


\section{Satz}

Seien $\left\{ p_{0},\ldots,p_{n}\right\} $ eine Basis von $\mathbb{P}_{n}$,
so ist für beliebige $x_{0}<\ldots<x_{n}$ die Matrix
\begin{align*}
A & =\left(\begin{array}{ccc}
p_{0}\left(x_{0}\right) & \ldots & p_{0}\left(x_{n}\right)\\
\vdots &  & \vdots\\
p_{n}\left(x_{0}\right) & \ldots & p_{n}\left(x_{n}\right)
\end{array}\right)
\end{align*}
nicht singulär ist.


\subsubsection*{Beweis}

\textcolor{green}{TODO: Rest (Beweis) einfügen (evtl. siehe Zentralübung)}

%DATE: Mo 14.01.13


\chapter{Iterationsverfahren zur Lösung linearer Gleichungssysteme}


\section{Einführung}

Bisher lösten wir $Ax=b$ für $A\in\mathbb{R}^{n\times n}$ und $b\in\mathbb{R}^{n}$
mittels einer direkten Methode (zum Beispiel der $LR$-Zerlegung,
oder der $QR$-Zerlegung). Die Lösung $x$ wird nach einer bestimmten
Anzahl von Schritten durch einen Algorithmus explizit berechnet.

In vielen Anwendungen ist $A$ eine sehr große, dafür aber dünn besetzte
Matrix, das heißt viele Einträge sind Null. Direkte Verfahren zerstören
häufig diese Eigenschaft. Dies führt zu einem unnötig hohen Aufwand.

Auswege:
\begin{enumerate}[label=\alph*)]
\item Passe direkte Verfahren an, zum Beispiel pivotisiere so geschickt,
dass wenig neue Nichtnullelemente auftreten, oder nutze Givens-Rotationen
bei $QR$-Zerlegung.
\item Nutze iterative Verfahren und berechne eine Näherungslösung (vergleiche
mit der Lösung nichtlinearer Gleichungen).
\end{enumerate}
Betrachte zunächst ein Beispiel einer Matrix mit vielen Null-Einträgen:


\section{Diskretisierung der Poisson-Gleichung}

Für eine offene Teilmenge $\Omega\subseteq\mathbb{R}^{d}$ und eine
gegebene Funktion $v:\Omega\to\mathbb{R}$ definiere den Laplace-Operator:
\begin{align*}
\Delta v & =\sum_{i=1}^{d}\frac{\partial^{2}}{\left(\partial x_{i}\right)^{2}}v
\end{align*}
Gesucht ist eine Funktion $u:\overline{\Omega}\to\mathbb{R}$, für
die die \emph{Poisson-Gleichung}
\begin{align*}
-\Delta u & =f & \text{auf } & \Omega\\
u & =g & \text{auf } & \partial\Omega
\end{align*}
erfüllt ist. Dabei sind $f:\Omega\to\mathbb{R}$ und $g:\partial\Omega\to\mathbb{R}$
gegebene Funktionen.

\emph{Ziel:} Bestimme $u$ näherungsweise.

Zur Vereinfachung sei nun $\Omega=\left(0,1\right)^{2}$ das Einheitsquadrat.
Führe ein regelmäßiges quadratisches Gitter mit einer Schrittweite
$h=\frac{1}{n}$ für $n\in\mathbb{N}_{\ge1}$ ein.
\begin{align*}
\Omega_{h} & :=\left\{ \left(ih,jh\right)\in\left(0,1\right)^{2}\big|1\le i,j\le n-1\right\} \\
\overline{\Omega_{h}} & :=\left\{ \left(ih,jh\right)\in\left(0,1\right)^{2}\big|0\le i,j\le n\right\} 
\end{align*}
\textcolor{green}{TODO: Abb20 einfügen}

Wie komme ich an Gleichungen?\\
Eine Taylor-Entwicklung in $x$ für eine glatte Funktion $u:\Omega\to\mathbb{R}$
liefert:
\begin{align*}
\frac{\partial^{2}}{\partial x^{2}}u\left(x,y\right) & =\frac{u\left(x-h,y\right)-2u\left(x,y\right)+u\left(x+h,y\right)}{h^{2}}+\mathcal{O}\left(h^{2}\right)
\end{align*}
Daraus ergibt sich für $\left(x,y\right)\in\Omega_{h}$:
\begin{align*}
-\Delta u\left(x,y\right) & =-\left(\frac{\partial^{2}}{\partial x^{2}}u\left(x,y\right)+\frac{\partial^{2}}{\partial y^{2}}u\left(x,y\right)\right)=\\
 & =\underbrace{\frac{u\left(x-h,y\right)+u\left(x+h,y\right)+u\left(x,y-h\right)+u\left(x,y+h\right)-4u\left(x,y\right)}{h^{2}}}_{=:\Delta_{h}u\left(x,y\right)}+\mathcal{O}\left(h^{2}\right)
\end{align*}
\textcolor{green}{TODO: Abb21 einfügen}

Es seien nun $\ell^{2}\left(\Omega_{h}\right)$ und $\ell^{2}\left(\overline{\Omega}_{h}\right)$
die Menge der Gitterfunktionen $u_{h}:\Omega_{h}\to\mathbb{R}$ beziehungsweise
$u_{h}:\overline{\Omega}_{h}\to\mathbb{R}$.


\subsubsection*{Diskretes Poisson-Problem(DP)}

Gesucht ist eine Funktion $u_{h}\in\ell^{2}\left(\overline{\Omega}_{h}\right)$
mit:
\begin{align*}
-\Delta_{h}u_{h}\left(\xi\right) & =f\left(\xi\right) & \text{für } & \xi\in\Omega_{h}\\
-\Delta_{h}u_{h}\left(\xi\right) & =g\left(\xi\right) & \text{für } & \xi\in\overline{\Omega}_{h}\setminus\Omega_{h}
\end{align*}
Dies ist ein lineares Gleichungssystem für die Werte $\left(u_{h}\left(\xi\right)\right)_{\xi\in\overline{\Omega}_{h}}$
der Gitterfunktion $u_{h}$. Die Werte $u_{h}\left(\xi\right)$ mit
$\xi\in\overline{\Omega}_{h}\setminus\Omega_{h}$ sind direkt bekannt.
Bringe jetzt die Gleichungen für $u_{h}\left(\xi\right)$ mit $\xi\in\Omega_{h}$
auf die Form eines linearen Gleichungssystems:
\begin{align*}
Ax & =b
\end{align*}
\textcolor{green}{TODO: Abb22 einfügen}

Dafür brauchen wir eine Nummerierung der Gitterpunkte.


\subsubsection*{Beispiel}

Betrachte $n=5$, also $h=\frac{1}{5}$, und wähle folgende Nummerierung:

\textcolor{green}{TODO: Abb23 einfügen}

\begin{align*}
\begin{array}{ccc}
\left(h,4h\right) & \ldots & \left(4h,4h\right)\\
\vdots &  & \vdots\\
\left(h,h\right) & \ldots & \left(4h,h\right)
\end{array}
\end{align*}
Mit dieser Anordnung lässt sich das diskrete Poisson-Problem wie folgt
schreiben:
\begin{align*}
A_{1}x & =b
\end{align*}
Dabei sind $A_{1}\in\mathbb{R}^{m\times m}$ und $b\in\mathbb{R}^{m}$
mit:
\begin{align*}
A_{1} & =\frac{1}{h^{2}}\left(\begin{array}{ccccc}
T & -\mathbbm{1} & 0 & \ldots & 0\\
-\mathbbm{1} & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & -\mathbbm{1}\\
0 & \ldots & 0 & -\mathbbm{1} & T
\end{array}\right) & T & =\left(\begin{array}{ccccc}
4 & -1 & 0 & \ldots & 0\\
-1 & \ddots & \ddots & \ddots & \vdots\\
0 & \ddots & \ddots & \ddots & 0\\
\vdots & \ddots & \ddots & \ddots & -1\\
0 & \ldots & 0 & -1 & 4
\end{array}\right) & b & =\left(\begin{array}{c}
b_{1}\\
\vdots\\
b_{n-1}
\end{array}\right)
\end{align*}
\begin{align*}
b_{1} & =\left(\begin{array}{c}
f\left(h,h\right)+\frac{1}{h^{2}}\left(g\left(h,0\right)+g\left(0,h\right)\right)\\
f\left(2h,h\right)+\frac{1}{h^{2}}g\left(2h,0\right)\\
\vdots\\
f\left(1-2h,h\right)+\frac{1}{h^{2}}g\left(1-2h,0\right)\\
f\left(1-h,h\right)+\frac{1}{h^{2}}\left(g\left(1-h,0\right)+g\left(1,h\right)\right)
\end{array}\right) & b_{j} & =\left(\begin{array}{c}
f\left(h,jh\right)+\frac{1}{h^{2}}g\left(0,jh\right)\\
f\left(2h,jh\right)\\
\vdots\\
f\left(1-2h,jh\right)\\
f\left(1-h,h\right)+\frac{1}{h^{2}}g\left(1,jh\right)
\end{array}\right)
\end{align*}
\begin{align*}
b_{n-1} & =\left(\begin{array}{c}
f\left(h,1-h\right)+\frac{1}{h^{2}}\left(g\left(h,1\right)+g\left(0,1-h\right)\right)\\
f\left(2h,1-h\right)+\frac{1}{h^{2}}g\left(2h,1\right)\\
\vdots\\
f\left(1-2h,1-h\right)+\frac{1}{h^{2}}g\left(1-2h,1\right)\\
f\left(1-h,1-h\right)+\frac{1}{h^{2}}\left(g\left(1-h,1\right)+g\left(1,1-h\right)\right)
\end{array}\right)
\end{align*}
Beispiel:
\begin{enumerate}[label=\roman*)]
\item Sei $f=1$, $g=0$ und $h=2^{-6}$, also $n=64$. Die Matrix $A_{1}$
hat die Dimension $3969\times3969$. Wichtig ist, dass die Dimension
der Matrix für $d=3$ deutlich größer wird.\\
\textcolor{green}{TODO: Lösung plotten; Label: Lösung der diskreten
Poisson-Gleichung}\\
Die obige Darstellung von $A_{1}$ zeigt, dass viele Einträge Null
sind, denn nur etwa $5\times\left(n-1\right)^{2}=19845$ der $\left(n-1\right)^{4}=15\,752\,961$
Einträge von $A_{1}$ sind von Null verschieden.
\item Jetzt betrachte $h=\frac{1}{16}$.\\
\textcolor{green}{TODO: Folie; Nichtnullelemente der Matrix $A_{1}$
und $A_{2}$}\\
Cholesky Zerlegung (Symmetrische $LR$-Zerlegung)\\
\textcolor{green}{TODO: Folie; Nichtnullelemente von $L$ bei der
Cholesky-Zerlegung $LL^{\TT}$ der Matrix}\\
Setzen wir $\text{nz}\left(A_{1}\right)$ als die Anzahl der Nichtnullelemente
von $A_{1}$, so folgt:
\begin{align*}
\text{nz}\left(A_{1}\right) & \approx5\cdot h^{-2} & \text{nz}\left(L\right) & \approx h^{-3}
\end{align*}
\textcolor{green}{TODO: Folie; Nichtnulleinträge der Matrizen $A_{1}$
und $L$}\\
\begin{tabular}{|c||c|c|c|c||c|}
\hline 
$h$ & $\frac{1}{16}$ &  &  &  & \tabularnewline
\hline 
\hline 
$\text{nz}\left(A_{1}\right)$ &  &  &  &  & $\approx5h^{-2}$\tabularnewline
\hline 
$\text{nz}\left(L\right)$ &  &  &  &  & $\approx h^{-3}$\tabularnewline
\hline 
\end{tabular}
\end{enumerate}

\section{Allgemeine Iterationsverfahren}

Schreibe $Ax=b$ in eine Fixpunktgleichung um.
\begin{align*}
Ax & =b\\
\Leftrightarrow\qquad B^{-1}\left(b-Ax\right) & =0\\
\Leftrightarrow\qquad\underbrace{\left(\mathbbm{1}-B^{-1}A\right)x+B^{-1}b}_{=:g\left(x\right)} & =x
\end{align*}
Definiere
\begin{align*}
G & =\mathbbm{1}-B^{-1}A & c & :=B^{-1}b
\end{align*}
und die Fixpunktiteration:
\begin{align*}
x^{\left(k+1\right)} & :=g\left(x^{\left(k\right)}\right)=Gx^{\left(k\right)}+c=B^{-1}\left(\left(B-A\right)x^{\left(k\right)}+b\right)
\end{align*}
Der Banachsche Fixpunktsatz sagt, dass $x^{\left(k+1\right)}=g\left(x^{\left(k\right)}\right)$
konvergiert, falls $g$ eine Kontraktion ist. Die Norm auf $\mathbb{R}^{n}$
ist zunächst beliebig, sie ist möglichst geschickt zu wählen.


\section{Konvergenzsatz\label{sec:Konvergenzsatz}}

Sei $A\in\text{GL}\left(n,\mathbb{R}\right)$ invertierbar, $b\in\mathbb{R}^{n}$
und $x=A^{-1}b$. Weiter sei $B\in\text{GL}\left(n,\mathbb{R}\right)$
invertierbar, $x^{\left(0\right)}\in\mathbb{R}^{n}$ der Startvektor
und $x^{\left(k+1\right)}=\left(\mathbbm{1}-B^{-1}A\right)x^{\left(k\right)}+B^{-1}b$.
\begin{enumerate}
\item Die Fixpunktiteration konvergiert für alle $b\in\mathbb{R}^{n}$ und
alle $x_{0}\in\mathbb{R}^{n}$ genau dann, wenn gilt:
\begin{align*}
\varrho\left(\mathbbm{1}-B^{-1}A\right) & <1
\end{align*}
Dabei ist $\varrho$ der Spektralradius, der für $G\in\mathbb{R}^{n\times n}$
wie folgt über die Eigenwerte $\lambda_{j}\left(G\right)$ von $G$
definiert ist:
\begin{align*}
\varrho\left(G\right) & :=\text{max}_{j}\abs{\lambda_{j}\left(G\right)}
\end{align*}

\item Hinreichend für die Konvergenz ist die Bedingung:
\begin{align*}
\opnorm{\mathbbm{1}-B^{-1}A} & <1
\end{align*}
Hier ist $\opnorm .$ eine beliebige Operatornorm.
\end{enumerate}

\subsubsection*{Beweis}
\begin{enumerate}
\item Nach Definition gilt:
\begin{align*}
x^{\left(k+1\right)} & =\left(\mathbbm{1}-B^{-1}A\right)x^{\left(k\right)}+B^{-1}b\\
x & =\left(\mathbbm{1}-B^{-1}A\right)x+B^{-1}b
\end{align*}
Für
\begin{align*}
f^{\left(k+1\right)} & :=x^{\left(k+1\right)}-x
\end{align*}
gilt:
\begin{align*}
f^{\left(k+1\right)} & =\left(\mathbbm{1}-B^{-1}A\right)f^{\left(k\right)}\\
\Rightarrow\qquad f^{\left(k\right)} & =\left(\mathbbm{1}-B^{-1}A\right)^{k}f^{\left(0\right)}
\end{align*}


\begin{enumerate}[label=\alph*)]
\item „$\Rightarrow$“: Es gelte für alle $f^{\left(0\right)}\in\mathbb{C}^{n}$:
\begin{align*}
f^{\left(k\right)} & \to0
\end{align*}
Wähle $f^{\left(0\right)}\not=0$ als Eigenvektor zum Eigenwert $\lambda$,
das heißt:
\begin{align*}
\left(\mathbbm{1}-B^{-1}A\right)f^{\left(0\right)} & =\lambda f^{\left(0\right)}
\end{align*}
Es folgt:
\begin{align*}
f^{\left(k\right)} & =\lambda^{k}f^{\left(0\right)}\\
\abs{\lambda}^{k}\norm{f^{\left(0\right)}} & =\norm{f^{\left(k\right)}}\xrightarrow{k\to\infty}0
\end{align*}
Es folgt $\abs{\lambda}<1$ und somit:
\begin{align*}
\varrho\left(\mathbbm{1}-B^{-1}A\right) & <1
\end{align*}
\qqed[„$\Rightarrow$“]
\item „$\Leftarrow$“: Es gelte $\varrho\left(\mathbbm{1}-B^{-1}A\right)<1$.\\
Es existiert eine Matrix $C$, sodass
\begin{align*}
J & =C^{-1}\left(\mathbbm{1}-B^{-1}A\right)C
\end{align*}
in Jordanscher Normalform ist:
\begin{align*}
J & =\left(\begin{array}{cccc}
\fbox{\ensuremath{J_{1}}} &  &  & 0\\
 & \fbox{\ensuremath{J_{2}}}\\
 &  & \ddots\\
0 &  &  & \fbox{\ensuremath{J_{m}}}
\end{array}\right) & J_{j} & =\left(\begin{array}{cccc}
\lambda_{j} & 1 &  & 0\\
 & \ddots & \ddots\\
 &  & \ddots & 1\\
0 &  &  & \lambda_{j}
\end{array}\right)
\end{align*}
Dabei sind die $\lambda_{i}\not=\lambda_{j}$ für $i\not=j$ die Eigenwerte
von $\mathbbm{1}-B^{-1}A$ mit $\abs{\lambda_{j}}<1$ und $J_{m}$
ist eine $m_{j}\times m_{j}$-Matrix.
\begin{align*}
g^{\left(k\right)} & :=C^{-1}f^{\left(k\right)}
\end{align*}
\begin{align*}
\underbrace{Cg^{\left(k+1\right)}}_{=f^{\left(k+1\right)}} & =\left(\mathbbm{1}-B^{-1}A\right)\underbrace{Cg^{\left(k\right)}}_{=f^{\left(k\right)}}
\end{align*}
\begin{align*}
g^{\left(k+1\right)} & =C^{-1}\left(\mathbbm{1}-B^{-1}A\right)Cg^{\left(k\right)}=Jg^{\left(k\right)}=J^{k+1}g^{\left(0\right)}
\end{align*}
%DATE: Mi 16.01.13
\begin{align*}
J^{k+1} & =\left(\begin{array}{cccc}
\fbox{\ensuremath{J_{1}^{k+1}}} &  &  & 0\\
 & \fbox{\ensuremath{J_{2}^{k+1}}}\\
 &  & \ddots\\
0 &  &  & \fbox{\ensuremath{J_{m}^{k+1}}}
\end{array}\right)
\end{align*}
\begin{align*}
J_{j}^{k} & =\left(\lambda_{j}\mathbbm{1}+J_{j}-\lambda_{j}\mathbbm{1}\right)^{k}=\sum_{i=0}^{k}\left(\begin{array}{c}
k\\
i
\end{array}\right)\lambda_{j}^{k-i}\left(J_{j}-\lambda_{j}\mathbbm{1}\right)^{i}
\end{align*}
\begin{align*}
J_{j}-\lambda_{j}\mathbbm{1} & =\underbrace{\left(\begin{array}{cccc}
0 & 1 &  & 0\\
 & \ddots & \ddots\\
 &  & \ddots & 1\\
0 &  &  & 0
\end{array}\right)}_{\left(m_{j}\times m_{j}\right)\text{-Matrix}}
\end{align*}
Mit
\begin{align*}
\left(J_{j}-\lambda_{j}\mathbbm{1}\right)^{m_{j}-1} & =0
\end{align*}
folgt:
\begin{align*}
J_{j}^{k} & =\sum_{i=0}^{m_{j}-1}\underbrace{\left(\begin{array}{c}
k\\
i
\end{array}\right)\lambda_{j}^{k-i}}_{\abs .\le k^{m_{j}}\abs{\lambda_{j}}^{k-i}\xrightarrow{k\to\infty}0}\left(J_{j}-\lambda_{j}\mathbbm{1}\right)^{i}\xrightarrow{k\to\infty}0
\end{align*}
Es folgt $J^{k}\to0$ und somit:
\begin{align*}
g^{\left(k\right)} & =J^{k}g^{\left(0\right)}\xrightarrow{k\to\infty}0\\
\Rightarrow\qquad f^{\left(k\right)} & =Cg^{\left(k\right)}\xrightarrow{k\to\infty}0\\
\Rightarrow\qquad x^{\left(k\right)} & \xrightarrow{k\to\infty}x
\end{align*}
\qqed[„$\Leftarrow$“]
\end{enumerate}
\item Es sei $\opnorm{\mathbbm{1}-B^{-1}A}<1$. Betrachte die Abbildung:
\begin{align*}
g:x & \mapsto\left(\mathbbm{1}-B^{-1}A\right)x+B^{-1}b
\end{align*}


\begin{description}
\item [{Behauptung:}] $g$ ist kontrahierend auf $\mathbb{R}^{n}$.
\item [{Beweis:}] 
\begin{align*}
\norm{g\left(x\right)-g\left(y\right)} & =\norm{\left(\mathbbm{1}-B^{-1}A\right)\left(x-y\right)}\le\underbrace{\opnorm{\mathbbm{1}-B^{-1}A}}_{<1}\cdot\norm{x-y}
\end{align*}
Also ist $g$ kontrahierend.\qqed[Behaputung]
\end{description}

Daher konvergiert die Folge $\left(x^{\left(k\right)}\right)_{k\in\mathbb{N}}$
gegen einen Fixpunkt von $g$.

\end{enumerate}
\qqed


\subsubsection*{Bemerkung}

Die Konvergenz des Fehlers $x^{\left(k\right)}-x$ gegen Null ist
umso schneller, je kleiner $\varrho\left(\mathbbm{1}-B^{-1}A\right)<1$
ist.

Für diagonalisierbare (zum Beispiel symmetrische) Matrizen $\mathbbm{1}-B^{-1}A$
gibt $\varrho\left(\mathbbm{1}-B^{-1}A\right)$ an, um welchen Faktor
sich der Fehler pro Schritt mindestens reduziert.


\section{Einfache (klassische) Iterationsverfahren}

Iteration zum Lösen von $Ax=b$:
\begin{align*}
x^{\left(k+1\right)} & =\left(\mathbbm{1}-B^{-1}A\right)x^{\left(k\right)}+B^{-1}b
\end{align*}
Wähle $B=A$, so gilt:
\begin{align*}
x^{\left(1\right)} & =B^{-1}b=A^{-1}b=x
\end{align*}
Dies bringt nichts, da die Berechnung der Inversen vermieden werden
soll.

Nächste Strategie: Wähle $B$ so, dass $B^{-1}$ einfach zu berechnen
ist.
\begin{enumerate}[label=\alph*)]
\item Die einfachste Wahl ist $B=\mathbbm{1}$. Dies ergibt das \emph{Richardson-Verfahren}:
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}+\left(b-Ax^{\left(k\right)}\right)
\end{align*}
Sei $G:=\mathbbm{1}-A$. Die Konvergenz für alle Startwerte ist äquivalent
zu:
\begin{align*}
\varrho\left(G\right) & =\max_{i\in\left\{ 1,\ldots,n\right\} }\abs{1-\lambda_{i}}<1
\end{align*}
\textcolor{green}{TODO: Abb24 einfügen}
\item Für bessere Wahl schreiben wir $A=L+D+R$.
\begin{align*}
A & =\left(a_{ij}\right)_{i,j\in\left\{ 1,\ldots,n\right\} } & D & =\left(\begin{array}{ccc}
a_{11} &  & 0\\
 & \ddots\\
0 &  & a_{nn}
\end{array}\right)\\
L & =\left(\begin{array}{cccc}
0 &  &  & 0\\
a_{21} & \ddots\\
\vdots & \ddots & \ddots\\
a_{n1} & \ldots & a_{n,n-1} & 0
\end{array}\right) & R & =\left(\begin{array}{cccc}
0 & a_{12} & \ldots & a_{1n}\\
 & \ddots & \ddots & \vdots\\
 &  & \ddots & a_{n-1,n}\\
0 &  &  & 0
\end{array}\right)
\end{align*}
Wähle $B=D$. Dies liefert das \emph{Gesamtschrittverfahren} (Jacobi-Verfahren).\\
\emph{Bemerkung:} Falls $a_{ii}=0$ ist für gewisse $i$, so permutiere
Zeilen und Spalten, sodass die Diagonalelemente nicht Null sind.
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}-D^{-1}Ax^{\left(k\right)}+D^{-1}b
\end{align*}
Dies ist äquivalent zu:
\begin{align*}
x^{\left(k+1\right)} & =x^{\left(k\right)}-D^{-1}\left(D+L+R\right)x^{\left(k\right)}+D^{-1}b\\
Dx^{\left(k+1\right)} & =\left(L+R\right)x^{\left(k\right)}+b
\end{align*}
Dies ist das Jacobi-Verfahren (J), das in Komponenten geschrieben
wie folgt ist:
\begin{align*}
a_{ii}x_{i}^{\left(k+1\right)} & =-\sum_{\sr{}{j=1}{j\not=i}}^{n}a_{ij}x_{j}^{\left(k\right)}+b_{i}
\end{align*}
Die Iterationsmatrix ist:
\begin{align*}
G & =-D^{-1}\left(L+R\right)
\end{align*}

\item \emph{Einzelschritt-Verfahren} (Gauß-Seidel-Verfahren): In (J) stehen
bei der Berechnung von $x_{i}^{\left(k+1\right)}$ die Werte $x_{1}^{\left(k+1\right)},\ldots,x_{i-1}^{\left(k+1\right)}$
schon zur Verfügung, was genutzt werden soll.\\
Für $i\in\left\{ 1,\ldots,n\right\} $ erhalte:
\begin{align*}
x_{i}^{\left(k+1\right)} & =\frac{1}{a_{ii}}\left(-\sum_{j=1}^{i-1}a_{ij}x_{j}^{\left(k+1\right)}-\sum_{j=i+1}^{n}a_{ij}x_{j}^{\left(k\right)}\right)+b
\end{align*}
Dies ist äquivalent zu:
\begin{align*}
Dx^{\left(k+1\right)} & =-Lx^{\left(k+1\right)}-Rx^{\left(k\right)}+b\\
\left(D+L\right)x^{\left(k+1\right)} & =-Rx^{\left(k\right)}+b\\
x^{\left(k+1\right)} & =x^{\left(k\right)}+\left(D+L\right)^{-1}\left(-Ax^{\left(k\right)}+b\right)
\end{align*}
Mit
\begin{align*}
B & =D+L
\end{align*}
erhalten wir die Iterationsmatrix:
\begin{align*}
G & =\mathbbm{1}-\left(D+L\right)^{-1}A=-\left(D+L\right)^{-1}R
\end{align*}

\end{enumerate}

\section{Definition \textmd{(Zeilensummenbedingungen)}}

Eine $\left(n\times n\right)$-Matrix $A$ erfüllt die \emph{starke
Zeilensummenbedingung}, falls für $i\in\left\{ 1,\ldots,n\right\} $
gilt:
\begin{align*}
\abs{a_{ii}} & >\sum_{\sr{}{j=1}{j\not=i}}^{n}\abs{a_{ij}}
\end{align*}
$A$ erfüllt die \emph{schwache Zeilensummenbedingung}, falls für
$i\in\left\{ 1,\ldots,n\right\} $ gilt:
\begin{align*}
\abs{a_{ii}} & \ge\sum_{\sr{}{j=1}{j\not=i}}^{n}\abs{a_{ij}}
\end{align*}



\section{Definition \textmd{(zerfallend)}}

Eine $\left(n\times n\right)$-Matrix $A$ heißt \emph{zerfallend},
wenn es eine echte Teilmenge $J\subsetneq\left\{ 1,2,\ldots,n\right\} $
mit $J\not=\emptyset$ gibt, sodass $a_{ij}=0$ für $i\in J$ und
$j\not\in J$ gilt.

Nach Umnummerierung hat eine zerfallende Matrix die Struktur:
\begin{align*}
\left(\begin{array}{cc}
A_{11} & 0\\
A_{21} & A_{22}
\end{array}\right)
\end{align*}
Hier sind $A_{11},A_{21}$ und $A_{22}$ Blockmatrizen.


\section{Satz \textmd{(Zeilensummenkriterium)}}
\begin{enumerate}[label=\roman*)]
\item Die $\left(n\times n\right)$-Matrix $A$ erfülle die schwache Zeilensummenbedingung
und sei \emph{nicht} zerfallend. Dann konvergieren sowohl Gesamtschritt-
als auch Einzelschrittverfahren.
\item Wenn die starke Zeilensummenbedingung erfüllt ist, kann auf die Voraussetzung
nicht zerfallend verzichtet werden. Die Spektralradien der Iterationsmatrizen
sind kleiner als:
\begin{align*}
\max_{i\in\left\{ 1,\ldots,n\right\} }\frac{1}{\abs{a_{ii}}}\sum_{j\not=i}\abs{a_{ij}} & <1
\end{align*}

\end{enumerate}

\subsubsection*{Beweis}

Es sei $x\not=0$. Das Gesamtschrittverfahren hat die Iterationsmatrix:
\begin{align*}
G & =-D^{-1}\left(L+R\right)
\end{align*}
Für die Maximumsnorm erhalten wir die Zeilensummennorm
\begin{align*}
\opnorm A_{\infty} & =\max_{i}\sum_{k=1}^{n}\abs{a_{ik}}
\end{align*}
als Operatornorm. Wegen der starken Zeilensummenbedingung gilt:
\begin{align*}
\opnorm{-D^{-1}\left(L+R\right)} & =\max_{i}\frac{1}{\abs{a_{ii}}}\sum_{\sr{}{j=1}{j\not=i}}\abs{a_{ij}}<1
\end{align*}
Satz \ref{sec:Konvergenzsatz} liefert die Konvergenz.

Die schwache Zeilensummenbedingung liefert:
\begin{align*}
\varrho\left(G\right) & \le\opnorm G_{\infty}\le1
\end{align*}
Falls $Gx=\lambda x$ ist, folgt:
\begin{align*}
\abs{\lambda}\norm x & =\norm{Gx}\le\opnorm G\cdot\norm x
\end{align*}
Zu zeigen ist, dass es keinen Eigenwert $\lambda$ von $G$ mit $\abs{\lambda}=1$
gibt.

Nehme also an, dass ein Eigenwert $\lambda$ von $G$ mit $\abs{\lambda}=1$
existiert. Ein zugehörige normiertem Eigenvektor mit $\norm x_{\infty}=1$
sei $x\in\mathbb{R}^{n}$.
\begin{align*}
Gx & =\lambda x
\end{align*}
Es gilt:
\begin{align*}
\abs{\left(Gx\right)_{i}} & =\abs{\lambda x_{i}}=\abs{x_{i}}
\end{align*}
Definiere:
\begin{align*}
N_{1} & :=\left\{ l\in\left\{ 1,\ldots,n\right\} \big|\abs{x_{l}}=1\right\} \\
N_{2} & :=\left\{ k\in\left\{ 1,\ldots,n\right\} \big|\abs{x_{k}}<1\right\} 
\end{align*}
Also gilt:
\begin{align*}
\abs{x_{i}} & =\abs{\left(Gx\right)_{i}}\le\frac{1}{\abs{a_{ii}}}\cdot\left(\sum_{j\not=i}\abs{a_{ij}}\cdot\abs{x_{j}}\right)=\\
 & =\frac{1}{\abs{a_{ii}}}\cdot\left(\sum_{\sr{}{j\in N_{1}}{j\not=i}}\abs{a_{ij}}\cdot\abs{x_{j}}+\sum_{\sr{}{j\in N_{2}}{j\not=i}}\abs{a_{ij}}\cdot\underbrace{\abs{x_{j}}}_{<1}\right)
\end{align*}
Falls für ein $i$ ein $j\in N_{2}$ mit $i\not=j$ und $a_{ij}\not=0$
existiert, so folgt:
\begin{align*}
\abs{x_{i}} & \le\frac{1}{\abs{a_{ii}}}\left(\sum_{j\not=i}\abs{a_{ij}}\cdot\abs{x_{j}}\right)<\overline{a}\le1
\end{align*}
\begin{align*}
\overline{a} & =\max_{i}\frac{1}{\abs{a_{ii}}}\sum_{j\not=i}\abs{a_{ij}}
\end{align*}
Somit muss für $i\in N_{1}$ nun $a_{ij}=0$ für alle $j\in N_{2}$
gelten, da wir sonst einen Widerspruch zu $\abs{x_{i}}=1$ erhalten.

Dies ist ein Widerspruch zur Tatsache, dass $A$ nicht zerfällt.

Für den Rest des Beweises siehe: \noun{Hackbusch}: \emph{Iterationsverfahren
großer schwach besetzter Gleichungssysteme}; Teubner.\qqed


\section{Kontraktionskriterium für spd-Matrizen}



\emph{spd} steht für symmetrisch und positiv definit.

\emph{Ziel:} Formuliere eine Bedingung für die Tatsache $\varrho\left(G\right)<1$,
wobei $G=\mathbbm{1}-B^{-1}A$ ist.

Für eine symmetrische und positiv definite Matrix $A$ definiere das
Skalarprodukt:
\begin{align*}
\left\langle x,y\right\rangle _{A} & :=\left\langle x,Ay\right\rangle =x\cdot Ay=\sum_{i,j}x_{i}a_{ij}y_{j}
\end{align*}
Dabei ist $\left\langle .,.\right\rangle $ das euklidische Skalarprodukt.
Für eine Matrix $F\in\mathbb{R}^{n\times n}$ ist:
\begin{align*}
F^{*} & :=A^{-1}F^{\TT}A
\end{align*}
Es gilt:
\begin{align*}
\left\langle Fx,y\right\rangle _{A} & =\left\langle Fx,Ay\right\rangle =\left\langle x,F^{\TT}Ay\right\rangle =\left\langle x,AA^{-1}F^{\TT}Ay\right\rangle =\left\langle x,F^{*}y\right\rangle _{A}
\end{align*}
Eine (bezüglich $\left\langle .,.\right\rangle _{A}$) selbstadjungierte
Matrix $B=B^{*}$ heißt positiv bezüglich $\left\langle .,.\right\rangle _{A}$,
falls $\left\langle Bx,x\right\rangle _{A}>0$ für alle $x\not=0$
gilt.




\part*{Anhang\thispagestyle{empty}}

\addcontentsline{toc}{part}{Anhang}

\fancyhead[R]{Index}
\fancyhead[C]{Anhang}


\chapter*{Danksagungen}

\addcontentsline{toc}{section}{\hspace*{2.7em}Danksagungen}

\fancyhead[R]{Danksagungen}

Mein besonderer Dank geht an Professor Garcke, der diese Vorlesung
hielt und es mir gestattete, diese Vorlesungsmitschrift zu veröffentlichen.

Außerdem möchte ich mich ganz herzlich bei allen bedanken, die durch
aufmerksames Lesen Fehler gefunden und mir diese mitgeteilt haben.

\vspace{1cm}


\hfill{}Andreas Völklein

\selectlanguage{english}%

\chapter*{GNU Free Documentation License}

\addcontentsline{toc}{section}{\hspace*{2.7em}GNU Free Documentation License}

\fancyhead[R]{GNU Free Documentation License}

\noindent \begin{center}
Version 1.3, 3 November 2008\\
Copyright © 2000, 2001, 2002, 2007, 2008 Free Software Foundation,
Inc. 
\par\end{center}

\noindent \begin{center}
\texttt{<}\url{https://fsf.org/}\texttt{>}
\par\end{center}

\noindent \begin{center}
Everyone is permitted to copy and distribute verbatim copies of this
license document,\\
but changing it is not allowed
\par\end{center}


\section*{\noun{0. Preamble}}

\noindent The purpose of this License is to make a manual, textbook,
or other functional and useful document “free” in the sense of freedom:
to assure everyone the effective freedom to copy and redistribute
it, with or without modifying it, either commercially or noncommercially.
Secondarily, this License preserves for the author and publisher a
way to get credit for their work, while not being considered responsible
for modifications made by others.

\noindent This License is a kind of “copyleft”, which means that derivative
works of the document must themselves be free in the same sense. It
complements the GNU General Public License, which is a copyleft license
designed for free software.

\noindent We have designed this License in order to use it for manuals
for free software, because free software needs free documentation:
a free program should come with manuals providing the same freedoms
that the software does. But this License is not limited to software
manuals; it can be used for any textual work, regardless of subject
matter or whether it is published as a printed book. We recommend
this License principally for works whose purpose is instruction or
reference.


\section*{\noun{1. Applicability and definitions}}

This License applies to any manual or other work, in any medium, that
contains a notice placed by the copyright holder saying it can be
distributed under the terms of this License. Such a notice grants
a world-wide, royalty-free license, unlimited in duration, to use
that work under the conditions stated herein. The “\textbf{Document}”,
below, refers to any such manual or work. Any member of the public
is a licensee, and is addressed as “\textbf{you}”. You accept the
license if you copy, modify or distribute the work in a way requiring
permission under copyright law.

A “\textbf{Modified Version}” of the Document means any work containing
the Document or a portion of it, either copied verbatim, or with modifications
and/or translated into another language.

A “\textbf{Secondary Section}” is a named appendix or a front-matter
section of the Document that deals exclusively with the relationship
of the publishers or authors of the Document to the Document's overall
subject (or to related matters) and contains nothing that could fall
directly within that overall subject. (Thus, if the Document is in
part a textbook of mathematics, a Secondary Section may not explain
any mathematics.) The relationship could be a matter of historical
connection with the subject or with related matters, or of legal,
commercial, philosophical, ethical or political position regarding
them.

The “\textbf{Invariant Sections}” are certain Secondary Sections whose
titles are designated, as being those of Invariant Sections, in the
notice that says that the Document is released under this License.
If a section does not fit the above definition of Secondary then it
is not allowed to be designated as Invariant. The Document may contain
zero Invariant Sections. If the Document does not identify any Invariant
Sections then there are none.

The “\textbf{Cover Texts}” are certain short passages of text that
are listed, as Front-Cover Texts or Back-Cover Texts, in the notice
that says that the Document is released under this License. A Front-Cover
Text may be at most 5 words, and a Back-Cover Text may be at most
25 words.

A “\textbf{Transparent}” copy of the Document means a machine-readable
copy, represented in a format whose specification is available to
the general public, that is suitable for revising the document straightforwardly
with generic text editors or (for images composed of pixels) generic
paint programs or (for drawings) some widely available drawing editor,
and that is suitable for input to text formatters or for automatic
translation to a variety of formats suitable for input to text formatters.
A copy made in an otherwise Transparent file format whose markup,
or absence of markup, has been arranged to thwart or discourage subsequent
modification by readers is not Transparent. An image format is not
Transparent if used for any substantial amount of text. A copy that
is not “Transparent” is called “\textbf{Opaque}”.

Examples of suitable formats for Transparent copies include plain
ASCII without markup, Texinfo input format, \LaTeX{} input format,
SGML or XML using a publicly available DTD, and standard-conforming
simple HTML, PostScript or PDF designed for human modification. Examples
of transparent image formats include PNG, XCF and JPG. Opaque formats
include proprietary formats that can be read and edited only by proprietary
word processors, SGML or XML for which the DTD and/or processing tools
are not generally available, and the machine-generated HTML, PostScript
or PDF produced by some word processors for output purposes only.

The “\textbf{Title Page}” means, for a printed book, the title page
itself, plus such following pages as are needed to hold, legibly,
the material this License requires to appear in the title page. For
works in formats which do not have any title page as such, “Title
Page” means the text near the most prominent appearance of the work's
title, preceding the beginning of the body of the text.

The “\textbf{publisher}” means any person or entity that distributes
copies of the Document to the public.

A section “\textbf{Entitled XYZ}” means a named subunit of the Document
whose title either is precisely XYZ or contains XYZ in parentheses
following text that translates XYZ in another language. (Here XYZ
stands for a specific section name mentioned below, such as “\textbf{Acknowledgements}”,
“\textbf{Dedications}”, “\textbf{Endorsements}”, or “\textbf{History}”.)
To “\textbf{Preserve the Title}” of such a section when you modify
the Document means that it remains a section “Entitled XYZ” according
to this definition.

The Document may include Warranty Disclaimers next to the notice which
states that this License applies to the Document. These Warranty Disclaimers
are considered to be included by reference in this License, but only
as regards disclaiming warranties: any other implication that these
Warranty Disclaimers may have is void and has no effect on the meaning
of this License.


\section*{\noun{2. Verbatim copying}}

You may copy and distribute the Document in any medium, either commercially
or noncommercially, provided that this License, the copyright notices,
and the license notice saying this License applies to the Document
are reproduced in all copies, and that you add no other conditions
whatsoever to those of this License. You may not use technical measures
to obstruct or control the reading or further copying of the copies
you make or distribute. However, you may accept compensation in exchange
for copies. If you distribute a large enough number of copies you
must also follow the conditions in section 3.

You may also lend copies, under the same conditions stated above,
and you may publicly display copies.


\section*{\noun{3. Copying in quantity}}

If you publish printed copies (or copies in media that commonly have
printed covers) of the Document, numbering more than 100, and the
Document's license notice requires Cover Texts, you must enclose the
copies in covers that carry, clearly and legibly, all these Cover
Texts: Front-Cover Texts on the front cover, and Back-Cover Texts
on the back cover. Both covers must also clearly and legibly identify
you as the publisher of these copies. The front cover must present
the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition. Copying with
changes limited to the covers, as long as they preserve the title
of the Document and satisfy these conditions, can be treated as verbatim
copying in other respects.

If the required texts for either cover are too voluminous to fit legibly,
you should put the first ones listed (as many as fit reasonably) on
the actual cover, and continue the rest onto adjacent pages.

If you publish or distribute Opaque copies of the Document numbering
more than 100, you must either include a machine-readable Transparent
copy along with each Opaque copy, or state in or with each Opaque
copy a computer-network location from which the general network-using
public has access to download using public-standard network protocols
a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps,
when you begin distribution of Opaque copies in quantity, to ensure
that this Transparent copy will remain thus accessible at the stated
location until at least one year after the last time you distribute
an Opaque copy (directly or through your agents or retailers) of that
edition to the public.

It is requested, but not required, that you contact the authors of
the Document well before redistributing any large number of copies,
to give them a chance to provide you with an updated version of the
Document.


\section*{\noun{4. Modifications}}

You may copy and distribute a Modified Version of the Document under
the conditions of sections 2 and 3 above, provided that you release
the Modified Version under precisely this License, with the Modified
Version filling the role of the Document, thus licensing distribution
and modification of the Modified Version to whoever possesses a copy
of it. In addition, you must do these things in the Modified Version:
\begin{description}
\item [{A.\hspace*{3.2mm}}] Use in the Title Page (and on the covers,
if any) a title distinct from that of the Document, and from those
of previous versions (which should, if there were any, be listed in
the History section of the Document). You may use the same title as
a previous version if the original publisher of that version gives
permission.
\item [{B.\hspace*{3.4mm}}] List on the Title Page, as authors, one or
more persons or entities responsible for authorship of the modifications
in the Modified Version, together with at least five of the principal
authors of the Document (all of its principal authors, if it has fewer
than five), unless they release you from this requirement.
\item [{C.\hspace*{3.4mm}}] State on the Title page the name of the publisher
of the Modified Version, as the publisher.
\item [{D.\hspace*{3.4mm}}] Preserve all the copyright notices of the
Document.
\item [{E.\hspace*{3.4mm}}] Add an appropriate copyright notice for your
modifications adjacent to the other copyright notices.
\item [{F.\hspace*{3.8mm}}] Include, immediately after the copyright notices,
a license notice giving the public permission to use the Modified
Version under the terms of this License, in the form shown in the
Addendum below.
\item [{G.\hspace*{3.1mm}}] Preserve in that license notice the full lists
of Invariant Sections and required Cover Texts given in the Document's
license notice.
\item [{H.\hspace*{3.2mm}}] Include an unaltered copy of this License.
\item [{I.\hspace*{5.0mm}}] Preserve the section Entitled “History”, Preserve
its Title, and add to it an item stating at least the title, year,
new authors, and publisher of the Modified Version as given on the
Title Page. If there is no section Entitled “History” in the Document,
create one stating the title, year, authors, and publisher of the
Document as given on its Title Page, then add an item describing the
Modified Version as stated in the previous sentence.
\item [{J.\hspace*{4.3mm}}] Preserve the network location, if any, given
in the Document for public access to a Transparent copy of the Document,
and likewise the network locations given in the Document for previous
versions it was based on. These may be placed in the “History” section.
You may omit a network location for a work that was published at least
four years before the Document itself, or if the original publisher
of the version it refers to gives permission.
\item [{K.\hspace*{3.3mm}}] For any section Entitled “Acknowledgements”
or “Dedications”, Preserve the Title of the section, and preserve
in the section all the substance and tone of each of the contributor
acknowledgements and/or dedications given therein.
\item [{L.\hspace*{4.1mm}}] Preserve all the Invariant Sections of the
Document, unaltered in their text and in their titles. Section numbers
or the equivalent are not considered part of the section titles.
\item [{M.\hspace*{3mm}}] Delete any section Entitled “Endorsements”.
Such a section may not be included in the Modified Version.
\item [{N.\hspace*{3.4mm}}] Do not retitle any existing section to be
Entitled “Endorsements” or to conflict in title with any Invariant
Section.
\item [{O.\hspace*{3.4mm}}] Preserve any Warranty Disclaimers.
\end{description}
If the Modified Version includes new front-matter sections or appendices
that qualify as Secondary Sections and contain no material copied
from the Document, you may at your option designate some or all of
these sections as invariant. To do this, add their titles to the list
of Invariant Sections in the Modified Version's license notice. These
titles must be distinct from any other section titles.

You may add a section Entitled “Endorsements”, provided it contains
nothing but endorsements of your Modified Version by various parties—for
example, statements of peer review or that the text has been approved
by an organization as the authoritative definition of a standard.

You may add a passage of up to five words as a Front-Cover Text, and
a passage of up to 25 words as a Back-Cover Text, to the end of the
list of Cover Texts in the Modified Version. Only one passage of Front-Cover
Text and one of Back-Cover Text may be added by (or through arrangements
made by) any one entity. If the Document already includes a cover
text for the same cover, previously added by you or by arrangement
made by the same entity you are acting on behalf of, you may not add
another; but you may replace the old one, on explicit permission from
the previous publisher that added the old one.

The author(s) and publisher(s) of the Document do not by this License
give permission to use their names for publicity for or to assert
or imply endorsement of any Modified Version.


\section*{\noun{5. Combining documents}}

You may combine the Document with other documents released under this
License, under the terms defined in section 4 above for modified versions,
provided that you include in the combination all of the Invariant
Sections of all of the original documents, unmodified, and list them
all as Invariant Sections of your combined work in its license notice,
and that you preserve all their Warranty Disclaimers.

The combined work need only contain one copy of this License, and
multiple identical Invariant Sections may be replaced with a single
copy. If there are multiple Invariant Sections with the same name
but different contents, make the title of each such section unique
by adding at the end of it, in parentheses, the name of the original
author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant
Sections in the license notice of the combined work.

In the combination, you must combine any sections Entitled “History”
in the various original documents, forming one section Entitled “History”;
likewise combine any sections Entitled “Acknowledgements”, and any
sections Entitled “Dedications”. You must delete all sections Entitled
“Endorsements”.


\section*{\noun{6. Collections of documents}}

You may make a collection consisting of the Document and other documents
released under this License, and replace the individual copies of
this License in the various documents with a single copy that is included
in the collection, provided that you follow the rules of this License
for verbatim copying of each of the documents in all other respects.

You may extract a single document from such a collection, and distribute
it individually under this License, provided you insert a copy of
this License into the extracted document, and follow this License
in all other respects regarding verbatim copying of that document.


\section*{\noun{7. Aggregation with independent works}}

A compilation of the Document or its derivatives with other separate
and independent documents or works, in or on a volume of a storage
or distribution medium, is called an “aggregate” if the copyright
resulting from the compilation is not used to limit the legal rights
of the compilation's users beyond what the individual works permit.
When the Document is included in an aggregate, this License does not
apply to the other works in the aggregate which are not themselves
derivative works of the Document.

If the Cover Text requirement of section 3 is applicable to these
copies of the Document, then if the Document is less than one half
of the entire aggregate, the Document's Cover Texts may be placed
on covers that bracket the Document within the aggregate, or the electronic
equivalent of covers if the Document is in electronic form. Otherwise
they must appear on printed covers that bracket the whole aggregate.


\section*{\noun{8. Translation}}

Translation is considered a kind of modification, so you may distribute
translations of the Document under the terms of section 4. Replacing
Invariant Sections with translations requires special permission from
their copyright holders, but you may include translations of some
or all Invariant Sections in addition to the original versions of
these Invariant Sections. You may include a translation of this License,
and all the license notices in the Document, and any Warranty Disclaimers,
provided that you also include the original English version of this
License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original
version of this License or a notice or disclaimer, the original version
will prevail.

If a section in the Document is Entitled “Acknowledgements”, “Dedications”,
or “History”, the requirement (section 4) to Preserve its Title (section
1) will typically require changing the actual title.


\section*{\noun{9. Termination}}

You may not copy, modify, sublicense, or distribute the Document except
as expressly provided under this License. Any attempt otherwise to
copy, modify, sublicense, or distribute it is void, and will automatically
terminate your rights under this License.

However, if you cease all violation of this License, then your license
from a particular copyright holder is reinstated (a) provisionally,
unless and until the copyright holder explicitly and finally terminates
your license, and (b) permanently, if the copyright holder fails to
notify you of the violation by some reasonable means prior to 60 days
after the cessation.

Moreover, your license from a particular copyright holder is reinstated
permanently if the copyright holder notifies you of the violation
by some reasonable means, this is the first time you have received
notice of violation of this License (for any work) from that copyright
holder, and you cure the violation prior to 30 days after your receipt
of the notice.

Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License. If your rights have been terminated and not permanently
reinstated, receipt of a copy of some or all of the same material
does not give you any rights to use it.


\section*{\noun{10. Future revisions of this license}}

The Free Software Foundation may publish new, revised versions of
the GNU Free Documentation License from time to time. Such new versions
will be similar in spirit to the present version, but may differ in
detail to address new problems or concerns. See \url{https://www.gnu.org/copyleft/}.

Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this
License \textquotedbl{}or any later version\textquotedbl{} applies
to it, you have the option of following the terms and conditions either
of that specified version or of any later version that has been published
(not as a draft) by the Free Software Foundation. If the Document
does not specify a version number of this License, you may choose
any version ever published (not as a draft) by the Free Software Foundation.
If the Document specifies that a proxy can decide which future versions
of this License can be used, that proxy's public statement of acceptance
of a version permanently authorizes you to choose that version for
the Document.


\section*{\noun{11. Relicensing}}

“Massive Multiauthor Collaboration Site” (or “MMC Site”) means any
World Wide Web server that publishes copyrightable works and also
provides prominent facilities for anybody to edit those works. A public
wiki that anybody can edit is an example of such a server. A “Massive
Multiauthor Collaboration” (or “MMC”) contained in the site means
any set of copyrightable works thus published on the MMC site.

“CC-BY-SA” means the Creative Commons Attribution-Share Alike 3.0
license published by Creative Commons Corporation, a not-for-profit
corporation with a principal place of business in San Francisco, California,
as well as future copyleft versions of that license published by that
same organization.

“Incorporate” means to publish or republish a Document, in whole or
in part, as part of another Document.

An MMC is “eligible for relicensing” if it is licensed under this
License, and if all works that were first published under this License
somewhere other than this MMC, and subsequently incorporated in whole
or in part into the MMC, (1) had no cover texts or invariant sections,
and (2) were thus incorporated prior to November 1, 2008.

The operator of an MMC Site may republish an MMC contained in the
site under CC-BY-SA on the same site at any time before August 1,
2009, provided the MMC is eligible for relicensing.


\section*{\noun{Addendum}: How to use this License for your documents}

To use this License in a document you have written, include a copy
of the License in the document and put the following copyright and
license notices just after the title page:
\begin{quote}
Copyright © YEAR YOUR NAME.\\
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;

with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
Texts.

A copy of the license is included in the section entitled \textquotedbl{}GNU
Free Documentation License\textquotedbl{}.
\end{quote}
If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
replace the \textquotedbl{}with $\ldots$ Texts.\textquotedbl{} line
with this:
\begin{quote}
with the Invariant Sections being LIST THEIR TITLES, with the Front-Cover
Texts being LIST, and with the Back-Cover Texts being LIST.
\end{quote}
If you have Invariant Sections without Cover Texts, or some other
combination of the three, merge those two alternatives to suit the
situation.

If your document contains nontrivial examples of program code, we
recommend releasing these examples in parallel under your choice of
free software license, such as the GNU General Public License, to
permit their use in free software.

\selectlanguage{ngerman}%
\label{END}
\end{document}
